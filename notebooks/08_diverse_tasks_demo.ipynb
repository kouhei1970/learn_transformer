{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08: Transformerで様々なタスクを学習\n",
    "\n",
    "このノートブックでは、Transformerを使って**様々なSequence-to-Sequenceタスク**を学習させます。\n",
    "\n",
    "## タスク一覧\n",
    "1. **コピータスク**: 入力をそのまま出力\n",
    "2. **反転タスク**: 入力を逆順に出力\n",
    "3. **ソートタスク**: 数字を昇順にソート\n",
    "4. **加算タスク**: 2つの数字を足し算\n",
    "5. **簡易翻訳タスク**: おもちゃの言語間翻訳\n",
    "\n",
    "これらのタスクを通じて、Transformerがどのようにパターンを学習するかを理解します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.transformer import Transformer, count_parameters\n",
    "\n",
    "# デバイスの設定\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 共通設定\n",
    "PAD_IDX = 0\n",
    "START_IDX = 1\n",
    "END_IDX = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 共通の学習関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data_generator, num_epochs=200, batch_size=64, lr=0.001, verbose=True):\n",
    "    \"\"\"\n",
    "    モデルを学習する共通関数\n",
    "    \n",
    "    Args:\n",
    "        model: Transformerモデル\n",
    "        data_generator: (src, tgt_input, tgt_output)を返すジェネレータ関数\n",
    "        num_epochs: エポック数\n",
    "        batch_size: バッチサイズ\n",
    "        lr: 学習率\n",
    "        verbose: 進捗を表示するか\n",
    "    \n",
    "    Returns:\n",
    "        losses, accuracies\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "    \n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    model.train()\n",
    "    iterator = range(num_epochs)\n",
    "    if verbose:\n",
    "        iterator = tqdm(iterator, desc=\"Training\")\n",
    "    \n",
    "    for epoch in iterator:\n",
    "        src, tgt_in, tgt_out = data_generator(batch_size)\n",
    "        src = src.to(device)\n",
    "        tgt_in = tgt_in.to(device)\n",
    "        tgt_out = tgt_out.to(device)\n",
    "        \n",
    "        logits = model(src, tgt_in)\n",
    "        \n",
    "        loss = criterion(\n",
    "            logits.reshape(-1, logits.size(-1)),\n",
    "            tgt_out.reshape(-1)\n",
    "        )\n",
    "        \n",
    "        predictions = logits.argmax(dim=-1)\n",
    "        mask = tgt_out != PAD_IDX\n",
    "        correct = ((predictions == tgt_out) & mask).sum().float() / mask.sum().float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        accuracies.append(correct.item())\n",
    "        \n",
    "        if verbose:\n",
    "            iterator.set_postfix(loss=f\"{loss.item():.4f}\", acc=f\"{correct.item():.4f}\")\n",
    "    \n",
    "    return losses, accuracies\n",
    "\n",
    "\n",
    "def plot_training(losses, accuracies, title):\n",
    "    \"\"\"学習曲線を描画\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    ax1.plot(losses)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title(f'{title} - Loss')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2.plot(accuracies)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title(f'{title} - Accuracy')\n",
    "    ax2.set_ylim(0, 1.05)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def test_model(model, data_generator, num_samples=5, max_len=20):\n",
    "    \"\"\"モデルをテストして結果を表示\"\"\"\n",
    "    model.eval()\n",
    "    src, tgt_in, tgt_out = data_generator(num_samples)\n",
    "    src = src.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated = model.greedy_decode(\n",
    "            src,\n",
    "            max_len=max_len,\n",
    "            start_token_id=START_IDX,\n",
    "            end_token_id=END_IDX,\n",
    "        )\n",
    "    \n",
    "    results = []\n",
    "    for i in range(num_samples):\n",
    "        src_tokens = [t for t in src[i].tolist() if t > END_IDX]\n",
    "        expected = [t for t in tgt_out[i].tolist() if t > END_IDX]\n",
    "        gen_tokens = [t for t in generated[i].tolist() if t > END_IDX]\n",
    "        \n",
    "        match = src_tokens == gen_tokens or expected == gen_tokens\n",
    "        results.append((src_tokens, expected, gen_tokens, match))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. コピータスク\n",
    "\n",
    "入力シーケンスをそのまま出力する最も基本的なタスク。\n",
    "\n",
    "```\n",
    "入力:  [3, 5, 7, 2, 9]\n",
    "出力:  [3, 5, 7, 2, 9]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_copy_data(batch_size, seq_len=8, vocab_size=20):\n",
    "    \"\"\"コピータスクのデータ生成\"\"\"\n",
    "    tokens = torch.randint(3, vocab_size, (batch_size, seq_len))\n",
    "    src = tokens.clone()\n",
    "    tgt_input = torch.cat([torch.full((batch_size, 1), START_IDX), tokens], dim=1)\n",
    "    tgt_output = torch.cat([tokens, torch.full((batch_size, 1), END_IDX)], dim=1)\n",
    "    return src, tgt_input, tgt_output\n",
    "\n",
    "# テストデータを表示\n",
    "src, tgt_in, tgt_out = generate_copy_data(1)\n",
    "print(\"Copy Task Example:\")\n",
    "print(f\"  Source:        {src[0].tolist()}\")\n",
    "print(f\"  Target Input:  {tgt_in[0].tolist()}\")\n",
    "print(f\"  Target Output: {tgt_out[0].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コピータスク用モデル\n",
    "copy_model = Transformer(\n",
    "    src_vocab_size=20, tgt_vocab_size=20,\n",
    "    d_model=64, num_heads=4,\n",
    "    num_encoder_layers=2, num_decoder_layers=2,\n",
    "    d_ff=256,\n",
    ").to(device)\n",
    "\n",
    "print(f\"Parameters: {count_parameters(copy_model):,}\")\n",
    "\n",
    "# 学習\n",
    "losses, accs = train_model(copy_model, generate_copy_data, num_epochs=150)\n",
    "plot_training(losses, accs, \"Copy Task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テスト\n",
    "print(\"Copy Task Test Results:\")\n",
    "print(\"=\" * 50)\n",
    "results = test_model(copy_model, generate_copy_data)\n",
    "for src_t, exp_t, gen_t, match in results:\n",
    "    mark = \"✓\" if match else \"✗\"\n",
    "    print(f\"{mark} Input:    {src_t}\")\n",
    "    print(f\"  Output:   {gen_t}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. 反転タスク\n",
    "\n",
    "入力シーケンスを逆順に出力するタスク。位置情報の理解が必要。\n",
    "\n",
    "```\n",
    "入力:  [3, 5, 7, 2, 9]\n",
    "出力:  [9, 2, 7, 5, 3]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reverse_data(batch_size, seq_len=8, vocab_size=20):\n",
    "    \"\"\"反転タスクのデータ生成\"\"\"\n",
    "    tokens = torch.randint(3, vocab_size, (batch_size, seq_len))\n",
    "    src = tokens.clone()\n",
    "    reversed_tokens = tokens.flip(dims=[1])  # 反転\n",
    "    tgt_input = torch.cat([torch.full((batch_size, 1), START_IDX), reversed_tokens], dim=1)\n",
    "    tgt_output = torch.cat([reversed_tokens, torch.full((batch_size, 1), END_IDX)], dim=1)\n",
    "    return src, tgt_input, tgt_output\n",
    "\n",
    "# テストデータを表示\n",
    "src, tgt_in, tgt_out = generate_reverse_data(1)\n",
    "print(\"Reverse Task Example:\")\n",
    "print(f\"  Source:        {src[0].tolist()}\")\n",
    "print(f\"  Target Input:  {tgt_in[0].tolist()}\")\n",
    "print(f\"  Target Output: {tgt_out[0].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 反転タスク用モデル\n",
    "reverse_model = Transformer(\n",
    "    src_vocab_size=20, tgt_vocab_size=20,\n",
    "    d_model=64, num_heads=4,\n",
    "    num_encoder_layers=2, num_decoder_layers=2,\n",
    "    d_ff=256,\n",
    ").to(device)\n",
    "\n",
    "# 学習\n",
    "losses, accs = train_model(reverse_model, generate_reverse_data, num_epochs=200)\n",
    "plot_training(losses, accs, \"Reverse Task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テスト\n",
    "print(\"Reverse Task Test Results:\")\n",
    "print(\"=\" * 50)\n",
    "results = test_model(reverse_model, generate_reverse_data)\n",
    "for src_t, exp_t, gen_t, match in results:\n",
    "    mark = \"✓\" if match else \"✗\"\n",
    "    print(f\"{mark} Input:    {src_t}\")\n",
    "    print(f\"  Expected: {exp_t}\")\n",
    "    print(f\"  Output:   {gen_t}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. ソートタスク\n",
    "\n",
    "入力の数字を昇順にソートして出力するタスク。より複雑なパターン認識が必要。\n",
    "\n",
    "```\n",
    "入力:  [7, 3, 9, 1, 5]\n",
    "出力:  [1, 3, 5, 7, 9]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sort_data(batch_size, seq_len=6, vocab_size=15):\n",
    "    \"\"\"ソートタスクのデータ生成\"\"\"\n",
    "    # 重複なしの数字を生成\n",
    "    tokens_list = []\n",
    "    for _ in range(batch_size):\n",
    "        nums = torch.randperm(vocab_size - 3)[:seq_len] + 3  # 3以上の数字\n",
    "        tokens_list.append(nums)\n",
    "    tokens = torch.stack(tokens_list)\n",
    "    \n",
    "    src = tokens.clone()\n",
    "    sorted_tokens, _ = tokens.sort(dim=1)  # ソート\n",
    "    tgt_input = torch.cat([torch.full((batch_size, 1), START_IDX), sorted_tokens], dim=1)\n",
    "    tgt_output = torch.cat([sorted_tokens, torch.full((batch_size, 1), END_IDX)], dim=1)\n",
    "    return src, tgt_input, tgt_output\n",
    "\n",
    "# テストデータを表示\n",
    "src, tgt_in, tgt_out = generate_sort_data(1)\n",
    "print(\"Sort Task Example:\")\n",
    "print(f\"  Source:        {src[0].tolist()}\")\n",
    "print(f\"  Target Input:  {tgt_in[0].tolist()}\")\n",
    "print(f\"  Target Output: {tgt_out[0].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ソートタスク用モデル（少し大きめ）\n",
    "sort_model = Transformer(\n",
    "    src_vocab_size=15, tgt_vocab_size=15,\n",
    "    d_model=128, num_heads=4,\n",
    "    num_encoder_layers=3, num_decoder_layers=3,\n",
    "    d_ff=512,\n",
    ").to(device)\n",
    "\n",
    "print(f\"Parameters: {count_parameters(sort_model):,}\")\n",
    "\n",
    "# 学習（ソートは難しいので多めのエポック）\n",
    "losses, accs = train_model(sort_model, generate_sort_data, num_epochs=500, lr=0.0005)\n",
    "plot_training(losses, accs, \"Sort Task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テスト\n",
    "print(\"Sort Task Test Results:\")\n",
    "print(\"=\" * 50)\n",
    "results = test_model(sort_model, generate_sort_data)\n",
    "for src_t, exp_t, gen_t, match in results:\n",
    "    mark = \"✓\" if match else \"✗\"\n",
    "    print(f\"{mark} Input:    {src_t}\")\n",
    "    print(f\"  Expected: {exp_t}\")\n",
    "    print(f\"  Output:   {gen_t}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. 加算タスク\n",
    "\n",
    "2つの数字を足し算するタスク。数字の各桁をトークンとして扱います。\n",
    "\n",
    "```\n",
    "入力:  [1, 2, 3, +, 4, 5, 6]  (123 + 456)\n",
    "出力:  [5, 7, 9]              (579)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加算タスク用の特殊トークン\n",
    "# 0: PAD, 1: START, 2: END, 3: +記号, 4-13: 数字0-9\n",
    "PLUS_IDX = 3\n",
    "DIGIT_OFFSET = 4  # 数字0は4, 数字9は13\n",
    "\n",
    "def num_to_tokens(n, min_digits=1):\n",
    "    \"\"\"数字をトークン列に変換\"\"\"\n",
    "    digits = [int(d) + DIGIT_OFFSET for d in str(n)]\n",
    "    while len(digits) < min_digits:\n",
    "        digits.insert(0, DIGIT_OFFSET)  # 0でパディング\n",
    "    return digits\n",
    "\n",
    "def tokens_to_num(tokens):\n",
    "    \"\"\"トークン列を数字に変換\"\"\"\n",
    "    digits = [t - DIGIT_OFFSET for t in tokens if DIGIT_OFFSET <= t <= DIGIT_OFFSET + 9]\n",
    "    if not digits:\n",
    "        return 0\n",
    "    return int(''.join(map(str, digits)))\n",
    "\n",
    "def generate_addition_data(batch_size, max_digits=3, vocab_size=14):\n",
    "    \"\"\"加算タスクのデータ生成\"\"\"\n",
    "    src_list = []\n",
    "    tgt_in_list = []\n",
    "    tgt_out_list = []\n",
    "    \n",
    "    max_num = 10 ** max_digits - 1\n",
    "    \n",
    "    for _ in range(batch_size):\n",
    "        a = torch.randint(0, max_num + 1, (1,)).item()\n",
    "        b = torch.randint(0, max_num + 1, (1,)).item()\n",
    "        result = a + b\n",
    "        \n",
    "        # ソース: a + b\n",
    "        src_tokens = num_to_tokens(a, max_digits) + [PLUS_IDX] + num_to_tokens(b, max_digits)\n",
    "        \n",
    "        # ターゲット: result\n",
    "        result_tokens = num_to_tokens(result, max_digits + 1)  # 桁上がり考慮\n",
    "        \n",
    "        src_list.append(torch.tensor(src_tokens))\n",
    "        tgt_in_list.append(torch.tensor([START_IDX] + result_tokens))\n",
    "        tgt_out_list.append(torch.tensor(result_tokens + [END_IDX]))\n",
    "    \n",
    "    # パディング\n",
    "    src = nn.utils.rnn.pad_sequence(src_list, batch_first=True, padding_value=PAD_IDX)\n",
    "    tgt_in = nn.utils.rnn.pad_sequence(tgt_in_list, batch_first=True, padding_value=PAD_IDX)\n",
    "    tgt_out = nn.utils.rnn.pad_sequence(tgt_out_list, batch_first=True, padding_value=PAD_IDX)\n",
    "    \n",
    "    return src, tgt_in, tgt_out\n",
    "\n",
    "# テストデータを表示\n",
    "src, tgt_in, tgt_out = generate_addition_data(3, max_digits=2)\n",
    "print(\"Addition Task Examples:\")\n",
    "for i in range(3):\n",
    "    src_t = src[i].tolist()\n",
    "    # +の位置を見つける\n",
    "    plus_pos = src_t.index(PLUS_IDX)\n",
    "    a = tokens_to_num(src_t[:plus_pos])\n",
    "    b = tokens_to_num(src_t[plus_pos+1:])\n",
    "    result = tokens_to_num([t for t in tgt_out[i].tolist() if t != END_IDX and t != PAD_IDX])\n",
    "    print(f\"  {a} + {b} = {result}\")\n",
    "    print(f\"    Tokens: {src_t} -> {[t for t in tgt_out[i].tolist() if t != END_IDX and t != PAD_IDX]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加算タスク用モデル\n",
    "add_model = Transformer(\n",
    "    src_vocab_size=14, tgt_vocab_size=14,\n",
    "    d_model=128, num_heads=4,\n",
    "    num_encoder_layers=3, num_decoder_layers=3,\n",
    "    d_ff=512,\n",
    ").to(device)\n",
    "\n",
    "print(f\"Parameters: {count_parameters(add_model):,}\")\n",
    "\n",
    "# 2桁の加算で学習\n",
    "def gen_add_2digit(batch_size):\n",
    "    return generate_addition_data(batch_size, max_digits=2)\n",
    "\n",
    "losses, accs = train_model(add_model, gen_add_2digit, num_epochs=500, lr=0.0005)\n",
    "plot_training(losses, accs, \"Addition Task (2 digits)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テスト\n",
    "print(\"Addition Task Test Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "add_model.eval()\n",
    "src, tgt_in, tgt_out = generate_addition_data(8, max_digits=2)\n",
    "src = src.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated = add_model.greedy_decode(src, max_len=6, start_token_id=START_IDX, end_token_id=END_IDX)\n",
    "\n",
    "correct_count = 0\n",
    "for i in range(8):\n",
    "    src_t = src[i].cpu().tolist()\n",
    "    plus_pos = src_t.index(PLUS_IDX) if PLUS_IDX in src_t else len(src_t)//2\n",
    "    a = tokens_to_num(src_t[:plus_pos])\n",
    "    b = tokens_to_num(src_t[plus_pos+1:])\n",
    "    expected = a + b\n",
    "    \n",
    "    gen_tokens = [t for t in generated[i].tolist() if t not in [START_IDX, END_IDX, PAD_IDX]]\n",
    "    predicted = tokens_to_num(gen_tokens)\n",
    "    \n",
    "    match = expected == predicted\n",
    "    if match:\n",
    "        correct_count += 1\n",
    "    mark = \"✓\" if match else \"✗\"\n",
    "    print(f\"{mark} {a:2d} + {b:2d} = {predicted:3d}  (expected: {expected})\")\n",
    "\n",
    "print(f\"\\nAccuracy: {correct_count}/8 = {correct_count/8:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. 簡易翻訳タスク\n",
    "\n",
    "おもちゃの「言語」間での翻訳タスク。\n",
    "\n",
    "**言語A（数字）→ 言語B（アルファベット相当のトークン）**\n",
    "\n",
    "```\n",
    "ルール例:\n",
    "  3 -> 13 (3+10)\n",
    "  5 -> 15 (5+10)\n",
    "  複数トークンの場合は各要素を変換\n",
    "\n",
    "入力:  [3, 5, 7]\n",
    "出力:  [13, 15, 17]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_translation_data(batch_size, seq_len=6, src_vocab=15, offset=10):\n",
    "    \"\"\"\n",
    "    簡易翻訳タスクのデータ生成\n",
    "    ルール: 各トークンに offset を足す\n",
    "    \"\"\"\n",
    "    tokens = torch.randint(3, src_vocab, (batch_size, seq_len))\n",
    "    src = tokens.clone()\n",
    "    translated = tokens + offset  # 翻訳ルール\n",
    "    tgt_input = torch.cat([torch.full((batch_size, 1), START_IDX), translated], dim=1)\n",
    "    tgt_output = torch.cat([translated, torch.full((batch_size, 1), END_IDX)], dim=1)\n",
    "    return src, tgt_input, tgt_output\n",
    "\n",
    "# テストデータを表示\n",
    "src, tgt_in, tgt_out = generate_translation_data(1)\n",
    "print(\"Translation Task Example:\")\n",
    "print(f\"  Source (Lang A):  {src[0].tolist()}\")\n",
    "print(f\"  Target (Lang B):  {[t for t in tgt_out[0].tolist() if t != END_IDX]}\")\n",
    "print(f\"  Rule: each token + 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 翻訳タスク用モデル（ソースとターゲットの語彙サイズが異なる）\n",
    "trans_model = Transformer(\n",
    "    src_vocab_size=15,   # 言語A: 3-14\n",
    "    tgt_vocab_size=25,   # 言語B: 13-24\n",
    "    d_model=64, num_heads=4,\n",
    "    num_encoder_layers=2, num_decoder_layers=2,\n",
    "    d_ff=256,\n",
    ").to(device)\n",
    "\n",
    "print(f\"Parameters: {count_parameters(trans_model):,}\")\n",
    "\n",
    "# 学習\n",
    "losses, accs = train_model(trans_model, generate_translation_data, num_epochs=150)\n",
    "plot_training(losses, accs, \"Translation Task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テスト\n",
    "print(\"Translation Task Test Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "trans_model.eval()\n",
    "src, tgt_in, tgt_out = generate_translation_data(5)\n",
    "src_device = src.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated = trans_model.greedy_decode(src_device, max_len=10, start_token_id=START_IDX, end_token_id=END_IDX)\n",
    "\n",
    "for i in range(5):\n",
    "    src_t = src[i].tolist()\n",
    "    expected = [t for t in tgt_out[i].tolist() if t != END_IDX]\n",
    "    gen_t = [t for t in generated[i].tolist() if t not in [START_IDX, END_IDX, PAD_IDX]]\n",
    "    \n",
    "    match = expected == gen_t\n",
    "    mark = \"✓\" if match else \"✗\"\n",
    "    print(f\"{mark} Source:   {src_t}\")\n",
    "    print(f\"  Expected: {expected}\")\n",
    "    print(f\"  Output:   {gen_t}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## タスク難易度の比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各タスクの最終精度を比較\n",
    "task_results = {\n",
    "    'Copy': accs[-1] if 'copy_model' in dir() else 0,\n",
    "    'Reverse': 0,\n",
    "    'Sort': 0,\n",
    "    'Addition': 0,\n",
    "    'Translation': 0,\n",
    "}\n",
    "\n",
    "# 各モデルで短いテストを実行\n",
    "def quick_test(model, data_gen, n=50):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for _ in range(n):\n",
    "        results = test_model(model, data_gen, num_samples=1)\n",
    "        if results[0][3]:  # match\n",
    "            correct += 1\n",
    "    return correct / n\n",
    "\n",
    "print(\"Testing each model...\")\n",
    "task_results['Copy'] = quick_test(copy_model, generate_copy_data)\n",
    "task_results['Reverse'] = quick_test(reverse_model, generate_reverse_data)\n",
    "task_results['Sort'] = quick_test(sort_model, generate_sort_data)\n",
    "task_results['Translation'] = quick_test(trans_model, generate_translation_data)\n",
    "\n",
    "# 加算は別途テスト\n",
    "add_model.eval()\n",
    "correct = 0\n",
    "for _ in range(50):\n",
    "    src, tgt_in, tgt_out = generate_addition_data(1, max_digits=2)\n",
    "    with torch.no_grad():\n",
    "        gen = add_model.greedy_decode(src.to(device), max_len=6, start_token_id=START_IDX, end_token_id=END_IDX)\n",
    "    src_t = src[0].tolist()\n",
    "    plus_pos = src_t.index(PLUS_IDX) if PLUS_IDX in src_t else len(src_t)//2\n",
    "    a = tokens_to_num(src_t[:plus_pos])\n",
    "    b = tokens_to_num(src_t[plus_pos+1:])\n",
    "    expected = a + b\n",
    "    predicted = tokens_to_num([t for t in gen[0].tolist() if t not in [START_IDX, END_IDX, PAD_IDX]])\n",
    "    if expected == predicted:\n",
    "        correct += 1\n",
    "task_results['Addition'] = correct / 50\n",
    "\n",
    "print(\"\\nTask Difficulty Comparison (Test Accuracy):\")\n",
    "print(\"=\" * 40)\n",
    "for task, acc in sorted(task_results.items(), key=lambda x: -x[1]):\n",
    "    bar = \"█\" * int(acc * 20)\n",
    "    print(f\"{task:12s}: {acc:5.1%} {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# バーチャートで可視化\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "tasks = list(task_results.keys())\n",
    "accs = list(task_results.values())\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(tasks)))\n",
    "\n",
    "bars = ax.bar(tasks, accs, color=colors)\n",
    "ax.set_ylabel('Test Accuracy')\n",
    "ax.set_title('Task Difficulty Comparison')\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# 値をバーの上に表示\n",
    "for bar, acc in zip(bars, accs):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "            f'{acc:.0%}', ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "### タスクごとの特徴\n",
    "\n",
    "| タスク | 難易度 | 必要な能力 |\n",
    "|--------|--------|------------|\n",
    "| **Copy** | 簡単 | 入力をそのまま出力 |\n",
    "| **Translation** | 簡単 | 単純なルール変換 |\n",
    "| **Reverse** | 中程度 | 位置情報の理解と逆順生成 |\n",
    "| **Sort** | 難しい | 全体の比較と順序付け |\n",
    "| **Addition** | 難しい | 桁上がりの概念、算術演算 |\n",
    "\n",
    "### 学習のポイント\n",
    "\n",
    "1. **Transformerの汎用性**: 同じアーキテクチャで多様なタスクを学習可能\n",
    "2. **タスクの複雑さ**: 単純なパターン変換より、論理的推論が必要なタスクは難しい\n",
    "3. **学習時間**: 複雑なタスクほど多くのエポックが必要\n",
    "4. **モデルサイズ**: タスクの複雑さに応じてモデルサイズを調整\n",
    "\n",
    "### 次のステップ\n",
    "- 実際の自然言語データ（翻訳、要約など）での学習\n",
    "- より長いシーケンスへの対応\n",
    "- Beam Searchによる生成品質の向上"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
