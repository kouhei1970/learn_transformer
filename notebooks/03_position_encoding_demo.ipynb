{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08ac4bc0",
   "metadata": {},
   "source": [
    "## å­¦ç¿’ç›®æ¨™\n",
    "\n",
    "1. ãªãœAttentionã«ã¯ä½ç½®æƒ…å ±ãŒå¿…è¦ãªã®ã‹ã‚’ç†è§£ã™ã‚‹\n",
    "2. Sin/Cosé–¢æ•°ã‚’ä½¿ã£ãŸä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®ä»•çµ„ã¿ã‚’ç†è§£ã™ã‚‹\n",
    "3. Position Encodingã‚’å®Ÿè£…ã—ã¦å¯è¦–åŒ–ã™ã‚‹\n",
    "4. ä½ç½®æƒ…å ±ãŒã©ã®ã‚ˆã†ã«åŸ‹ã‚è¾¼ã¿ã«åŠ ç®—ã•ã‚Œã‚‹ã‹ã‚’ç¢ºèªã™ã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584bac93",
   "metadata": {},
   "source": [
    "## 1. ãªãœä½ç½®æƒ…å ±ãŒå¿…è¦ã‹ï¼Ÿ\n",
    "\n",
    "### å•é¡Œ: Attentionã¯é †åºã‚’è€ƒæ…®ã—ãªã„\n",
    "\n",
    "Self-Attentionã‚„Multi-Head Attentionã¯ã€å…¥åŠ›ã®**é †åºï¼ˆä½ç½®ï¼‰ã‚’è€ƒæ…®ã—ã¾ã›ã‚“**ã€‚\n",
    "\n",
    "ä¾‹ãˆã°ä»¥ä¸‹ã®2ã¤ã®æ–‡:\n",
    "- ã€ŒçŠ¬ãŒçŒ«ã‚’è¿½ã„ã‹ã‘ãŸã€\n",
    "- ã€ŒçŒ«ãŒçŠ¬ã‚’è¿½ã„ã‹ã‘ãŸã€\n",
    "\n",
    "å˜èªã®é †åºã‚’å…¥ã‚Œæ›¿ãˆã¦ã‚‚ã€Attentionæ©Ÿæ§‹ã¯åŒã˜å‡ºåŠ›ã‚’è¿”ã—ã¦ã—ã¾ã„ã¾ã™ï¼ˆå˜èªã®é›†åˆã¨ã—ã¦ã®ã¿è¦‹ã‚‹ï¼‰ã€‚\n",
    "\n",
    "### è§£æ±ºç­–: Position Encoding\n",
    "\n",
    "å„å˜èªã®**ä½ç½®æƒ…å ±**ã‚’åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã«åŠ ç®—ã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ã«é †åºæƒ…å ±ã‚’ä¸ãˆã¾ã™:\n",
    "\n",
    "```\n",
    "æœ€çµ‚çš„ãªå…¥åŠ› = å˜èªåŸ‹ã‚è¾¼ã¿ + ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "```\n",
    "\n",
    "ã“ã‚Œã«ã‚ˆã‚Šã€åŒã˜å˜èªã§ã‚‚ã€Œæ–‡ã®å…ˆé ­ã€ã¨ã€Œæ–‡ã®ä¸­é–“ã€ã§ã¯ç•°ãªã‚‹è¡¨ç¾ã«ãªã‚Šã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30ba242",
   "metadata": {},
   "source": [
    "## 2. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048968e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'Hiragino Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ã‚·ãƒ¼ãƒ‰å€¤è¨­å®š\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e081916",
   "metadata": {},
   "source": [
    "## 3. Position Encodingã®æ•°å¼\n",
    "\n",
    "Transformerã®å…ƒè«–æ–‡ \"Attention is All You Need\" ã§ã¯ã€Sin/Cosé–¢æ•°ã‚’ä½¿ã£ãŸä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒææ¡ˆã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "### æ•°å¼\n",
    "\n",
    "ä½ç½® $pos$ ã®æ¬¡å…ƒ $i$ ã«å¯¾ã—ã¦:\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
    "$$\n",
    "\n",
    "### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "\n",
    "- $pos$: ä½ç½®ï¼ˆ0, 1, 2, ...ï¼‰\n",
    "- $i$: æ¬¡å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "- $d_{\\text{model}}$: ãƒ¢ãƒ‡ãƒ«ã®æ¬¡å…ƒæ•°\n",
    "- å¶æ•°æ¬¡å…ƒã«ã¯sinã€å¥‡æ•°æ¬¡å…ƒã«ã¯cosã‚’ä½¿ç”¨\n",
    "\n",
    "### ãªãœã“ã®å¼ï¼Ÿ\n",
    "\n",
    "1. **å‘¨æœŸæ€§**: ç•°ãªã‚‹å‘¨æ³¢æ•°ã®sin/cosæ³¢ã‚’ä½¿ã†ã“ã¨ã§ã€ä½ç½®ã®ç›¸å¯¾çš„ãªé–¢ä¿‚ã‚’æ‰ãˆã‚‹\n",
    "2. **å¤–æŒ¿å¯èƒ½**: å­¦ç¿’æ™‚ã‚ˆã‚Šé•·ã„ç³»åˆ—ã«ã‚‚å¯¾å¿œå¯èƒ½\n",
    "3. **ç·šå½¢å¤‰æ›ã§ç›¸å¯¾ä½ç½®ã‚’è¡¨ç¾**: $PE_{pos+k}$ ãŒ $PE_{pos}$ ã®ç·šå½¢çµåˆã§è¡¨ç¾ã§ãã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774355a2",
   "metadata": {},
   "source": [
    "## 4. Position Encodingã®å®Ÿè£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e2a150",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å±¤\n",
    "    \n",
    "    Sin/Cosé–¢æ•°ã‚’ä½¿ã£ã¦å„ä½ç½®ã«å›ºæœ‰ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆã—ã¾ã™ã€‚\n",
    "    \n",
    "    Args:\n",
    "        d_model: ãƒ¢ãƒ‡ãƒ«ã®æ¬¡å…ƒæ•°\n",
    "        max_len: ã‚µãƒãƒ¼ãƒˆã™ã‚‹æœ€å¤§ç³»åˆ—é•·\n",
    "        dropout: ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¡Œåˆ—ã‚’ä½œæˆ\n",
    "        # shape: [max_len, d_model]\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        \n",
    "        # ä½ç½®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: [0, 1, 2, ..., max_len-1]\n",
    "        # shape: [max_len, 1]\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # åˆ†æ¯ã®è¨ˆç®—: 10000^(2i/d_model)\n",
    "        # shape: [d_model/2]\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        \n",
    "        # å¶æ•°æ¬¡å…ƒã«ã¯sin\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        # å¥‡æ•°æ¬¡å…ƒã«ã¯cos\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # ãƒãƒƒãƒæ¬¡å…ƒã‚’è¿½åŠ : [1, max_len, d_model]\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã¯ãªããƒãƒƒãƒ•ã‚¡ã¨ã—ã¦ç™»éŒ²ï¼ˆå­¦ç¿’ã—ãªã„ï¼‰\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        å…¥åŠ›ã«ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’åŠ ç®—\n",
    "        \n",
    "        Args:\n",
    "            x: å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ« [batch_size, seq_len, d_model]\n",
    "        Returns:\n",
    "            ä½ç½®æƒ…å ±ãŒåŠ ç®—ã•ã‚ŒãŸå…¥åŠ› [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # å…¥åŠ›ã®ç³»åˆ—é•·åˆ†ã ã‘ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å–å¾—ã—ã¦åŠ ç®—\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f029fa",
   "metadata": {},
   "source": [
    "## 5. Position Encodingã®å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfd94c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š\n",
    "d_model = 128\n",
    "max_len = 100\n",
    "\n",
    "# Position Encodingã‚’ä½œæˆ\n",
    "pos_encoding = PositionalEncoding(d_model, max_len, dropout=0.0)\n",
    "\n",
    "# ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¡Œåˆ—ã‚’å–å¾—\n",
    "pe_matrix = pos_encoding.pe[0].numpy()  # [max_len, d_model]\n",
    "\n",
    "print(f\"ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¡Œåˆ—ã®å½¢çŠ¶: {pe_matrix.shape}\")\n",
    "print(f\"  - å„è¡Œ: 1ã¤ã®ä½ç½®ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\")\n",
    "print(f\"  - å„åˆ—: 1ã¤ã®ç‰¹å¾´æ¬¡å…ƒ\")\n",
    "\n",
    "# ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§å¯è¦–åŒ–\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(pe_matrix, cmap='RdBu_r', aspect='auto')\n",
    "plt.colorbar(label='å€¤')\n",
    "plt.xlabel('æ¬¡å…ƒ (d_model)', fontsize=12)\n",
    "plt.ylabel('ä½ç½® (pos)', fontsize=12)\n",
    "plt.title('Position Encoding ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—', fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('position_encoding_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ è¦³å¯Ÿãƒã‚¤ãƒ³ãƒˆ:\")\n",
    "print(\"  - ç¸¦ã®ç¸æ¨¡æ§˜: å„æ¬¡å…ƒãŒç•°ãªã‚‹å‘¨æ³¢æ•°ã®æ³¢ã‚’æŒã¤\")\n",
    "print(\"  - å·¦å´ï¼ˆä½æ¬¡å…ƒï¼‰: ä½å‘¨æ³¢ï¼ˆã‚†ã£ãã‚Šå¤‰åŒ–ï¼‰\")\n",
    "print(\"  - å³å´ï¼ˆé«˜æ¬¡å…ƒï¼‰: é«˜å‘¨æ³¢ï¼ˆé€Ÿãå¤‰åŒ–ï¼‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4b81c2",
   "metadata": {},
   "source": [
    "## 6. ç‰¹å®šã®ä½ç½®ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©³ã—ãè¦‹ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832ac607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¤‡æ•°ã®ä½ç½®ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "positions = [0, 10, 50, 99]\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for pos in positions:\n",
    "    plt.plot(pe_matrix[pos], label=f'ä½ç½® {pos}', alpha=0.7, linewidth=2)\n",
    "\n",
    "plt.xlabel('æ¬¡å…ƒ', fontsize=12)\n",
    "plt.ylabel('ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å€¤', fontsize=12)\n",
    "plt.title('ç•°ãªã‚‹ä½ç½®ã®Position Encoding', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('position_encoding_by_pos.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ’¡ å„ä½ç½®ãŒå›ºæœ‰ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŒã£ã¦ã„ã¾ã™\")\n",
    "print(\"   ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã¯ä½ç½®ã‚’åŒºåˆ¥ã§ãã¾ã™\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af596cf",
   "metadata": {},
   "source": [
    "## 7. ç‰¹å®šã®æ¬¡å…ƒã®å¤‰åŒ–ã‚’è¦‹ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b24bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã„ãã¤ã‹ã®æ¬¡å…ƒã«ã¤ã„ã¦ã€ä½ç½®ã«ã‚ˆã‚‹å¤‰åŒ–ã‚’ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "dimensions = [0, 1, 10, 20, 50, 100]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, dim in enumerate(dimensions):\n",
    "    axes[idx].plot(pe_matrix[:, dim], linewidth=2)\n",
    "    axes[idx].set_title(f'æ¬¡å…ƒ {dim}', fontsize=11)\n",
    "    axes[idx].set_xlabel('ä½ç½®')\n",
    "    axes[idx].set_ylabel('å€¤')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('position_encoding_by_dim.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ’¡ è¦³å¯Ÿ:\")\n",
    "print(\"  - ä½ã„æ¬¡å…ƒï¼ˆ0, 1ï¼‰: é•·ã„å‘¨æœŸã®sin/cosæ³¢\")\n",
    "print(\"  - é«˜ã„æ¬¡å…ƒï¼ˆ50, 100ï¼‰: çŸ­ã„å‘¨æœŸã®æ³¢\")\n",
    "print(\"  - è¤‡æ•°ã®å‘¨æ³¢æ•°ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ä½ç½®ã‚’ä¸€æ„ã«è­˜åˆ¥\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09da7e12",
   "metadata": {},
   "source": [
    "## 8. å®Ÿéš›ã®ä½¿ç”¨ä¾‹: å˜èªåŸ‹ã‚è¾¼ã¿ + Position Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff599301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "d_model = 64\n",
    "\n",
    "# ãƒ©ãƒ³ãƒ€ãƒ ãªå˜èªåŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆï¼ˆå®Ÿéš›ã«ã¯Embeddingå±¤ã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹ï¼‰\n",
    "word_embeddings = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"å˜èªåŸ‹ã‚è¾¼ã¿ã®ã¿:\")\n",
    "print(f\"  å½¢çŠ¶: {word_embeddings.shape}\")\n",
    "print(f\"  æœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«ã€æœ€åˆã®å˜èªã®åŸ‹ã‚è¾¼ã¿ï¼ˆä¸€éƒ¨ï¼‰:\")\n",
    "print(f\"  {word_embeddings[0, 0, :10].numpy()}\")\n",
    "\n",
    "# Position Encodingã‚’é©ç”¨\n",
    "pos_enc = PositionalEncoding(d_model, dropout=0.0)\n",
    "embeddings_with_pos = pos_enc(word_embeddings)\n",
    "\n",
    "print(\"\\nå˜èªåŸ‹ã‚è¾¼ã¿ + Position Encoding:\")\n",
    "print(f\"  å½¢çŠ¶: {embeddings_with_pos.shape}\")\n",
    "print(f\"  æœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«ã€æœ€åˆã®å˜èªã®åŸ‹ã‚è¾¼ã¿ï¼ˆä¸€éƒ¨ï¼‰:\")\n",
    "print(f\"  {embeddings_with_pos[0, 0, :10].detach().numpy()}\")\n",
    "\n",
    "# ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®å¯„ä¸ã‚’ç¢ºèª\n",
    "position_contribution = pos_enc.pe[0, :seq_len, :]\n",
    "print(\"\\nä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®å¯„ä¸ï¼ˆä¸€éƒ¨ï¼‰:\")\n",
    "print(f\"  {position_contribution[0, :10].numpy()}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ å˜èªåŸ‹ã‚è¾¼ã¿ã«ä½ç½®æƒ…å ±ãŒåŠ ç®—ã•ã‚Œã€\")\n",
    "print(\"   åŒã˜å˜èªã§ã‚‚ä½ç½®ã«ã‚ˆã£ã¦ç•°ãªã‚‹è¡¨ç¾ã«ãªã‚Šã¾ã™ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d79c00b",
   "metadata": {},
   "source": [
    "## 9. ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®åŠ¹æœã‚’ç¢ºèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32df31e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŒã˜å˜èªãŒç•°ãªã‚‹ä½ç½®ã«ã‚ã‚‹å ´åˆã‚’æƒ³å®š\n",
    "# ã™ã¹ã¦ã®ä½ç½®ã§åŒã˜åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½¿ç”¨\n",
    "same_word_embedding = torch.randn(1, d_model)\n",
    "repeated_embeddings = same_word_embedding.repeat(1, seq_len, 1)\n",
    "\n",
    "print(\"ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãªã—ï¼ˆåŒã˜å˜èªï¼‰:\")\n",
    "print(f\"  ã™ã¹ã¦ã®ä½ç½®ã§åŒã˜ãƒ™ã‚¯ãƒˆãƒ«\")\n",
    "print(f\"  ä½ç½®0ã¨ä½ç½®5ã®å·®: {(repeated_embeddings[0, 0] - repeated_embeddings[0, 5]).abs().sum().item():.6f}\")\n",
    "\n",
    "# Position Encodingã‚’é©ç”¨\n",
    "with_pos = pos_enc(repeated_embeddings)\n",
    "\n",
    "print(\"\\nä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚ã‚Šï¼ˆåŒã˜å˜èªï¼‰:\")\n",
    "print(f\"  å„ä½ç½®ã§ç•°ãªã‚‹ãƒ™ã‚¯ãƒˆãƒ«\")\n",
    "print(f\"  ä½ç½®0ã¨ä½ç½®5ã®å·®: {(with_pos[0, 0] - with_pos[0, 5]).abs().sum().item():.6f}\")\n",
    "\n",
    "# ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã§ç¢ºèª\n",
    "cos_sim_without = nn.functional.cosine_similarity(\n",
    "    repeated_embeddings[0, 0].unsqueeze(0),\n",
    "    repeated_embeddings[0, 5].unsqueeze(0)\n",
    ")\n",
    "cos_sim_with = nn.functional.cosine_similarity(\n",
    "    with_pos[0, 0].unsqueeze(0),\n",
    "    with_pos[0, 5].unsqueeze(0)\n",
    ")\n",
    "\n",
    "print(f\"\\nã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦:\")\n",
    "print(f\"  ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãªã—: {cos_sim_without.item():.6f} (1.0 = å®Œå…¨ã«åŒã˜)\")\n",
    "print(f\"  ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚ã‚Š: {cos_sim_with.item():.6f}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã«ã‚ˆã‚Šã€åŒã˜å˜èªã§ã‚‚\")\n",
    "print(\"   ä½ç½®ãŒç•°ãªã‚Œã°ç•°ãªã‚‹è¡¨ç¾ã«ãªã‚‹ã“ã¨ãŒç¢ºèªã§ãã¾ã—ãŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bd768b",
   "metadata": {},
   "source": [
    "## 10. ã¾ã¨ã‚\n",
    "\n",
    "### Position Encodingã®å½¹å‰²\n",
    "\n",
    "1. **ä½ç½®æƒ…å ±ã®ä»˜ä¸**: Attentionæ©Ÿæ§‹ã«é †åºæƒ…å ±ã‚’ä¸ãˆã‚‹\n",
    "2. **å›ºå®šãƒ‘ã‚¿ãƒ¼ãƒ³**: å­¦ç¿’ä¸è¦ã®å›ºå®šçš„ãªsin/cosæ³¢ã‚’ä½¿ç”¨\n",
    "3. **åŠ ç®—æ–¹å¼**: å˜èªåŸ‹ã‚è¾¼ã¿ã«åŠ ç®—ã™ã‚‹ã ã‘ã®ã‚·ãƒ³ãƒ—ãƒ«ãªå®Ÿè£…\n",
    "\n",
    "### Sin/Cosé–¢æ•°ã‚’ä½¿ã†ç†ç”±\n",
    "\n",
    "1. **å‘¨æœŸæ€§**: ç•°ãªã‚‹å‘¨æ³¢æ•°ã§ä½ç½®ã®ç›¸å¯¾é–¢ä¿‚ã‚’æ‰ãˆã‚‹\n",
    "2. **å¤–æŒ¿å¯èƒ½**: å­¦ç¿’æ™‚ã‚ˆã‚Šé•·ã„ç³»åˆ—ã«ã‚‚å¯¾å¿œ\n",
    "3. **è¨ˆç®—åŠ¹ç‡**: äº‹å‰è¨ˆç®—ã—ã¦ãƒãƒƒãƒ•ã‚¡ã«ä¿å­˜ã§ãã‚‹\n",
    "\n",
    "### å®Ÿè£…ã®ãƒã‚¤ãƒ³ãƒˆ\n",
    "\n",
    "- å¶æ•°æ¬¡å…ƒã«sinã€å¥‡æ•°æ¬¡å…ƒã«cosã‚’ä½¿ç”¨\n",
    "- `register_buffer`ã§å­¦ç¿’ã—ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦ç™»éŒ²\n",
    "- ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã§æ­£å‰‡åŒ–\n",
    "\n",
    "### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "\n",
    "- Feed Forward Network: Attentionå¾Œã®éç·šå½¢å¤‰æ›\n",
    "- Layer Normalization: å±¤ã®æ­£è¦åŒ–\n",
    "- Encoder Block: Self-Attention + FFN + æ®‹å·®æ¥ç¶š\n",
    "- Decoder Block: Masked Attention + Cross Attention\n",
    "\n",
    "Position Encodingã«ã‚ˆã‚Šã€Transformerã¯å˜èªã®é †åºã‚’ç†è§£ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸï¼"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
