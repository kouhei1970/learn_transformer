{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01e7dd5a",
   "metadata": {},
   "source": [
    "# Multi-Head Attention ã®å®Ÿè£…ã¨ç†è§£\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€Multi-Head Attentionæ©Ÿæ§‹ã‚’å®Ÿè£…ã—ã€ãã®å‹•ä½œã‚’ç†è§£ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf67635",
   "metadata": {},
   "source": [
    "## 1. Multi-Head Attentionã¨ã¯\n",
    "\n",
    "Multi-Head Attentionã¯ã€Self-Attentionã‚’è¤‡æ•°ä¸¦åˆ—ã«å®Ÿè¡Œã™ã‚‹ä»•çµ„ã¿ã§ã™ã€‚\n",
    "\n",
    "### ä¸»ãªç‰¹å¾´:\n",
    "\n",
    "1. **è¤‡æ•°ã®Attention Head**: ç•°ãªã‚‹è¡¨ç¾éƒ¨åˆ†ç©ºé–“ã‹ã‚‰æƒ…å ±ã‚’æ‰ãˆã‚‹\n",
    "2. **ä¸¦åˆ—å‡¦ç†**: å„headã¯ç‹¬ç«‹ã—ã¦è¨ˆç®—ã•ã‚Œã‚‹\n",
    "3. **çµåˆã¨å¤‰æ›**: å…¨headã®å‡ºåŠ›ã‚’çµåˆã—ã€ç·šå½¢å¤‰æ›ã™ã‚‹\n",
    "\n",
    "### æ•°å¼:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O\n",
    "$$\n",
    "\n",
    "å„headã¯:\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "$$\n",
    "\n",
    "### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:\n",
    "\n",
    "- $h$: headæ•°(é€šå¸¸8å€‹)\n",
    "- $d_{\\text{model}}$: ãƒ¢ãƒ‡ãƒ«ã®æ¬¡å…ƒ(ä¾‹: 512)\n",
    "- $d_k = d_v = d_{\\text{model}} / h$: å„headã®æ¬¡å…ƒ(ä¾‹: 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1f289f",
   "metadata": {},
   "source": [
    "## 2. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f25e683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'Hiragino Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468552a6",
   "metadata": {},
   "source": [
    "## 3. Multi-Head Attentionã®å®Ÿè£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2e57bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attentionæ©Ÿæ§‹ã®å®Ÿè£…\n",
    "    \n",
    "    Args:\n",
    "        d_model: ãƒ¢ãƒ‡ãƒ«ã®æ¬¡å…ƒæ•°\n",
    "        num_heads: Attention headã®æ•°\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        # d_modelãŒnum_headsã§å‰²ã‚Šåˆ‡ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # å„headã®æ¬¡å…ƒ\n",
    "        \n",
    "        # Q, K, Vç”¨ã®ç·šå½¢å¤‰æ›å±¤ï¼ˆå…¨headã‚’ã¾ã¨ã‚ã¦å‡¦ç†ï¼‰\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        # å‡ºåŠ›ç”¨ã®ç·šå½¢å¤‰æ›å±¤\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        å…¥åŠ›ã‚’è¤‡æ•°ã®headã«åˆ†å‰²\n",
    "        \n",
    "        Args:\n",
    "            x: shape [batch_size, seq_len, d_model]\n",
    "        Returns:\n",
    "            shape [batch_size, num_heads, seq_len, d_k]\n",
    "        \"\"\"\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 2)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Multi-Head Attentionã®é †ä¼æ’­\n",
    "        \n",
    "        Args:\n",
    "            query: shape [batch_size, seq_len, d_model]\n",
    "            key: shape [batch_size, seq_len, d_model]\n",
    "            value: shape [batch_size, seq_len, d_model]\n",
    "            mask: Optional mask\n",
    "        Returns:\n",
    "            output: shape [batch_size, seq_len, d_model]\n",
    "            attention_weights: shape [batch_size, num_heads, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # 1. ç·šå½¢å¤‰æ›: [batch, seq_len, d_model]\n",
    "        Q = self.W_q(query)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "        \n",
    "        # 2. è¤‡æ•°headã«åˆ†å‰²: [batch, num_heads, seq_len, d_k]\n",
    "        Q = self.split_heads(Q, batch_size)\n",
    "        K = self.split_heads(K, batch_size)\n",
    "        V = self.split_heads(V, batch_size)\n",
    "        \n",
    "        # 3. Scaled Dot-Product Attention\n",
    "        # scores: [batch, num_heads, seq_len, seq_len]\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # ãƒã‚¹ã‚¯ã®é©ç”¨ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Attentioné‡ã¿: [batch, num_heads, seq_len, seq_len]\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Attentionã®é©ç”¨: [batch, num_heads, seq_len, d_k]\n",
    "        attention_output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # 4. å…¨headã‚’çµåˆ: [batch, seq_len, d_model]\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous()\n",
    "        attention_output = attention_output.view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # 5. å‡ºåŠ›ç·šå½¢å¤‰æ›\n",
    "        output = self.W_o(attention_output)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01761d2",
   "metadata": {},
   "source": [
    "## 4. å‹•ä½œç¢ºèª\n",
    "\n",
    "Multi-Head Attentionã®åŸºæœ¬çš„ãªå‹•ä½œã‚’ç¢ºèªã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d029f98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "# Multi-Head Attentionã®ä½œæˆ\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# ãƒ©ãƒ³ãƒ€ãƒ ãªå…¥åŠ›ã‚’ä½œæˆ\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# é †ä¼æ’­\n",
    "output, attention_weights = mha(x, x, x)\n",
    "\n",
    "print(\"å…¥åŠ›ã®å½¢çŠ¶:\", x.shape)\n",
    "print(\"å‡ºåŠ›ã®å½¢çŠ¶:\", output.shape)\n",
    "print(\"Attentioné‡ã¿ã®å½¢çŠ¶:\", attention_weights.shape)\n",
    "print(f\"\\nnum_heads: {num_heads}\")\n",
    "print(f\"d_k (å„headã®æ¬¡å…ƒ): {mha.d_k}\")\n",
    "print(f\"d_model: {d_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3817d4d5",
   "metadata": {},
   "source": [
    "## 5. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®ç¢ºèª\n",
    "\n",
    "Multi-Head Attentionã®å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç¢ºèªã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecd228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Multi-Head Attentionã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total_params = 0\n",
    "for name, param in mha.named_parameters():\n",
    "    num_params = param.numel()\n",
    "    total_params += num_params\n",
    "    print(f\"{name:20s}: shape {str(list(param.shape)):20s} = {num_params:,} params\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}\")\n",
    "print(f\"\\nè¨ˆç®—å¼: 4 Ã— (d_model Ã— d_model) = 4 Ã— ({d_model} Ã— {d_model}) = {4 * d_model * d_model:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c30ea23",
   "metadata": {},
   "source": [
    "## 6. Attentioné‡ã¿ã®å¯è¦–åŒ–\n",
    "\n",
    "å„headãŒã©ã®ã‚ˆã†ã«ç•°ãªã‚‹æƒ…å ±ã‚’æ‰ãˆã¦ã„ã‚‹ã‹ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623ea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç°¡å˜ãªä¾‹ã§å¯è¦–åŒ–\n",
    "d_model_small = 64\n",
    "num_heads_small = 4\n",
    "seq_len_small = 8\n",
    "\n",
    "mha_small = MultiHeadAttention(d_model_small, num_heads_small)\n",
    "x_small = torch.randn(1, seq_len_small, d_model_small)\n",
    "\n",
    "output_small, attention_weights_small = mha_small(x_small, x_small, x_small)\n",
    "\n",
    "# å„headã®Attentioné‡ã¿ã‚’å¯è¦–åŒ–\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(num_heads_small):\n",
    "    # [seq_len, seq_len]\n",
    "    attn = attention_weights_small[0, i].detach().numpy()\n",
    "    \n",
    "    im = axes[i].imshow(attn, cmap='viridis', aspect='auto')\n",
    "    axes[i].set_title(f'Head {i+1}')\n",
    "    axes[i].set_xlabel('Keyä½ç½®')\n",
    "    axes[i].set_ylabel('Queryä½ç½®')\n",
    "    plt.colorbar(im, ax=axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('multi_head_attention_weights.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"å„headãŒç•°ãªã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ³¨æ„ã‚’å­¦ç¿’ã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669f43ac",
   "metadata": {},
   "source": [
    "## 7. Multi-Headã¨Single-Headã®æ¯”è¼ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e537bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŒã˜d_modelã§headæ•°ã‚’å¤‰ãˆã¦æ¯”è¼ƒ\n",
    "d_model_comp = 64\n",
    "\n",
    "# Single-Head (num_heads=1)\n",
    "mha_single = MultiHeadAttention(d_model_comp, num_heads=1)\n",
    "single_params = sum(p.numel() for p in mha_single.parameters())\n",
    "\n",
    "# Multi-Head (num_heads=8)\n",
    "mha_multi = MultiHeadAttention(d_model_comp, num_heads=8)\n",
    "multi_params = sum(p.numel() for p in mha_multi.parameters())\n",
    "\n",
    "print(\"Single-Head vs Multi-Head ã®æ¯”è¼ƒ:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Single-Head (num_heads=1):\")\n",
    "print(f\"  - d_k = {mha_single.d_k}\")\n",
    "print(f\"  - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {single_params:,}\")\n",
    "print()\n",
    "print(f\"Multi-Head (num_heads=8):\")\n",
    "print(f\"  - d_k = {mha_multi.d_k}\")\n",
    "print(f\"  - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {multi_params:,}\")\n",
    "print()\n",
    "print(f\"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¯åŒã˜: {single_params == multi_params}\")\n",
    "print(\"\\nğŸ’¡ é‡è¦: Multi-Headã§ã‚‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¯å¢—ãˆã¾ã›ã‚“!\")\n",
    "print(\"   å„headã®d_kãŒå°ã•ããªã‚‹ãŸã‚ã€ç·è¨ˆç®—é‡ã¯åŒã˜ã§ã™ã€‚\")\n",
    "print(\"   ã—ã‹ã—ã€ç•°ãªã‚‹è¡¨ç¾éƒ¨åˆ†ç©ºé–“ã‹ã‚‰æƒ…å ±ã‚’å­¦ç¿’ã§ãã¾ã™ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c20661",
   "metadata": {},
   "source": [
    "## 8. ã¾ã¨ã‚\n",
    "\n",
    "### Multi-Head Attentionã®åˆ©ç‚¹:\n",
    "\n",
    "1. **è¤‡æ•°ã®è¡¨ç¾éƒ¨åˆ†ç©ºé–“**: ç•°ãªã‚‹ç¨®é¡ã®æƒ…å ±ï¼ˆä½ç½®ã€æ„å‘³ã€æ–‡æ³•ãªã©ï¼‰ã‚’ä¸¦åˆ—ã«å­¦ç¿’\n",
    "2. **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡**: headæ•°ã‚’å¢—ã‚„ã—ã¦ã‚‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¯å¤‰ã‚ã‚‰ãªã„\n",
    "3. **è¡¨ç¾åŠ›ã®å‘ä¸Š**: å˜ä¸€ã®Attentionã‚ˆã‚Šã‚‚è±Šã‹ãªè¡¨ç¾ã‚’å­¦ç¿’\n",
    "\n",
    "### å®Ÿè£…ã®ãƒã‚¤ãƒ³ãƒˆ:\n",
    "\n",
    "1. `d_model % num_heads == 0` ãŒå¿…è¦\n",
    "2. å„headã®æ¬¡å…ƒ: `d_k = d_model / num_heads`\n",
    "3. å…¨headã‚’ã¾ã¨ã‚ã¦å‡¦ç†ã™ã‚‹ã“ã¨ã§åŠ¹ç‡åŒ–\n",
    "4. æœ€å¾Œã«å‡ºåŠ›ã‚’çµåˆã—ã¦ç·šå½¢å¤‰æ›\n",
    "\n",
    "### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:\n",
    "\n",
    "- Position Encoding: ç³»åˆ—ã®ä½ç½®æƒ…å ±ã‚’åŸ‹ã‚è¾¼ã‚€\n",
    "- Feed Forward Network: Attentionå¾Œã®éç·šå½¢å¤‰æ›\n",
    "- Encoder/Decoderãƒ–ãƒ­ãƒƒã‚¯ã®æ§‹ç¯‰"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
