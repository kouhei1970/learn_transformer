{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Q&A Part 3: アーキテクチャ（Position Encoding, FFN, Encoder, Decoder）\n\nTransformer学習中の質問と回答集\n\n- Q17-Q18: Position Encoding\n- Q19-Q20: Feed Forward Network\n- Q21-Q23: 残差接続（Residual Connection）\n- Q24: Encoder出力とCross-Attention\n- Q25: Encoder Layerの直列接続\n- Q26-Q27: 文脈を考慮した表現\n- Q28: 特徴量ベクトルから単語への変換\n- Q29-Q30: 翻訳タスクとTeacher Forcing\n- Q31: シーケンス長の違いとCross-Attention"
  },
  {
   "cell_type": "markdown",
   "id": "d1codjr7h5w",
   "source": "---\n\n## Q17: Position Encodingは特徴量ベクトルの何次元を使うのか？\n\n### 質問\nPosition Encodingは特徴量ベクトルの中で数次元を使うのですか？1次元だと三角関数なので、同じ位置を一意に決定できないと思います。\n\n### 回答\n\n**d_model次元すべてを使います。** 1次元では周期的なので位置を一意に特定できませんが、**複数の異なる周波数を組み合わせる**ことで一意に特定できます。\n\n#### 数式の確認\n\n```\nPE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\nPE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n```\n\n- `i = 0, 1, 2, ..., d_model/2 - 1`\n- 各iに対してsin/cosのペアがある\n- つまり **d_model次元すべて** がPosition Encodingに使われる\n\n#### なぜ複数次元で一意になるのか？\n\n**二進数との類似**で理解できます：\n\n| 二進数 | Position Encoding |\n|--------|-------------------|\n| 各ビットは0か1の2値 | 各次元は-1〜1のsin/cos |\n| 低位ビット: 速く変化 | 低次元(i小): 高周波 |\n| 高位ビット: 遅く変化 | 高次元(i大): 低周波 |\n| 複数ビットで多くの数を表現 | 複数次元で多くの位置を表現 |\n\n#### 具体例（d_model=64の場合）\n\n```\n次元0,1 (i=0):  周期 ≈ 6.28   (高周波、近い位置を区別)\n次元16,17 (i=8): 周期 ≈ 158   (中周波)\n次元32,33 (i=16): 周期 ≈ 3969  (低周波)\n次元62,63 (i=31): 周期 ≈ 62832 (最低周波、遠い位置を区別)\n```\n\n各次元が異なる「粒度」で位置を見ている：\n- **低次元**: 隣り合う位置の違いを捉える（細かい目盛り）\n- **高次元**: 大きく離れた位置の違いを捉える（粗い目盛り）\n\n#### 視覚的理解\n\n```\n位置0:  [sin(0), cos(0), sin(0), cos(0), ...]\n位置1:  [sin(1), cos(1), sin(0.1), cos(0.1), ...]  ← 各次元で異なる変化\n位置2:  [sin(2), cos(2), sin(0.2), cos(0.2), ...]\n\n        ↓高周波次元     ↓低周波次元\n        大きく変化      ゆっくり変化\n```\n\n**64次元の組み合わせパターン**は膨大なので、実用上十分な数の位置を一意に表現できます。\n\n#### コードで確認",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ojmgga2owc",
   "source": "# Position Encodingが全次元を使うことを確認\nimport torch\nimport math\n\nd_model = 64\nmax_len = 100\n\n# Position Encodingを計算\npe = torch.zeros(max_len, d_model)\nposition = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\ndiv_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n\npe[:, 0::2] = torch.sin(position * div_term)  # 偶数次元: sin\npe[:, 1::2] = torch.cos(position * div_term)  # 奇数次元: cos\n\nprint(f\"Position Encoding shape: {pe.shape}\")\nprint(f\"→ 各位置に対して{d_model}次元すべてを使用\\n\")\n\n# 各次元ペアの周期を確認\nprint(\"各次元ペアの周期（position単位）:\")\nprint(\"-\" * 40)\nfor i in [0, 8, 16, 24, 31]:\n    period = 2 * math.pi / div_term[i].item()\n    print(f\"次元 {2*i:2d},{2*i+1:2d} (i={i:2d}): 周期 ≈ {period:8.1f}\")\n\nprint(\"\\n→ 異なる周波数の組み合わせで位置を一意に特定\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "2sxvmwul273",
   "source": "# 各位置のPEが一意であることを確認\n# 異なる位置のPE間のユークリッド距離を計算\n\nprint(\"位置間のユークリッド距離:\")\nprint(\"-\" * 40)\nfor pos1, pos2 in [(0, 1), (0, 10), (0, 50), (10, 11), (50, 51)]:\n    distance = (pe[pos1] - pe[pos2]).norm().item()\n    print(f\"位置 {pos1:2d} と 位置 {pos2:2d}: 距離 = {distance:.4f}\")\n\nprint(\"\\n→ すべての位置ペアで距離 > 0（一意に区別可能）\")\n\n# 同じ位置は距離0\nsame_pos_distance = (pe[5] - pe[5]).norm().item()\nprint(f\"\\n位置 5 と 位置 5: 距離 = {same_pos_distance:.10f}（同一）\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c32aouv1cb",
   "source": "#### まとめ\n\n**Q: Position Encodingは特徴量ベクトルの何次元を使うのか？**\n\n**A: d_model次元すべてを使います。**\n\n##### ポイント\n\n1. **1次元では不十分**: sin(pos)だけでは周期的で、位置0と位置2πが区別できない\n\n2. **複数周波数の組み合わせ**: 各次元ペアが異なる周波数を持ち、組み合わせで一意に\n\n3. **二進数に似た構造**:\n   - 低次元 = 低位ビット（速く変化、近い位置を区別）\n   - 高次元 = 高位ビット（遅く変化、遠い位置を区別）\n\n4. **埋め込みベクトルへの加算**: Position Encodingはトークン埋め込みに**加算**される\n   ```\n   入力 = トークン埋め込み(d_model次元) + Position Encoding(d_model次元)\n   ```\n\n##### 注意\nPosition Encodingは「数次元を専用に使う」のではなく、**全次元に位置情報を重畳**します。トークンの意味情報と位置情報が同じ空間で混ざり合い、Attentionで両方を活用できます。",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "160e5p0q3t9",
   "source": "---\n\n## Q18: Position Encodingは特徴量ベクトルの全要素に加法的に加わるのか？\n\n### 質問\n特徴量ベクトルの全ての要素に加法的に位置情報が加わるということですか？\n\n### 回答\n\n**はい、その通りです。** 各要素ごとに加算されます。\n\n#### 具体的な計算\n\n```python\n入力 = トークン埋め込み + Position Encoding\n\n# 各要素ごとの加算（element-wise addition）\n[e₀, e₁, e₂, ..., e₆₃]      # トークン埋め込み（意味情報）\n         +\n[pe₀, pe₁, pe₂, ..., pe₆₃]  # Position Encoding（位置情報）\n         ↓\n[e₀+pe₀, e₁+pe₁, e₂+pe₂, ..., e₆₃+pe₆₃]  # 最終的な入力\n```\n\n#### なぜ加算で問題ないのか？\n\n意味情報と位置情報が混ざっても問題にならない理由：\n\n| 懸念 | 解決策 |\n|------|--------|\n| 情報が混ざって区別できない？ | Attentionが学習を通じて分離を学ぶ |\n| 情報が上書きされない？ | 高次元空間（64〜512次元）は十分広い |\n| 連結(concat)の方が良い？ | 加算の方がパラメータ効率的 |\n\n#### 加算 vs 連結の比較\n\n```\n【連結（concat）の場合】\nトークン埋め込み: [e₀, e₁, ..., e₆₃]        (64次元)\nPosition Encoding: [pe₀, pe₁, ..., pe₆₃]    (64次元)\n結果: [e₀, ..., e₆₃, pe₀, ..., pe₆₃]        (128次元) ← 次元が2倍に！\n\n【加算（add）の場合】\n結果: [e₀+pe₀, e₁+pe₁, ..., e₆₃+pe₆₃]       (64次元) ← 次元そのまま\n```\n\n加算を使うことで：\n- モデルサイズを抑えられる\n- 計算コストが低い\n- 実験的にも性能は十分\n\n#### コードで確認",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "2hc6jdt0b2d",
   "source": "# 加法的に位置情報が加わることを確認\nimport torch\nimport math\n\nd_model = 8  # 見やすくするため小さい次元で\nseq_len = 3\n\n# トークン埋め込み（ダミー）\ntoken_embedding = torch.tensor([\n    [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],  # 位置0のトークン\n    [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],  # 位置1のトークン\n    [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5],  # 位置2のトークン\n])\n\n# Position Encodingを計算\npe = torch.zeros(seq_len, d_model)\nposition = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\ndiv_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\npe[:, 0::2] = torch.sin(position * div_term)\npe[:, 1::2] = torch.cos(position * div_term)\n\n# 加算\ninput_with_pe = token_embedding + pe\n\nprint(\"トークン埋め込み（位置0）:\")\nprint(token_embedding[0].numpy())\nprint(\"\\nPosition Encoding（位置0）:\")\nprint(pe[0].numpy())\nprint(\"\\n加算後（位置0）:\")\nprint(input_with_pe[0].numpy())\nprint(\"\\n確認: 各要素が element-wise に加算されている\")\nprint(f\"  {token_embedding[0, 0]:.2f} + {pe[0, 0]:.2f} = {input_with_pe[0, 0]:.2f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "81xkxpfo572",
   "source": "#### まとめ\n\n**Q: Position Encodingは特徴量ベクトルの全要素に加法的に加わるのか？**\n\n**A: はい、全要素に対してelement-wiseに加算されます。**\n\n##### 重要なポイント\n\n1. **加算（+）演算**: `入力 = embedding + position_encoding`\n\n2. **次元は変わらない**: d_model次元のまま\n\n3. **情報の混合は問題ない**: \n   - 高次元空間には十分な余裕がある\n   - Attentionが意味と位置を使い分けることを学習する\n\n4. **実装上のシンプルさ**: \n   ```python\n   x = x + self.pe[:, :seq_len, :]  # これだけ！\n   ```\n\n##### なぜ連結(concat)ではなく加算なのか\n\n- **効率性**: 次元数が2倍にならない\n- **実験結果**: 加算でも十分な性能\n- **論文の選択**: 元のTransformer論文で加算が採用された",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "j5w43y3qgt",
   "source": "---\n\n## Q19: FFNの数式は「フィードフォワード」を示しているのか？\n\n### 質問\nFFNの数式はフィードフォワードを示していますか？バイアス付きの線形変換にしか見えませんが。\n\n### 回答\n\n**その観察は正しいです。** FFNの中身は「2つの線形変換 + 活性化関数」であり、特別なものではありません。\n\n#### 数式を分解する\n\n$$\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2$$\n\nこれは：\n1. **線形変換1**: $xW_1 + b_1$\n2. **活性化関数**: $\\max(0, \\cdot)$（ReLU）\n3. **線形変換2**: $(\\cdot)W_2 + b_2$\n\nつまり、**2層のMLP（多層パーセプトロン）** そのものです。\n\n#### 「フィードフォワード」の意味\n\n「Feed Forward」はネットワークの**構造**を表す用語で、**情報の流れ方**を指しています：\n\n| 用語 | 意味 | 例 |\n|------|------|-----|\n| **Feed Forward** | 入力→出力へ一方向に流れる | MLP, CNN |\n| **Recurrent** | 出力が入力に戻る（ループ） | RNN, LSTM |\n| **Feedback** | 後の層から前の層へ情報が戻る | 一部の特殊なネット |\n\n```\nFeed Forward（順伝播）:\n入力 → 層1 → 層2 → 出力\n      →→→→→→→→→→→\n      （一方向のみ）\n\nRecurrent（再帰）:\n入力 → 層 → 出力\n       ↑_____|\n      （ループあり）\n```\n\n#### なぜ「FFN」と呼ぶのか\n\nTransformerの文脈では、**Attentionと区別するため**にこの名前が使われています：\n\n| コンポーネント | 特徴 |\n|---------------|------|\n| **Attention** | トークン間で情報を交換（相互作用あり） |\n| **FFN** | 各トークンを独立に変換（相互作用なし） |\n\n```\nAttention:\nトークン1 ←→ トークン2 ←→ トークン3\n    （トークン間で情報交換）\n\nFFN:\nトークン1 → 変換 → 出力1\nトークン2 → 変換 → 出力2  （同じ変換を独立に適用）\nトークン3 → 変換 → 出力3\n```\n\n#### 本質的には何か\n\n| 別名 | 説明 |\n|------|------|\n| MLP | Multi-Layer Perceptron（多層パーセプトロン） |\n| 全結合層 | Fully Connected Layer |\n| Dense層 | Kerasでの呼び方 |\n| Position-wise FFN | Transformerでの正式名称（位置ごとのFFN） |\n\nすべて同じものを指しています。\n\n#### まとめ\n\n**Q: FFNの数式は「フィードフォワード」を示しているのか？**\n\n**A: はい。ただし「フィードフォワード」は中身ではなく、情報の流れ方（一方向）を表す用語です。**\n\n- 中身は「2層MLP」（線形→活性化→線形）\n- 「フィードフォワード」= 再帰やフィードバックがない構造\n- Transformerでは「Attentionと対比する」ためにこの名前を使用\n- 各トークンに独立に適用されるため「Position-wise FFN」とも呼ばれる",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "2q4l16zfd8m",
   "source": "---\n\n## Q20: 「1層あたりのパラメータ配分」の「1層」とは？\n\n### 質問\n1層あたりのパラメータ配分の「1層」とはどういうことですか？\n\n### 回答\n\n**Transformerにおける「1層」とは、Attention + FFN のセット**を指します。\n\n#### Transformerの構造\n\n```\n┌─────────────────────────────────────┐\n│           Transformer               │\n│                                     │\n│  ┌─────────────┐  ┌─────────────┐  │\n│  │   Encoder   │  │   Decoder   │  │\n│  │             │  │             │  │\n│  │  ┌───────┐  │  │  ┌───────┐  │  │\n│  │  │ 層 6  │  │  │  │ 層 6  │  │  │\n│  │  ├───────┤  │  │  ├───────┤  │  │\n│  │  │ 層 5  │  │  │  │ 層 5  │  │  │\n│  │  ├───────┤  │  │  ├───────┤  │  │\n│  │  │  ...  │  │  │  │  ...  │  │  │\n│  │  ├───────┤  │  │  ├───────┤  │  │\n│  │  │ 層 1  │  │  │  │ 層 1  │  │  │\n│  │  └───────┘  │  │  └───────┘  │  │\n│  └─────────────┘  └─────────────┘  │\n└─────────────────────────────────────┘\n```\n\n元の論文では：\n- Encoder: 6層\n- Decoder: 6層\n\n#### 「1層」の中身（Encoder層の場合）\n\n```\n┌────────────────────────────────┐\n│          Encoder 1層           │\n│                                │\n│  入力                          │\n│    ↓                          │\n│  ┌──────────────────────────┐ │\n│  │  Multi-Head Attention    │ │  ← パラメータの約1/3\n│  └──────────────────────────┘ │\n│    ↓                          │\n│  ┌──────────────────────────┐ │\n│  │  Feed Forward Network    │ │  ← パラメータの約2/3\n│  └──────────────────────────┘ │\n│    ↓                          │\n│  出力                          │\n└────────────────────────────────┘\n```\n\n（※ 実際にはLayer NormalizationとResidual Connectionも含まれます）\n\n#### パラメータ数の内訳\n\nd_model = 512, d_ff = 2048, num_heads = 8 の場合：\n\n| コンポーネント | パラメータ数 | 割合 |\n|---------------|-------------|------|\n| Multi-Head Attention | 約100万 | ~33% |\n| Feed Forward Network | 約210万 | ~67% |\n| **1層の合計** | **約310万** | 100% |\n\n#### 層を積み重ねる理由\n\n```\n入力 → 層1 → 層2 → 層3 → ... → 層N → 出力\n```\n\n- 各層で少しずつ抽象的な表現を獲得\n- 浅い層: 表面的なパターン（文法など）\n- 深い層: 意味的な理解（文脈など）\n- 層を増やすほど表現力が上がる（ただし計算コストも増加）\n\n#### 具体例\n\n| モデル | 層数 | d_model | パラメータ総数 |\n|--------|------|---------|---------------|\n| Transformer (論文) | 6層 | 512 | ~65M |\n| BERT-base | 12層 | 768 | ~110M |\n| GPT-2 | 12層 | 768 | ~117M |\n| GPT-3 | 96層 | 12288 | ~175B |\n\n#### まとめ\n\n**Q: 「1層あたりのパラメータ配分」の「1層」とは？**\n\n**A: Attention + FFN のセット（= 1つのEncoder/Decoderブロック）を指します。**\n\n- Transformerは複数の「層」を積み重ねた構造\n- 各層の中に Attention と FFN が含まれる\n- 層を増やすほどモデルの表現力が向上",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "13c53921",
   "metadata": {},
   "source": [
    "# Transformer学習 Q&A\n",
    "\n",
    "このノートブックは、Transformerの実装学習中に出た質問と回答をまとめたものです。\n",
    "\n",
    "各Q&Aには、質問内容、回答、実際に動かせるコード例が含まれています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q21: Add & Norm の「Add」とは何か？\n",
    "\n",
    "**質問日**: 2025年12月21日\n",
    "\n",
    "### 質問\n",
    "\n",
    "Encoderの構造で「Add & Norm」という表記がありますが、「Add」の部分は何をしているのですか？\n",
    "\n",
    "### 回答\n",
    "\n",
    "「Add」は**残差接続（Residual Connection）**のことです。\n",
    "\n",
    "#### 構造\n",
    "\n",
    "```\n",
    "        入力 x\n",
    "          │\n",
    "          ├──────────────┐\n",
    "          ↓              │\n",
    "    ┌──────────┐         │\n",
    "    │ Sublayer │         │  ← Attention または FFN\n",
    "    └────┬─────┘         │\n",
    "         ↓               │\n",
    "       F(x)              │\n",
    "         │               │\n",
    "         ↓               │\n",
    "        (+) ←────────────┘   ← Add（残差接続）: x + F(x)\n",
    "         │\n",
    "         ↓\n",
    "    ┌──────────┐\n",
    "    │LayerNorm │             ← Norm\n",
    "    └────┬─────┘\n",
    "         ↓\n",
    "       出力\n",
    "```\n",
    "\n",
    "#### 数式\n",
    "\n",
    "```\n",
    "output = LayerNorm(x + Sublayer(x))\n",
    "                   ↑\n",
    "                  Add\n",
    "```\n",
    "\n",
    "#### なぜ Add（残差接続）が必要か\n",
    "\n",
    "| 理由 | 説明 |\n",
    "|------|------|\n",
    "| **勾配の流れ** | 深い層でも勾配が消失しにくい（ショートカットパス） |\n",
    "| **恒等写像** | `F(x) = 0` を学習すれば入力をそのまま出力できる |\n",
    "| **学習の安定化** | 各層の変化を小さく保てる |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q22: なぜ「残差」と呼ぶのか？英語では何と表現する？\n",
    "\n",
    "**質問日**: 2025年12月21日\n",
    "\n",
    "### 質問\n",
    "\n",
    "「残差接続」の「残差」とはどういう意味ですか？英語では何と言いますか？\n",
    "\n",
    "### 回答\n",
    "\n",
    "#### 英語での表現\n",
    "\n",
    "**Residual Connection** または **Skip Connection**\n",
    "\n",
    "#### なぜ「残差」と呼ぶか\n",
    "\n",
    "元々は **ResNet（Residual Network, 2015）** で導入された概念です。\n",
    "\n",
    "##### 考え方\n",
    "\n",
    "通常のネットワークは目標の関数 `H(x)` を直接学習しようとします：\n",
    "\n",
    "```\n",
    "出力 = H(x)\n",
    "```\n",
    "\n",
    "しかしResNetでは、**入力との差分（残差）** を学習します：\n",
    "\n",
    "```\n",
    "F(x) = H(x) - x    ← これが「残差」(Residual)\n",
    "```\n",
    "\n",
    "つまり：\n",
    "\n",
    "```\n",
    "H(x) = F(x) + x\n",
    "出力 = F(x) + x\n",
    "       ↑      ↑\n",
    "     残差   入力をそのまま加算\n",
    "```\n",
    "\n",
    "##### なぜ「残差」なのか\n",
    "\n",
    "| 用語 | 意味 |\n",
    "|------|------|\n",
    "| **Residual** | 「残り」「差分」という意味 |\n",
    "| **残差** | 目標値と入力の差 = `H(x) - x` |\n",
    "\n",
    "ネットワークが学習するのは「入力からどれだけ変化させるべきか」という**差分**だけ。\n",
    "何も変化させる必要がなければ `F(x) = 0` を学習すればよい。\n",
    "\n",
    "##### 統計学での「残差」\n",
    "\n",
    "回帰分析でも同じ用語を使います：\n",
    "```\n",
    "残差 = 実測値 - 予測値\n",
    "```\n",
    "\n",
    "これと同じ発想で、「目標 - 入力 = 残差」を学習するという意味です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q23: ResNetとは何か？\n",
    "\n",
    "**質問日**: 2025年12月21日\n",
    "\n",
    "### 質問\n",
    "\n",
    "ResNetとは簡単に言うと何ですか？\n",
    "\n",
    "### 回答\n",
    "\n",
    "**2015年にMicrosoftが発表した画像認識のためのディープニューラルネットワーク**\n",
    "\n",
    "#### 解決した問題\n",
    "\n",
    "それまでの課題：層を深くすると精度が**下がる**（勾配消失・劣化問題）\n",
    "\n",
    "```\n",
    "層を増やす → 精度向上 → ある点から精度低下 😢\n",
    "```\n",
    "\n",
    "#### ResNetの解決策\n",
    "\n",
    "**残差接続（Skip Connection）を導入**\n",
    "\n",
    "```\n",
    "従来:    x → [層] → 出力\n",
    "\n",
    "ResNet:  x → [層] → (+) → 出力\n",
    "         └─────────↗\n",
    "            ショートカット\n",
    "```\n",
    "\n",
    "#### 結果\n",
    "\n",
    "| 項目 | 内容 |\n",
    "|------|------|\n",
    "| 層数 | 152層まで積み重ね可能に（従来は20層程度が限界） |\n",
    "| 成果 | ImageNet 2015で優勝、人間の精度を超えた |\n",
    "| 影響 | Transformerを含む現代のほぼ全てのディープラーニングで採用 |\n",
    "\n",
    "#### 一言でまとめると\n",
    "\n",
    "> **「入力をそのまま出力に足す」という単純なアイデアで、超深いネットワークを可能にした**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q24: Encoderの出力はどういう意味があるのか？Decoderでどう使われるのか？\n",
    "\n",
    "**質問日**: 2025年12月21日\n",
    "\n",
    "### 質問\n",
    "\n",
    "Encoderの出力自体では損失を計算しないので、本当に意味のある学習がなされるのですか？\n",
    "Decoderでどのように使われますか？\n",
    "\n",
    "### 回答\n",
    "\n",
    "#### 1. Encoder出力とは何か\n",
    "\n",
    "Encoderの出力は、入力シーケンスの**文脈を考慮した表現（Contextual Representation）**です。\n",
    "\n",
    "```\n",
    "入力: [\"I\", \"love\", \"cats\"]\n",
    "        ↓\n",
    "     Encoder\n",
    "        ↓\n",
    "出力: [h₁, h₂, h₃]  ← 各トークンが他のトークンの情報を含む\n",
    "```\n",
    "\n",
    "例えば「bank」という単語：\n",
    "- 「river bank」→ 川岸の意味を反映した表現\n",
    "- 「money bank」→ 銀行の意味を反映した表現\n",
    "\n",
    "#### 2. Decoderでの使われ方：Cross-Attention\n",
    "\n",
    "```\n",
    "Decoder側\n",
    "    ↓\n",
    "┌─────────────────────────────────┐\n",
    "│  Cross-Attention                │\n",
    "│  Q = Decoder の状態             │\n",
    "│  K, V = Encoder の出力 ←────────│── ここで使う！\n",
    "└─────────────────────────────────┘\n",
    "```\n",
    "\n",
    "翻訳タスクの例（英→日）：\n",
    "```\n",
    "Encoder入力: \"I love cats\"\n",
    "Encoder出力: [h_I, h_love, h_cats]\n",
    "\n",
    "Decoder生成時:\n",
    "  \"私\" を生成 → h_I に強くAttend\n",
    "  \"は\" を生成 → h_I に強くAttend  \n",
    "  \"猫\" を生成 → h_cats に強くAttend\n",
    "  \"が\" を生成 → h_cats に強くAttend\n",
    "  \"好き\" を生成 → h_love に強くAttend\n",
    "```\n",
    "\n",
    "#### 3. Encoderは本当に学習するのか？\n",
    "\n",
    "**はい、学習します。** 鍵は**誤差逆伝播（Backpropagation）**です。\n",
    "\n",
    "```\n",
    "順伝播:\n",
    "  入力 → Encoder → Encoder出力 → Decoder → 予測\n",
    "\n",
    "逆伝播:\n",
    "  損失 ← Decoder ← Cross-Attention ← Encoder出力 ← Encoder\n",
    "                         ↑\n",
    "                    ここを通って勾配がEncoderに流れる\n",
    "```\n",
    "\n",
    "Decoderが正しい出力を生成できなければ、**その責任はEncoderにも伝わる**。\n",
    "Encoderは「Decoderが使いやすい表現」を学習するように最適化されます。\n",
    "\n",
    "#### 4. 図解\n",
    "\n",
    "```\n",
    "        Encoder                     Decoder\n",
    "    ┌───────────┐              ┌───────────┐\n",
    "入力→│           │→ Encoder出力 →│           │→ 出力\n",
    "    │           │      ↓        │Cross-Attn │\n",
    "    │           │    K, V       │  Q=自身   │\n",
    "    └───────────┘              └───────────┘\n",
    "         ↑                           │\n",
    "         └───────── 勾配 ────────────┘\n",
    "```\n",
    "\n",
    "#### まとめ\n",
    "\n",
    "| 疑問 | 回答 |\n",
    "|------|------|\n",
    "| Encoder出力の意味 | 文脈を考慮した入力の表現 |\n",
    "| Decoderでの使い方 | Cross-AttentionのK, Vとして使用 |\n",
    "| Encoderは学習するか | はい、Decoderの損失が逆伝播で伝わる |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q25: Encoderは複数のEncoder Layerが直列に接続されているのか？\n",
    "\n",
    "**質問日**: 2025年12月21日\n",
    "\n",
    "### 質問\n",
    "\n",
    "Encoderは複数個のEncoderが直列に接続されているという理解で良いですか？\n",
    "\n",
    "### 回答\n",
    "\n",
    "はい、その理解で正しいです。\n",
    "\n",
    "#### 構造\n",
    "\n",
    "```\n",
    "入力埋め込み + Position Encoding\n",
    "              ↓\n",
    "    ┌─────────────────────┐\n",
    "    │   Encoder Layer 1   │  ← 同じ構造\n",
    "    └──────────┬──────────┘\n",
    "              ↓\n",
    "    ┌─────────────────────┐\n",
    "    │   Encoder Layer 2   │  ← 同じ構造\n",
    "    └──────────┬──────────┘\n",
    "              ↓\n",
    "             ...\n",
    "              ↓\n",
    "    ┌─────────────────────┐\n",
    "    │   Encoder Layer N   │  ← 同じ構造\n",
    "    └──────────┬──────────┘\n",
    "              ↓\n",
    "          Encoder出力\n",
    "```\n",
    "\n",
    "#### 重要なポイント\n",
    "\n",
    "| 項目 | 内容 |\n",
    "|------|------|\n",
    "| 層数 N | 元論文では **N=6** |\n",
    "| 各層の構造 | 同じ（Self-Attention + FFN + Add & Norm） |\n",
    "| パラメータ | **各層は独立**（共有しない） |\n",
    "| 入出力形状 | すべての層で同じ `[batch, seq_len, d_model]` |\n",
    "\n",
    "#### コードでの実装\n",
    "\n",
    "```python\n",
    "# src/encoder.py\n",
    "self.layers = nn.ModuleList([\n",
    "    EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "    for _ in range(num_layers)  # N個のLayerを作成\n",
    "])\n",
    "\n",
    "# 順伝播\n",
    "for layer in self.layers:\n",
    "    x = layer(x, mask)  # 直列に接続\n",
    "```\n",
    "\n",
    "#### なぜ複数層を積むのか\n",
    "\n",
    "1. **浅い層**: 局所的な関係を学習（隣接する単語の関係）\n",
    "2. **深い層**: 抽象的・大域的な関係を学習（文全体の意味）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q26: 「文脈を考慮した表現」とは具体的に何を意味するのか？\n",
    "\n",
    "**質問日**: 2025年12月21日\n",
    "\n",
    "### 質問\n",
    "\n",
    "「入力シーケンスの文脈を考慮した表現」とは抽象的ですが、具体的に何を意味しているのですか？\n",
    "\n",
    "### 回答\n",
    "\n",
    "#### Before: Token Embedding（文脈なし）\n",
    "\n",
    "```python\n",
    "# 単語は常に同じベクトル\n",
    "embedding[\"bank\"] = [0.2, -0.5, 0.8, ...]  # 常に同じ\n",
    "```\n",
    "\n",
    "**問題**: 同じ単語でも意味が違う場合がある\n",
    "```\n",
    "\"I went to the bank to deposit money\"  → bank = 銀行\n",
    "\"I sat on the river bank\"              → bank = 川岸\n",
    "```\n",
    "でも embedding[\"bank\"] は**同じベクトル**。\n",
    "\n",
    "---\n",
    "\n",
    "#### After: Encoder出力（文脈あり）\n",
    "\n",
    "Encoderを通すと、**周囲の単語の情報が混ざる**。\n",
    "\n",
    "```\n",
    "文1: \"I went to the bank to deposit money\"\n",
    "                    ↓ Encoder\n",
    "     bank の表現 = [0.3, -0.2, 0.9, ...]  ← money, deposit の情報を含む\n",
    "\n",
    "文2: \"I sat on the river bank\"\n",
    "                    ↓ Encoder  \n",
    "     bank の表現 = [-0.1, 0.4, 0.5, ...]  ← river, sat の情報を含む\n",
    "```\n",
    "\n",
    "**結果**: 同じ「bank」でも異なるベクトルになる。\n",
    "\n",
    "---\n",
    "\n",
    "#### どうやって混ざるのか？\n",
    "\n",
    "**Self-Attention** が他の単語の情報を集める。\n",
    "\n",
    "```\n",
    "\"The cat sat on the mat\"\n",
    "\n",
    "\"cat\" の表現を計算するとき:\n",
    "  - \"The\" から少し情報を取る（冠詞）\n",
    "  - \"sat\" から多く情報を取る（猫が座っている）\n",
    "  - \"mat\" から情報を取る（どこに座っているか）\n",
    "  \n",
    "結果: \"cat\" = 元の埋め込み + 周囲の情報の重み付き和\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### まとめ\n",
    "\n",
    "| 段階 | 表現 | 特徴 |\n",
    "|------|------|------|\n",
    "| Token Embedding | 静的 | 同じ単語 = 同じベクトル |\n",
    "| Encoder出力 | 動的 | 同じ単語でも文脈で変わる |\n",
    "\n",
    "**「文脈を考慮した表現」= 周囲の単語の情報が混ざったベクトル**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q27: 同じ単語が複数回出現する場合はどう区別されるのか？\n",
    "\n",
    "**質問日**: 2025年12月21日\n",
    "\n",
    "### 質問\n",
    "\n",
    "文章の中に複数の違う意味を持った同じ単語が現れる場合はどうなりますか？シーケンスで見分けるのですか？\n",
    "\n",
    "### 回答\n",
    "\n",
    "#### 例文\n",
    "\n",
    "```\n",
    "\"The bank near the river bank has a bank account service\"\n",
    "     ↑              ↑              ↑\n",
    "   銀行(1)         川岸           銀行(2)\n",
    "```\n",
    "\n",
    "#### 各位置は独立した表現を持つ\n",
    "\n",
    "Encoderは**トークンの位置ごとに**別々の表現を出力します。\n",
    "\n",
    "```\n",
    "位置:    0     1      2     3     4      5     6    7      8       9\n",
    "入力:  [The] [bank] [near] [the] [river] [bank] [has] [a] [bank] [account]\n",
    "                ↓ Encoder\n",
    "出力:  [h₀]  [h₁]   [h₂]  [h₃]   [h₄]   [h₅]  [h₆] [h₇]  [h₈]    [h₉]\n",
    "              ↑                          ↑                 ↑\n",
    "         銀行の文脈              川岸の文脈         銀行の文脈\n",
    "```\n",
    "\n",
    "**h₁, h₅, h₈ は全て「bank」だが、異なるベクトル**になる。\n",
    "\n",
    "---\n",
    "\n",
    "#### なぜ区別できるのか\n",
    "\n",
    "##### 1. Position Encoding\n",
    "\n",
    "```\n",
    "bank(位置1) = embedding[\"bank\"] + PE[1]\n",
    "bank(位置5) = embedding[\"bank\"] + PE[5]\n",
    "bank(位置8) = embedding[\"bank\"] + PE[8]\n",
    "```\n",
    "\n",
    "位置が違うので、**入力時点で既に異なる**。\n",
    "\n",
    "##### 2. Self-Attentionで異なる単語に注目\n",
    "\n",
    "```\n",
    "bank(位置1) の計算時:\n",
    "  → \"service\", \"account\" に強くAttend → 銀行の意味\n",
    "\n",
    "bank(位置5) の計算時:\n",
    "  → \"river\" に強くAttend → 川岸の意味\n",
    "\n",
    "bank(位置8) の計算時:\n",
    "  → \"account\", \"service\" に強くAttend → 銀行の意味\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 図解\n",
    "\n",
    "```\n",
    "入力シーケンス:\n",
    "  位置:  1       5       8\n",
    "       [bank]  [bank]  [bank]\n",
    "         ↓       ↓       ↓\n",
    "       +PE[1]  +PE[5]  +PE[8]   ← 位置情報で区別\n",
    "         ↓       ↓       ↓\n",
    "    ┌─────────────────────────┐\n",
    "    │    Self-Attention       │  ← 周囲の単語を見て\n",
    "    │  各位置が異なる単語に注目   │     文脈を取り込む\n",
    "    └─────────────────────────┘\n",
    "         ↓       ↓       ↓\n",
    "       [h₁]    [h₅]    [h₈]    ← 全て異なるベクトル\n",
    "       銀行    川岸     銀行\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### まとめ\n",
    "\n",
    "| 仕組み | 役割 |\n",
    "|--------|------|\n",
    "| **Position Encoding** | 同じ単語でも位置で区別 |\n",
    "| **Self-Attention** | 周囲の単語に基づき意味を決定 |\n",
    "| **出力形状** | `[seq_len, d_model]` = 各位置に1つのベクトル |\n",
    "\n",
    "**シーケンスの各位置は常に独立した表現を持つ**ので、同じ単語が何回出ても混同しない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q28: 複数の特徴量ベクトルが一つの単語に結びつくのか？\n",
    "\n",
    "**質問日**: 2025年12月21日\n",
    "\n",
    "### 質問\n",
    "\n",
    "人間が理解できる文章を生成する際に特徴量ベクトルと単語の辞書を引くと理解していますが、複数の特徴量ベクトルに一つの単語が結びつくのですか？\n",
    "\n",
    "### 回答\n",
    "\n",
    "はい、その理解で正しいです。\n",
    "\n",
    "#### Decoderの出力層\n",
    "\n",
    "```\n",
    "Decoder出力: [h₁, h₂, h₃, ...]   (各位置のベクトル, d_model次元)\n",
    "                 ↓\n",
    "            線形変換 (d_model → vocab_size)\n",
    "                 ↓\n",
    "            Softmax\n",
    "                 ↓\n",
    "           確率分布: [P(\"the\"), P(\"cat\"), P(\"sat\"), ...]\n",
    "                 ↓\n",
    "            argmax（最大確率の単語を選択）\n",
    "                 ↓\n",
    "              出力単語\n",
    "```\n",
    "\n",
    "#### 複数のベクトル → 同じ単語\n",
    "\n",
    "**はい、異なるベクトルが同じ単語に変換されます。**\n",
    "\n",
    "```\n",
    "文1: \"I love cats\"\n",
    "     Decoder出力 h₁ = [0.8, 0.3, -0.2, ...] → \"I\"\n",
    "\n",
    "文2: \"I hate dogs\"  \n",
    "     Decoder出力 h₁ = [0.7, 0.4, -0.1, ...] → \"I\"\n",
    "     \n",
    "     ベクトルは違うが、同じ \"I\" を出力\n",
    "```\n",
    "\n",
    "#### なぜベクトルが違うのか\n",
    "\n",
    "ベクトルには**次に何が来るかの情報**も含まれている。\n",
    "\n",
    "```\n",
    "\"I love ...\" の \"I\" のベクトル:\n",
    "  → \"love\" や \"cats\" に繋がる方向を向いている\n",
    "\n",
    "\"I hate ...\" の \"I\" のベクトル:\n",
    "  → \"hate\" や \"dogs\" に繋がる方向を向いている\n",
    "```\n",
    "\n",
    "#### 図解\n",
    "\n",
    "```\n",
    "            特徴量空間                    語彙空間\n",
    "         (d_model次元)                (vocab_size次元)\n",
    "         \n",
    "    h₁ ●──────┐\n",
    "              │\n",
    "    h₂ ●──────┼─────→  \"the\" ●\n",
    "              │\n",
    "    h₃ ●──────┘\n",
    "    \n",
    "    h₄ ●─────────────→  \"cat\" ●\n",
    "    \n",
    "    h₅ ●──────┐\n",
    "              ├─────→  \"is\"  ●\n",
    "    h₆ ●──────┘\n",
    "```\n",
    "\n",
    "**多対一の関係**: 複数の特徴量ベクトルが1つの単語に写像される。\n",
    "\n",
    "#### コードでの実装\n",
    "\n",
    "```python\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        # 出力層: d_model → vocab_size\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, d_model]\n",
    "        logits = self.output_projection(x)  # [batch, seq_len, vocab_size]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        return probs\n",
    "```\n",
    "\n",
    "#### まとめ\n",
    "\n",
    "| 質問 | 回答 |\n",
    "|------|------|\n",
    "| 複数ベクトル → 1単語？ | はい、多対一の写像 |\n",
    "| ベクトルの違いは何？ | 文脈情報（前後の単語との関係） |\n",
    "| どうやって単語に変換？ | 線形変換 + Softmax + argmax |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q29: 翻訳タスクでのEncoder/Decoderの入出力は？\n",
    "\n",
    "**質問日**: 2025年12月22日\n",
    "\n",
    "### 質問\n",
    "\n",
    "日英翻訳タスクの場合、Encoderの入力は日本語シーケンス、Decoderへの入力はその訳である英文シーケンスという理解で良いですか？\n",
    "\n",
    "### 回答\n",
    "\n",
    "はい、その理解で正しいです。\n",
    "\n",
    "```\n",
    "日本語: \"私は猫が好きです\"\n",
    "         ↓\n",
    "    ┌─────────┐\n",
    "    │ Encoder │  ← 日本語シーケンス（ソース）\n",
    "    └────┬────┘\n",
    "         ↓ Encoder出力\n",
    "    ┌─────────┐\n",
    "    │ Decoder │  ← 英語シーケンス（ターゲット）\n",
    "    └────┬────┘\n",
    "         ↓\n",
    "英語: \"I love cats\"\n",
    "```\n",
    "\n",
    "#### 学習時 vs 推論時\n",
    "\n",
    "| フェーズ | Encoder入力 | Decoder入力 | Decoder出力 |\n",
    "|---------|------------|-------------|-------------|\n",
    "| **学習時** | 日本語文 | 正解の英語文（シフト） | 次のトークン予測 |\n",
    "| **推論時** | 日本語文 | 生成済みトークン | 次のトークン |\n",
    "\n",
    "#### 学習時の詳細（Teacher Forcing）\n",
    "\n",
    "```\n",
    "Encoder入力: [\"私\", \"は\", \"猫\", \"が\", \"好き\", \"です\"]\n",
    "\n",
    "Decoder入力: [\"<start>\", \"I\", \"love\", \"cats\"]  ← 正解を1つずらして入力\n",
    "Decoder出力: [\"I\", \"love\", \"cats\", \"<end>\"]    ← 次のトークンを予測\n",
    "```\n",
    "\n",
    "#### 推論時の詳細（自己回帰生成）\n",
    "\n",
    "```\n",
    "Step 1: Decoder入力 [\"<start>\"]        → 予測 \"I\"\n",
    "Step 2: Decoder入力 [\"<start>\", \"I\"]   → 予測 \"love\"\n",
    "Step 3: Decoder入力 [\"<start>\", \"I\", \"love\"] → 予測 \"cats\"\n",
    "Step 4: Decoder入力 [\"<start>\", \"I\", \"love\", \"cats\"] → 予測 \"<end>\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q30: Decoderの学習でTeacher Forcingとは何か？\n",
    "\n",
    "**質問日**: 2025年12月22日\n",
    "\n",
    "### 質問\n",
    "\n",
    "Decoderの出力は次のトークンの予測で、それをDecoder入力として使い、答えの英文は教師信号として使うという理解で良いですか？\n",
    "\n",
    "### 回答\n",
    "\n",
    "はい、正確な理解です。\n",
    "\n",
    "#### 学習時の流れ\n",
    "\n",
    "```\n",
    "正解英文: [\"I\", \"love\", \"cats\", \"<end>\"]\n",
    "              ↑\n",
    "          教師信号（損失計算に使用）\n",
    "\n",
    "Decoder入力: [\"<start>\", \"I\", \"love\", \"cats\"]  ← Teacher Forcing\n",
    "                  ↓\n",
    "             ┌─────────┐\n",
    "             │ Decoder │\n",
    "             └────┬────┘\n",
    "                  ↓\n",
    "Decoder出力: [P(\"I\"), P(\"love\"), P(\"cats\"), P(\"<end>\")]\n",
    "                  ↓\n",
    "              損失計算（Cross-Entropy Loss）\n",
    "                  ↓\n",
    "            正解と比較して学習\n",
    "```\n",
    "\n",
    "#### ポイント\n",
    "\n",
    "| 項目 | 内容 |\n",
    "|------|------|\n",
    "| **Decoder入力** | 正解文を1つ右にシフト（Teacher Forcing） |\n",
    "| **Decoder出力** | 各位置で次トークンの確率分布 |\n",
    "| **教師信号** | 正解の英文（シフトなし） |\n",
    "| **損失関数** | 出力確率と正解トークンのCross-Entropy |\n",
    "\n",
    "#### なぜTeacher Forcingを使うか\n",
    "\n",
    "```\n",
    "❌ 自分の予測を入力に使う場合（推論時と同じ）:\n",
    "  間違った予測 → 次も間違える → エラーが蓄積\n",
    "\n",
    "✅ Teacher Forcing（正解を入力に使う）:\n",
    "  常に正解を入力 → 安定した学習 → 高速収束\n",
    "```\n",
    "\n",
    "#### 推論時との違い\n",
    "\n",
    "| フェーズ | Decoder入力 | \n",
    "|---------|------------|\n",
    "| **学習時** | 正解文（Teacher Forcing） |\n",
    "| **推論時** | 自分の予測を使う（自己回帰） |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g488yucwsx",
   "source": "---\n\n## Q31: 翻訳時にソースとターゲットのシーケンス長が異なる場合はどこで吸収するのか？\n\n**質問日**: 2025年12月22日\n\n### 質問\n\n日英翻訳の場合、日本語と英語でシーケンス長に違いが生じますが、どこで吸収していますか？\n\n### 回答\n\n**Cross-Attention**で吸収します。\n\n#### Cross-Attentionの行列サイズ\n\n```\n日本語（ソース）: 5トークン (src_len = 5)\n英語（ターゲット）: 8トークン (tgt_len = 8)\n\nCross-Attention:\n  Q = Decoder状態     [tgt_len, d_model] = [8, 512]\n  K = Encoder出力     [src_len, d_model] = [5, 512]\n  V = Encoder出力     [src_len, d_model] = [5, 512]\n```\n\n#### 行列積の計算（なぜ次元が合うのか）\n\n```\nK^T（Kの転置）= [d_model, src_len] = [512, 5]\n                 ↑ 行と列が入れ替わる\n\nQ @ K^T の計算:\n  [tgt_len, d_model] @ [d_model, src_len]\n  [8,       512    ] @ [512,     5      ]\n       ↑                   ↑\n       └─── 共通次元（512）で内積 ───┘\n              ↓\n         結果: [8, 5]\n```\n\n**d_model次元が共通**なので、tgt_lenとsrc_lenが異なっても計算可能です。\n\n#### 図解\n\n```\n        Q [8, 512]              K^T [512, 5]\n     ┌─────────────┐          ┌───────────┐\n     │ → → → → → → │ 512次元  │ ↓ ↓ ↓ ↓ ↓ │\n  8行│ → → → → → → │    @     │ ↓ ↓ ↓ ↓ ↓ │ 5列\n     │ → → → → → → │          │ ↓ ↓ ↓ ↓ ↓ │ 512行\n     │    ...      │          │   ...     │\n     └─────────────┘          └───────────┘\n           ↓\n      結果 [8, 5]\n     ┌───────────┐\n     │ ● ● ● ● ● │  ← 各要素は512次元の内積\n  8行│ ● ● ● ● ● │    = Qの各行とKの各行の類似度\n     │   ...     │\n     └───────────┘\n        5列\n```\n\n#### 完全な計算の流れ\n\n```\nAttention重み = softmax(Q @ K^T / √d_k)\n             = softmax([8, 512] @ [512, 5] / √512)\n             = softmax([8, 5])\n             = [8, 5]  ← 各行の和が1になる\n\n出力 = Attention重み @ V\n     = [8, 5] @ [5, 512]\n     = [8, 512]  ← ターゲット長に揃う\n```\n\n#### 具体例\n\n```\n日本語: [\"私\", \"は\", \"猫\", \"が\", \"好き\"]  (5トークン)\n英語:   [\"I\", \"love\", \"cats\", \"very\", \"much\", \".\", \"<pad>\", \"<pad>\"]  (8トークン)\n\nCross-Attention重み (8×5):\n        私    は    猫    が    好き\n\"I\"    [0.7  0.2  0.0  0.0  0.1 ]  ← 「私」に強く注目\n\"love\" [0.1  0.1  0.1  0.1  0.6 ]  ← 「好き」に強く注目\n\"cats\" [0.0  0.0  0.8  0.1  0.1 ]  ← 「猫」に強く注目\n\"very\" [0.1  0.1  0.1  0.1  0.6 ]  ← 「好き」に注目\n\"much\" [0.1  0.1  0.1  0.1  0.6 ]  ← 「好き」に注目\n\".\"    [0.1  0.2  0.2  0.2  0.3 ]  ← 分散\n...\n```\n\n各ターゲット位置は、ソース全体（日本語5トークン）への重み付き和を計算し、\n結果としてターゲット長（英語8トークン）に揃った表現を得ます。\n\n#### ポイント\n\n| 項目 | 内容 |\n|------|------|\n| **どこで吸収** | Cross-Attention |\n| **なぜ計算可能** | d_model次元が共通なので、Q @ K^T が計算できる |\n| **仕組み** | Q [tgt, d] × K^T [d, src] → 重み [tgt, src] |\n| **結果** | ターゲットの各位置が、ソース全体を参照できる |\n\n#### Self-Attentionとの違い\n\n| Attention | Q, K, V | 結果のサイズ |\n|-----------|---------|-------------|\n| **Self-Attention** | 全て同じシーケンス | [seq_len, seq_len] → [seq_len, d_model] |\n| **Cross-Attention** | QとK,Vが異なる | [tgt_len, src_len] → [tgt_len, d_model] |",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}