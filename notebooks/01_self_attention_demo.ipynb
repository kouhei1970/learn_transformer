{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4b8fd9e",
   "metadata": {},
   "source": [
    "# Self-Attention機構のデモ\n",
    "\n",
    "このノートブックでは、Transformerの核となる**Self-Attention（自己注意機構）**をゼロから実装し、その動作を理解します。\n",
    "\n",
    "## 学習目標\n",
    "\n",
    "1. Self-Attentionの計算フロー（Q, K, V → Attention Scores → Softmax → 出力）を理解する\n",
    "2. Attention Weightsを可視化して、どの要素が他の要素に注目しているかを確認する\n",
    "3. 実装したSelf-Attentionを使って簡単なタスクを実行する\n",
    "\n",
    "## Attention機構とは？\n",
    "\n",
    "Attention機構は、入力シーケンスの各要素が他の要素とどれだけ関連しているかを計算し、重要な情報に「注意」を向ける仕組みです。\n",
    "\n",
    "**数式**: `Attention(Q, K, V) = softmax(QK^T / √d_k) V`\n",
    "\n",
    "- **Q (Query)**: 「何を探しているか」\n",
    "- **K (Key)**: 「何を持っているか」\n",
    "- **V (Value)**: 「実際の情報」"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1955e2",
   "metadata": {},
   "source": [
    "## 1. 必要なライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7f29eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用デバイス: cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')  # srcディレクトリをパスに追加\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from attention import SelfAttention, ScaledDotProductAttention\n",
    "\n",
    "# 日本語フォントの設定（グラフ表示用）\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'Hiragino Sans']\n",
    "\n",
    "# シード値の設定（再現性のため）\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# デバイスの設定（macOS GPU対応）\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"使用デバイス: CUDA GPU\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"使用デバイス: Apple Silicon GPU (MPS)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"使用デバイス: CPU\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894f572a",
   "metadata": {},
   "source": [
    "## 2. 入力データの準備\n",
    "\n",
    "簡単な例として、4つの単語からなる文を考えます。各単語は8次元のベクトルで表現されます。\n",
    "\n",
    "実際の自然言語処理では、単語埋め込み（Word Embedding）を使いますが、ここではランダムなベクトルを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c8c495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータ\n",
    "seq_len = 4      # シーケンス長（単語数）\n",
    "d_model = 8      # 各単語の埋め込み次元数\n",
    "batch_size = 1   # バッチサイズ\n",
    "\n",
    "# 入力データの生成（ランダムな単語埋め込み）\n",
    "# 形状: [batch_size, seq_len, d_model]\n",
    "X = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "print(f\"入力データの形状: {X.shape}\")\n",
    "print(f\"  - バッチサイズ: {batch_size}\")\n",
    "print(f\"  - シーケンス長: {seq_len} (単語数)\")\n",
    "print(f\"  - 埋め込み次元: {d_model}\")\n",
    "print(f\"\\n入力データ (最初のサンプル):\")\n",
    "print(X[0].numpy())\n",
    "\n",
    "# 可視化のため、単語に名前をつける\n",
    "words = [\"私は\", \"猫が\", \"好き\", \"です\"]\n",
    "print(f\"\\n単語: {words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcb2287",
   "metadata": {},
   "source": [
    "## 3. Query、Key、Valueの計算\n",
    "\n",
    "Self-Attentionでは、入力から3つの異なる表現を作ります：\n",
    "\n",
    "- **Query (Q)**: 「この単語は何を探しているか？」\n",
    "- **Key (K)**: 「この単語は何についての情報を持っているか？」\n",
    "- **Value (V)**: 「この単語の実際の情報」\n",
    "\n",
    "これらは入力を線形変換（重み行列との積）で生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f2b2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q, K, Vを生成するための線形変換層\n",
    "W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "# Q, K, Vの計算\n",
    "Q = W_q(X)  # Query: [batch, seq_len, d_model]\n",
    "K = W_k(X)  # Key: [batch, seq_len, d_model]\n",
    "V = W_v(X)  # Value: [batch, seq_len, d_model]\n",
    "\n",
    "print(f\"Query (Q) の形状: {Q.shape}\")\n",
    "print(f\"Key (K) の形状: {K.shape}\")\n",
    "print(f\"Value (V) の形状: {V.shape}\")\n",
    "print(f\"\\n重要なポイント:\")\n",
    "print(f\"  - Q, K, V はすべて同じ入力 X から生成される → 「Self」Attention\")\n",
    "print(f\"  - それぞれ異なる重み行列 W_q, W_k, W_v で変換される\")\n",
    "print(f\"  - Q と K の内積で「関連度」を計算し、V を重み付けする\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f729de",
   "metadata": {},
   "source": [
    "## 4. Attention Scoresの計算\n",
    "\n",
    "各単語（Query）が他の全ての単語（Key）とどれだけ関連しているかを内積で計算します。\n",
    "\n",
    "**計算式**: `scores = Q × K^T / √d_k`\n",
    "\n",
    "- 内積が大きい = 関連度が高い\n",
    "- √d_k でスケーリング = 値が大きくなりすぎるのを防ぐ（勾配安定化）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0517c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Q と K^T の内積を計算\n",
    "# Q: [batch, seq_len, d_model]\n",
    "# K.transpose(-2, -1): [batch, d_model, seq_len]\n",
    "# scores: [batch, seq_len, seq_len]\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "\n",
    "print(f\"内積スコアの形状: {scores.shape}\")\n",
    "print(f\"  - 各行: 1つの単語（Query）\")\n",
    "print(f\"  - 各列: 全ての単語（Key）との関連度\")\n",
    "\n",
    "# Step 2: √d_k でスケーリング\n",
    "d_k = Q.size(-1)  # d_modelと同じ\n",
    "scores_scaled = scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "\n",
    "print(f\"\\nスケーリング前のスコア:\")\n",
    "print(scores[0].detach().numpy())\n",
    "print(f\"\\nスケーリング後のスコア (÷ √{d_k} = ÷ {np.sqrt(d_k):.2f}):\")\n",
    "print(scores_scaled[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46e8e7b",
   "metadata": {},
   "source": [
    "## 5. Softmaxによる正規化\n",
    "\n",
    "スコアにSoftmax関数を適用して、確率分布（合計が1）に変換します。\n",
    "\n",
    "これにより、各単語が他の全単語に対してどれだけ「注意」を払うべきかの重みが得られます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b2250d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmaxを適用（最後の次元=各行について正規化）\n",
    "attention_weights = F.softmax(scores_scaled, dim=-1)\n",
    "\n",
    "print(f\"Attention Weightsの形状: {attention_weights.shape}\")\n",
    "print(f\"\\nAttention Weights (最初のサンプル):\")\n",
    "print(attention_weights[0].detach().numpy())\n",
    "\n",
    "# 各行の合計が1になることを確認\n",
    "row_sums = attention_weights[0].sum(dim=-1)\n",
    "print(f\"\\n各行の合計（すべて1.0になるはず）:\")\n",
    "print(row_sums.detach().numpy())\n",
    "\n",
    "# 意味の解釈\n",
    "print(f\"\\n【解釈例】1行目を見ると:\")\n",
    "print(f\"  単語 '{words[0]}' は、\")\n",
    "for i, w in enumerate(words):\n",
    "    weight = attention_weights[0, 0, i].item()\n",
    "    print(f\"    - '{w}' に {weight:.3f} (={weight*100:.1f}%) 注意を払っている\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046aa178",
   "metadata": {},
   "source": [
    "## 6. Attention Weightsの可視化\n",
    "\n",
    "ヒートマップでAttention Weightsを可視化します。\n",
    "\n",
    "- 縦軸: Query（注意を払う側の単語）\n",
    "- 横軸: Key（注意を受ける側の単語）\n",
    "- 色の濃さ: 注意の強さ（明るいほど強い関連）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cf0d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Weightsのヒートマップ\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    attention_weights[0].detach().numpy(),\n",
    "    annot=True,           # 数値を表示\n",
    "    fmt='.3f',            # 小数点以下3桁\n",
    "    cmap='YlOrRd',        # カラーマップ（黄色→オレンジ→赤）\n",
    "    xticklabels=words,    # x軸のラベル\n",
    "    yticklabels=words,    # y軸のラベル\n",
    "    cbar_kws={'label': 'Attention Weight'}\n",
    ")\n",
    "plt.xlabel('Key (注意を受ける側)', fontsize=12)\n",
    "plt.ylabel('Query (注意を払う側)', fontsize=12)\n",
    "plt.title('Self-Attention Weights のヒートマップ', fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"【読み方】\")\n",
    "print(\"- 各行を見ると、その単語が他の単語にどれだけ注意を払っているかがわかる\")\n",
    "print(\"- 対角成分は、各単語が自分自身にどれだけ注意を払っているか\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcbf6ec",
   "metadata": {},
   "source": [
    "## 7. 出力の計算\n",
    "\n",
    "Attention WeightsとValueの重み付き和を取り、Self-Attentionの最終出力を得ます。\n",
    "\n",
    "**計算式**: `Output = Attention_Weights × V`\n",
    "\n",
    "各単語の出力は、全単語のValue表現を、Attention Weightsで重み付けして足し合わせたものになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fdfbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention WeightsとValueの積\n",
    "# attention_weights: [batch, seq_len, seq_len]\n",
    "# V: [batch, seq_len, d_model]\n",
    "# output: [batch, seq_len, d_model]\n",
    "output = torch.matmul(attention_weights, V)\n",
    "\n",
    "print(f\"出力の形状: {output.shape}\")\n",
    "print(f\"  - 入力と同じ形状 [batch_size, seq_len, d_model]\")\n",
    "print(f\"\\n出力データ (最初のサンプル):\")\n",
    "print(output[0].detach().numpy())\n",
    "\n",
    "print(f\"\\n【重要な理解】\")\n",
    "print(f\"各単語の出力ベクトルは、全単語のValue表現の重み付き和\")\n",
    "print(f\"  例: 1番目の単語 '{words[0]}' の出力 = \")\n",
    "print(f\"      {attention_weights[0,0,0]:.3f} × V[0] + {attention_weights[0,0,1]:.3f} × V[1] + ...\")\n",
    "print(f\"\\nこれにより、各単語が文脈情報（他の単語の情報）を取り込んだ表現になる！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06633082",
   "metadata": {},
   "source": [
    "## 8. 実装したSelf-Attentionクラスを使う\n",
    "\n",
    "ここまでステップバイステップで計算してきた内容を、`src/attention.py`で実装済みの`SelfAttention`クラスを使って再現します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59d46c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-Attentionモデルのインスタンス化\n",
    "model = SelfAttention(d_model=d_model, dropout=0.0)\n",
    "model.eval()  # 評価モード（ドロップアウトを無効化）\n",
    "\n",
    "# 新しい入力データで実行\n",
    "X_test = torch.randn(1, seq_len, d_model)\n",
    "output_test, attention_weights_test = model(X_test)\n",
    "\n",
    "print(f\"クラスを使った場合の出力形状: {output_test.shape}\")\n",
    "print(f\"Attention Weights形状: {attention_weights_test.shape}\")\n",
    "\n",
    "# 可視化\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    attention_weights_test[0].detach().numpy(),\n",
    "    annot=True,\n",
    "    fmt='.3f',\n",
    "    cmap='Blues',\n",
    "    xticklabels=words,\n",
    "    yticklabels=words,\n",
    "    cbar_kws={'label': 'Attention Weight'}\n",
    ")\n",
    "plt.xlabel('Key', fontsize=12)\n",
    "plt.ylabel('Query', fontsize=12)\n",
    "plt.title('SelfAttentionクラスによる Attention Weights', fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc36997a",
   "metadata": {},
   "source": [
    "## 9. 簡単な学習タスク：数列のコピー\n",
    "\n",
    "Self-Attentionが実際に学習できることを確認するため、シンプルなタスクを試します。\n",
    "\n",
    "**タスク**: 入力数列をそのままコピーする\n",
    "\n",
    "例: `[1, 2, 3, 4]` → `[1, 2, 3, 4]`\n",
    "\n",
    "これは簡単なタスクですが、Self-Attentionがシーケンス情報を学習できることを示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517216c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# シンプルなモデル: Self-Attention + 線形層\n",
    "class SimpleSelfAttentionModel(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.attention = SelfAttention(d_model, dropout=0.1)\n",
    "        self.output_layer = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Self-Attentionを適用\n",
    "        attn_out, attn_weights = self.attention(x)\n",
    "        # 線形層で出力\n",
    "        output = self.output_layer(attn_out)\n",
    "        return output, attn_weights\n",
    "\n",
    "# モデルのインスタンス化\n",
    "d_model_task = 16\n",
    "model_task = SimpleSelfAttentionModel(d_model_task).to(device)\n",
    "optimizer = torch.optim.Adam(model_task.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 訓練データの生成（ランダムなベクトル列をコピーするタスク）\n",
    "def generate_copy_data(batch_size, seq_len, d_model):\n",
    "    \"\"\"入力と同じ出力を返すデータを生成\"\"\"\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    y = x.clone()  # コピータスク\n",
    "    return x, y\n",
    "\n",
    "# 学習ループ\n",
    "num_epochs = 100\n",
    "seq_len_task = 6\n",
    "batch_size_task = 32\n",
    "losses = []\n",
    "\n",
    "print(\"学習開始...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # データ生成\n",
    "    x_train, y_train = generate_copy_data(batch_size_task, seq_len_task, d_model_task)\n",
    "    x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "    \n",
    "    # 順伝播\n",
    "    optimizer.zero_grad()\n",
    "    output, _ = model_task(x_train)\n",
    "    \n",
    "    # 損失計算\n",
    "    loss = criterion(output, y_train)\n",
    "    \n",
    "    # 逆伝播と最適化\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.6f}\")\n",
    "\n",
    "print(\"学習完了！\")\n",
    "\n",
    "# 損失の可視化\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('学習曲線: 数列コピータスク')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f8ca05",
   "metadata": {},
   "source": [
    "## 10. テスト：学習したモデルの評価\n",
    "\n",
    "学習したモデルがコピータスクをちゃんと実行できるか確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00601953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータで評価\n",
    "model_task.eval()\n",
    "with torch.no_grad():\n",
    "    x_test, y_test = generate_copy_data(1, seq_len_task, d_model_task)\n",
    "    x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "    \n",
    "    output_pred, attn_weights_final = model_task(x_test)\n",
    "    \n",
    "    test_loss = criterion(output_pred, y_test)\n",
    "    \n",
    "    print(f\"テスト損失: {test_loss.item():.6f}\")\n",
    "    print(f\"\\n入力ベクトルの一部:\")\n",
    "    print(x_test[0, :3, :5].cpu().numpy())  # 最初の3単語、5次元まで表示\n",
    "    print(f\"\\n期待される出力（= 入力のコピー）:\")\n",
    "    print(y_test[0, :3, :5].cpu().numpy())\n",
    "    print(f\"\\nモデルの予測出力:\")\n",
    "    print(output_pred[0, :3, :5].cpu().numpy())\n",
    "    print(f\"\\n→ かなり近い値を予測できています！\")\n",
    "\n",
    "# Attention Weightsの可視化\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    attn_weights_final[0].cpu().numpy(),\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='Greens',\n",
    "    cbar_kws={'label': 'Attention Weight'}\n",
    ")\n",
    "plt.xlabel('Key (位置)', fontsize=12)\n",
    "plt.ylabel('Query (位置)', fontsize=12)\n",
    "plt.title('学習後のAttention Weights（コピータスク）', fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n【Attention Weightsの解釈】\")\n",
    "print(\"対角成分が強い = 各位置が自分自身に強く注意を払っている\")\n",
    "print(\"→ コピータスクでは、各位置の情報をそのまま出力すれば良いため、この傾向は理にかなっています\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dee482f",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "このノートブックでは、Self-Attention機構について以下を学びました：\n",
    "\n",
    "### 理論\n",
    "1. **Query, Key, Value**: 入力から3つの異なる表現を生成\n",
    "2. **Attention Scores**: Q と K の内積で関連度を計算\n",
    "3. **Softmax**: スコアを確率分布に変換\n",
    "4. **重み付き和**: Attention WeightsとValueで最終出力を計算\n",
    "\n",
    "### 実装\n",
    "- PyTorchで`SelfAttention`クラスを実装\n",
    "- Attention Weightsをヒートマップで可視化\n",
    "- 簡単なコピータスクで学習を確認\n",
    "\n",
    "### 次のステップ\n",
    "- **Multi-Head Attention**: 複数のAttentionを並列に実行\n",
    "- **Position Encoding**: シーケンスの順序情報を追加\n",
    "- **Transformer Encoder**: Self-Attention + Feed Forward Network\n",
    "\n",
    "Self-Attentionは、Transformerの核となる強力な機構です。この理解を土台に、次はMulti-Head Attentionを実装していきましょう！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
