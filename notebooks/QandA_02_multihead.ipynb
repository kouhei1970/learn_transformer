{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A Part 2: Multi-Head Attention\n",
    "\n",
    "Transformerå­¦ç¿’ä¸­ã®è³ªå•ã¨å›ç­”é›†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5f69f6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q11: Concatã¨ã¯ï¼Ÿ\n",
    "\n",
    "**è³ªå•æ—¥**: 2025å¹´11æœˆ17æ—¥\n",
    "\n",
    "### è³ªå•\n",
    "Multi-Head Attentionã®æ•°å¼ã«å‡ºã¦ãã‚‹`Concat`ã¨ã¯ä½•ã‚’æ„å‘³ã—ã¾ã™ã‹ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02c336e",
   "metadata": {},
   "source": [
    "### å›ç­”\n",
    "\n",
    "`Concat`ã¯**Concatenateï¼ˆé€£çµï¼‰**ã®ç•¥ã§ã€è¤‡æ•°ã®ãƒ†ãƒ³ã‚½ãƒ«ã‚’ç¹‹ã’ã‚‹æ“ä½œã‚’æ„å‘³ã—ã¾ã™ã€‚\n",
    "\n",
    "#### Multi-Head Attentionã§ã®å½¹å‰²\n",
    "\n",
    "Multi-Head Attentionã§ã¯ã€å„headãŒä¸¦åˆ—ã«è¨ˆç®—ã—ãŸçµæœã‚’**çµåˆï¼ˆconcatï¼‰**ã—ã¦ã€å…ƒã®æ¬¡å…ƒã«æˆ»ã—ã¾ã™ã€‚\n",
    "\n",
    "**æ•°å¼**:\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, ..., \\text{head}_h)W^O\n",
    "$$\n",
    "\n",
    "å„headã¯:\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "$$\n",
    "\n",
    "#### å…·ä½“çš„ãªå‹•ä½œ\n",
    "\n",
    "`d_model=512`ã€`num_heads=8`ã®å ´åˆï¼š\n",
    "\n",
    "1. **å„headã®å‡ºåŠ›**: `[batch, seq_len, 64]` (64 = 512/8)\n",
    "2. **Concatå¾Œ**: 8å€‹ã®headã‚’é€£çµ â†’ `[batch, seq_len, 512]`\n",
    "3. **æœ€çµ‚å¤‰æ›**: `W^O`ã§ç·šå½¢å¤‰æ›\n",
    "\n",
    "#### ã‚¤ãƒ¡ãƒ¼ã‚¸å›³\n",
    "\n",
    "```\n",
    "Head 1: [batch, seq_len, 64]\n",
    "Head 2: [batch, seq_len, 64]\n",
    "Head 3: [batch, seq_len, 64]\n",
    "   ...\n",
    "Head 8: [batch, seq_len, 64]\n",
    "        â†“ Concat\n",
    "[batch, seq_len, 512]  (64Ã—8 = 512)\n",
    "```\n",
    "\n",
    "å„headã®64æ¬¡å…ƒã®å‡ºåŠ›ã‚’æ¨ªã«ä¸¦ã¹ã¦ã€512æ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½œã‚Šã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f98d87",
   "metadata": {},
   "source": [
    "#### ã‚³ãƒ¼ãƒ‰ä¾‹: Concatã®å®Ÿè£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6cfa1be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å„headã®å‡ºåŠ›å½¢çŠ¶: torch.Size([2, 8, 5, 64])\n",
      "  - batch_size: 2\n",
      "  - num_heads: 8\n",
      "  - seq_len: 5\n",
      "  - d_k (å„headã®æ¬¡å…ƒ): 64\n",
      "\n",
      "è»¢ç½®å¾Œã®å½¢çŠ¶: torch.Size([2, 5, 8, 64])\n",
      "Concatå¾Œã®å½¢çŠ¶: torch.Size([2, 5, 512])\n",
      "  - d_model = num_heads Ã— d_k = 8 Ã— 64 = 512\n",
      "\n",
      "æœ€çµ‚å‡ºåŠ›ã®å½¢çŠ¶: torch.Size([2, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Multi-Head Attentionã®Concatéƒ¨åˆ†ã‚’å†ç¾\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "num_heads = 8\n",
    "d_k = 64  # å„headã®æ¬¡å…ƒ\n",
    "d_model = num_heads * d_k  # 512\n",
    "\n",
    "# å„headã®å‡ºåŠ›ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ\n",
    "# å®Ÿéš›ã«ã¯å„headãŒä¸¦åˆ—ã«Attentionã‚’è¨ˆç®—ã—ãŸçµæœ\n",
    "attention_output = torch.randn(batch_size, num_heads, seq_len, d_k)\n",
    "print(f\"å„headã®å‡ºåŠ›å½¢çŠ¶: {attention_output.shape}\")\n",
    "print(f\"  - batch_size: {batch_size}\")\n",
    "print(f\"  - num_heads: {num_heads}\")\n",
    "print(f\"  - seq_len: {seq_len}\")\n",
    "print(f\"  - d_k (å„headã®æ¬¡å…ƒ): {d_k}\")\n",
    "\n",
    "# Concatæ“ä½œ: å…¨headã‚’çµåˆ\n",
    "# [batch, num_heads, seq_len, d_k] -> [batch, seq_len, num_heads, d_k]\n",
    "attention_output = attention_output.transpose(1, 2).contiguous()\n",
    "print(f\"\\nè»¢ç½®å¾Œã®å½¢çŠ¶: {attention_output.shape}\")\n",
    "\n",
    "# [batch, seq_len, num_heads, d_k] -> [batch, seq_len, d_model]\n",
    "attention_output = attention_output.view(batch_size, seq_len, d_model)\n",
    "print(f\"Concatå¾Œã®å½¢çŠ¶: {attention_output.shape}\")\n",
    "print(f\"  - d_model = num_heads Ã— d_k = {num_heads} Ã— {d_k} = {d_model}\")\n",
    "\n",
    "# ã“ã®å¾Œã€W_o (å‡ºåŠ›ç·šå½¢å¤‰æ›) ã‚’é©ç”¨\n",
    "W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "output = W_o(attention_output)\n",
    "print(f\"\\næœ€çµ‚å‡ºåŠ›ã®å½¢çŠ¶: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475b485f",
   "metadata": {},
   "source": [
    "#### å®Ÿè£…ã§ã®å¯¾å¿œã‚³ãƒ¼ãƒ‰\n",
    "\n",
    "`src/attention.py`ã®`MultiHeadAttention`ã‚¯ãƒ©ã‚¹ã§ã¯ã€ä»¥ä¸‹ã®éƒ¨åˆ†ãŒConcatã«å¯¾å¿œã—ã¾ã™ï¼š\n",
    "\n",
    "```python\n",
    "# 4. å„headã§Scaled Dot-Product Attentionã‚’å®Ÿè¡Œ\n",
    "# attention_output: [batch, num_heads, seq_len, d_k]\n",
    "attention_output, attention_weights = self.attention(Q, K, V, mask)\n",
    "\n",
    "# 5. å…¨headã‚’çµåˆ: [batch, seq_len, d_model]\n",
    "attention_output = self.combine_heads(attention_output, batch_size)\n",
    "\n",
    "# 6. å‡ºåŠ›ç·šå½¢å¤‰æ›\n",
    "output = self.W_o(attention_output)\n",
    "```\n",
    "\n",
    "`combine_heads`ãƒ¡ã‚½ãƒƒãƒ‰ãŒå®Ÿéš›ã®Concatæ“ä½œã‚’è¡Œã£ã¦ã„ã¾ã™ï¼š\n",
    "\n",
    "```python\n",
    "def combine_heads(self, x, batch_size):\n",
    "    \"\"\"\n",
    "    è¤‡æ•°ã®headã‚’çµåˆ\n",
    "    \n",
    "    Args:\n",
    "        x: shape [batch_size, num_heads, seq_len, d_k]\n",
    "    Returns:\n",
    "        shape [batch_size, seq_len, d_model]\n",
    "    \"\"\"\n",
    "    # [batch, num_heads, seq_len, d_k] -> [batch, seq_len, num_heads, d_k]\n",
    "    x = x.transpose(1, 2).contiguous()\n",
    "    # [batch, seq_len, num_heads, d_k] -> [batch, seq_len, d_model]\n",
    "    return x.view(batch_size, -1, self.d_model)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a8a7f3",
   "metadata": {},
   "source": [
    "#### ã¾ã¨ã‚\n",
    "\n",
    "**Concatã®å½¹å‰²**:\n",
    "- å„headã®å‡ºåŠ›ï¼ˆå°ã•ã„æ¬¡å…ƒï¼‰ã‚’æ¨ªã«ç¹‹ã’ã¦ã€å…ƒã®å¤§ãã„æ¬¡å…ƒã«æˆ»ã™\n",
    "- 8å€‹ã®headï¼ˆå„64æ¬¡å…ƒï¼‰â†’ 1ã¤ã®ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆ512æ¬¡å…ƒï¼‰\n",
    "\n",
    "**ãªãœConcatãŒå¿…è¦ï¼Ÿ**:\n",
    "1. å„headã¯ç‹¬ç«‹ã—ã¦ç•°ãªã‚‹ç¨®é¡ã®æƒ…å ±ã‚’å­¦ç¿’\n",
    "2. ãã‚Œã‚‰ã‚’çµ±åˆã—ã¦ã€è±Šã‹ãªè¡¨ç¾ã‚’ä½œã‚‹\n",
    "3. æœ€çµ‚çš„ã«å…ƒã®æ¬¡å…ƒã«æˆ»ã™ã“ã¨ã§ã€æ¬¡ã®å±¤ã¸ã®å…¥åŠ›ã¨ã—ã¦ä½¿ãˆã‚‹\n",
    "\n",
    "**PyTorchã§ã®å®Ÿè£…**:\n",
    "- `transpose()`: æ¬¡å…ƒã®é †åºã‚’å…¥ã‚Œæ›¿ãˆ\n",
    "- `view()` ã¾ãŸã¯ `reshape()`: ãƒ†ãƒ³ã‚½ãƒ«ã®å½¢çŠ¶ã‚’å¤‰æ›´\n",
    "- ã“ã®2ã¤ã®æ“ä½œã§Concatã‚’å®Ÿç¾"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c64c6c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q12: ãªãœQ, K, Vã«ç·šå½¢å¤‰æ›ã‚’é€šã™ã®ã‹ï¼Ÿ\n",
    "\n",
    "**è³ªå•æ—¥**: 2025å¹´11æœˆ17æ—¥\n",
    "\n",
    "### è³ªå•\n",
    "Self-Attentionã§ã€å…¥åŠ›ã‹ã‚‰ Query, Key, Value ã‚’ç”Ÿæˆã™ã‚‹ã¨ãã«ã€ãªãœç·šå½¢å¤‰æ›å±¤ï¼ˆ`nn.Linear`ï¼‰ã‚’é€šã™å¿…è¦ãŒã‚ã‚‹ã®ã§ã™ã‹ï¼Ÿå…¥åŠ›ã‚’ãã®ã¾ã¾ä½¿ã£ã¦ã¯ã„ã‘ãªã„ã®ã§ã—ã‚‡ã†ã‹ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53fac62",
   "metadata": {},
   "source": [
    "### å›ç­”\n",
    "\n",
    "ç·šå½¢å¤‰æ›ã‚’é€šã™ç†ç”±ã¯ã€**Q, K, V ãã‚Œãã‚Œã«ç•°ãªã‚‹å½¹å‰²ã‚’å­¦ç¿’ã•ã›ã‚‹**ãŸã‚ã§ã™ã€‚\n",
    "\n",
    "#### ä¸»ãªç†ç”±\n",
    "\n",
    "##### 1. **ç•°ãªã‚‹è¡¨ç¾ç©ºé–“ã¸ã®å°„å½±**\n",
    "\n",
    "åŒã˜å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€3ã¤ã®ç•°ãªã‚‹ã€Œè¦–ç‚¹ã€ã‚’ä½œã‚Šå‡ºã—ã¾ã™ï¼š\n",
    "\n",
    "- **Query (Q)**: ã€Œä½•ã‚’æ¢ã—ã¦ã„ã‚‹ã‹ã€ã‚’è¡¨ç¾\n",
    "- **Key (K)**: ã€Œä½•ã‚’æŒã£ã¦ã„ã‚‹ã‹ã€ã‚’è¡¨ç¾  \n",
    "- **Value (V)**: ã€Œå®Ÿéš›ã®æƒ…å ±ã€ã‚’è¡¨ç¾\n",
    "\n",
    "ãã‚Œãã‚Œç‹¬ç«‹ã—ãŸé‡ã¿è¡Œåˆ—ã‚’æŒã¤ã“ã¨ã§ã€ç•°ãªã‚‹ç‰¹å¾´ã‚’æŠ½å‡ºã§ãã¾ã™ã€‚\n",
    "\n",
    "##### 2. **å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**\n",
    "\n",
    "ç·šå½¢å¤‰æ›ã®é‡ã¿è¡Œåˆ—ã¯**å­¦ç¿’å¯èƒ½**ãªã®ã§ã€ã‚¿ã‚¹ã‚¯ã«å¿œã˜ã¦æœ€é©ãªQ, K, Vã®ç”Ÿæˆæ–¹æ³•ã‚’å­¦ç¿’ã—ã¾ã™ã€‚\n",
    "\n",
    "```python\n",
    "# å„å¤‰æ›ã¯ç‹¬ç«‹ã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤\n",
    "self.W_q = nn.Linear(d_model, d_model, bias=False)  # Queryç”¨ã®é‡ã¿\n",
    "self.W_k = nn.Linear(d_model, d_model, bias=False)  # Keyç”¨ã®é‡ã¿\n",
    "self.W_v = nn.Linear(d_model, d_model, bias=False)  # Valueç”¨ã®é‡ã¿\n",
    "```\n",
    "\n",
    "##### 3. **æŸ”è»Ÿæ€§ã®å‘ä¸Š**\n",
    "\n",
    "å…¥åŠ›ã‚’ãã®ã¾ã¾ä½¿ã†ã¨ã€Q, K, VãŒå…¨ã¦åŒã˜ã«ãªã£ã¦ã—ã¾ã„ã¾ã™ã€‚ã“ã‚Œã§ã¯ï¼š\n",
    "- AttentionãŒå˜ç´”ãªè‡ªå·±ç›¸é–¢ã«ãªã‚‹\n",
    "- è¤‡é›‘ãªé–¢ä¿‚æ€§ã‚’å­¦ç¿’ã§ããªã„\n",
    "- è¡¨ç¾åŠ›ãŒå¤§å¹…ã«ä½ä¸‹ã™ã‚‹\n",
    "\n",
    "##### 4. **æ¬¡å…ƒå¤‰æ›ã®è‡ªç”±åº¦**\n",
    "\n",
    "ç·šå½¢å¤‰æ›ã«ã‚ˆã‚Šã€å…¥åŠ›ã¨ç•°ãªã‚‹æ¬¡å…ƒã®Q, K, Vã‚’ç”Ÿæˆã§ãã¾ã™ï¼ˆç‰¹ã«Multi-Head Attentionã§é‡è¦ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fd0265",
   "metadata": {},
   "source": [
    "#### æ¯”è¼ƒå®Ÿé¨“: ç·šå½¢å¤‰æ›ã‚ã‚Švsç·šå½¢å¤‰æ›ãªã—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "99e2f431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å…¥åŠ› x:\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.]])\n",
      "\n",
      "============================================================\n",
      "ãƒ‘ã‚¿ãƒ¼ãƒ³1: ç·šå½¢å¤‰æ›ãªã— (Q = K = V = x)\n",
      "============================================================\n",
      "Attentioné‡ã¿:\n",
      "tensor([[0.4519, 0.2741, 0.2741],\n",
      "        [0.2741, 0.4519, 0.2741],\n",
      "        [0.2741, 0.2741, 0.4519]])\n",
      "\n",
      "å‡ºåŠ›:\n",
      "tensor([[0.4519, 0.2741, 0.2741, 0.0000],\n",
      "        [0.2741, 0.4519, 0.2741, 0.0000],\n",
      "        [0.2741, 0.2741, 0.4519, 0.0000]])\n",
      "\n",
      "â†’ Q, K, VãŒåŒã˜ãªã®ã§ã€Attentioné‡ã¿ã¯å˜ä½è¡Œåˆ—ã«è¿‘ããªã‚‹\n",
      "  ï¼ˆå„å˜èªãŒè‡ªåˆ†è‡ªèº«ã ã‘ã‚’å¼·ãè¦‹ã‚‹å‚¾å‘ï¼‰\n",
      "\n",
      "============================================================\n",
      "ãƒ‘ã‚¿ãƒ¼ãƒ³2: ç·šå½¢å¤‰æ›ã‚ã‚Š\n",
      "============================================================\n",
      "å¤‰æ›å¾Œã®Query:\n",
      "tensor([[-0.2511, -0.1070,  0.4648, -0.2016],\n",
      "        [ 0.1678, -0.1392, -0.0421,  0.4507],\n",
      "        [ 0.2406,  0.3260,  0.2089,  0.1084]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "å¤‰æ›å¾Œã®Key:\n",
      "tensor([[ 0.1900,  0.3202,  0.0652,  0.4665],\n",
      "        [ 0.2541,  0.2216,  0.1829,  0.2462],\n",
      "        [-0.3990,  0.0568,  0.2020, -0.0690]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "Attentioné‡ã¿:\n",
      "tensor([[0.3089, 0.3237, 0.3675],\n",
      "        [0.3549, 0.3410, 0.3041],\n",
      "        [0.3483, 0.3455, 0.3063]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "å‡ºåŠ›:\n",
      "tensor([[-0.2888, -0.2399,  0.0518,  0.0343],\n",
      "        [-0.3073, -0.2421,  0.0619,  0.0247],\n",
      "        [-0.3072, -0.2431,  0.0581,  0.0274]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "â†’ Q, K, VãŒç•°ãªã‚‹ã®ã§ã€ã‚ˆã‚Šè¤‡é›‘ãªé–¢ä¿‚æ€§ã‚’å­¦ç¿’ã§ãã‚‹\n",
      "  ï¼ˆå„å˜èªãŒä»–ã®å˜èªã¨ã®é–¢ä¿‚ã‚’æŸ”è»Ÿã«å­¦ç¿’ï¼‰\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# å…¥åŠ›ãƒ‡ãƒ¼ã‚¿\n",
    "batch_size = 1\n",
    "seq_len = 3\n",
    "d_model = 4\n",
    "\n",
    "x = torch.tensor([\n",
    "    [[1.0, 0.0, 0.0, 0.0],  # å˜èª1\n",
    "     [0.0, 1.0, 0.0, 0.0],  # å˜èª2\n",
    "     [0.0, 0.0, 1.0, 0.0]]  # å˜èª3\n",
    "])\n",
    "\n",
    "print(\"å…¥åŠ› x:\")\n",
    "print(x[0])\n",
    "print()\n",
    "\n",
    "# ãƒ‘ã‚¿ãƒ¼ãƒ³1: ç·šå½¢å¤‰æ›ãªã—ï¼ˆQ=K=V=xï¼‰\n",
    "print(\"=\" * 60)\n",
    "print(\"ãƒ‘ã‚¿ãƒ¼ãƒ³1: ç·šå½¢å¤‰æ›ãªã— (Q = K = V = x)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "Q1 = x\n",
    "K1 = x\n",
    "V1 = x\n",
    "\n",
    "# Attentionè¨ˆç®—\n",
    "scores1 = torch.matmul(Q1, K1.transpose(-2, -1)) / math.sqrt(d_model)\n",
    "attention_weights1 = F.softmax(scores1, dim=-1)\n",
    "output1 = torch.matmul(attention_weights1, V1)\n",
    "\n",
    "print(\"Attentioné‡ã¿:\")\n",
    "print(attention_weights1[0])\n",
    "print(\"\\nå‡ºåŠ›:\")\n",
    "print(output1[0])\n",
    "print(\"\\nâ†’ Q, K, VãŒåŒã˜ãªã®ã§ã€Attentioné‡ã¿ã¯å˜ä½è¡Œåˆ—ã«è¿‘ããªã‚‹\")\n",
    "print(\"  ï¼ˆå„å˜èªãŒè‡ªåˆ†è‡ªèº«ã ã‘ã‚’å¼·ãè¦‹ã‚‹å‚¾å‘ï¼‰\")\n",
    "\n",
    "# ãƒ‘ã‚¿ãƒ¼ãƒ³2: ç·šå½¢å¤‰æ›ã‚ã‚Š\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ãƒ‘ã‚¿ãƒ¼ãƒ³2: ç·šå½¢å¤‰æ›ã‚ã‚Š\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ç•°ãªã‚‹ç·šå½¢å¤‰æ›ã‚’å®šç¾©\n",
    "W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "# å­¦ç¿’æ¸ˆã¿ã®é‡ã¿ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆå®Ÿéš›ã«ã¯å­¦ç¿’ã§æœ€é©åŒ–ã•ã‚Œã‚‹ï¼‰\n",
    "torch.manual_seed(42)\n",
    "Q2 = W_q(x)\n",
    "K2 = W_k(x)\n",
    "V2 = W_v(x)\n",
    "\n",
    "print(\"å¤‰æ›å¾Œã®Query:\")\n",
    "print(Q2[0])\n",
    "print(\"\\nå¤‰æ›å¾Œã®Key:\")\n",
    "print(K2[0])\n",
    "\n",
    "# Attentionè¨ˆç®—\n",
    "scores2 = torch.matmul(Q2, K2.transpose(-2, -1)) / math.sqrt(d_model)\n",
    "attention_weights2 = F.softmax(scores2, dim=-1)\n",
    "output2 = torch.matmul(attention_weights2, V2)\n",
    "\n",
    "print(\"\\nAttentioné‡ã¿:\")\n",
    "print(attention_weights2[0])\n",
    "print(\"\\nå‡ºåŠ›:\")\n",
    "print(output2[0])\n",
    "print(\"\\nâ†’ Q, K, VãŒç•°ãªã‚‹ã®ã§ã€ã‚ˆã‚Šè¤‡é›‘ãªé–¢ä¿‚æ€§ã‚’å­¦ç¿’ã§ãã‚‹\")\n",
    "print(\"  ï¼ˆå„å˜èªãŒä»–ã®å˜èªã¨ã®é–¢ä¿‚ã‚’æŸ”è»Ÿã«å­¦ç¿’ï¼‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c443856",
   "metadata": {},
   "source": [
    "#### å…·ä½“çš„ãªä¾‹: æ–‡ç« ç†è§£ã§ã®å½¹å‰²\n",
    "\n",
    "ä¾‹ãˆã°ã€ŒçŒ«ãŒé­šã‚’é£Ÿã¹ãŸã€ã¨ã„ã†æ–‡ã‚’å‡¦ç†ã™ã‚‹å ´åˆï¼š\n",
    "\n",
    "##### ç·šå½¢å¤‰æ›ãªã—ï¼ˆQ=K=Vï¼‰ã®å ´åˆ:\n",
    "- ã€ŒçŒ«ã€ã¯ã€ŒçŒ«ã€è‡ªèº«ã¨ã—ã‹é–¢ä¿‚æ€§ã‚’æŒã¦ãªã„\n",
    "- ã€Œé£Ÿã¹ãŸã€ã¯ã€Œé£Ÿã¹ãŸã€è‡ªèº«ã¨ã—ã‹é–¢ä¿‚æ€§ã‚’æŒã¦ãªã„\n",
    "- æ–‡æ³•çš„ãƒ»æ„å‘³çš„ãªé–¢ä¿‚ã‚’å­¦ç¿’ã§ããªã„\n",
    "\n",
    "##### ç·šå½¢å¤‰æ›ã‚ã‚Šï¼ˆQâ‰ Kâ‰ Vï¼‰ã®å ´åˆ:\n",
    "- **Queryå¤‰æ›**: ã€ŒçŒ«ã€ã‹ã‚‰ã€Œä¸»èªã¨ã—ã¦ã®çŒ«ã€ã‚’æŠ½å‡º\n",
    "- **Keyå¤‰æ›**: ã€Œé£Ÿã¹ãŸã€ã‹ã‚‰ã€Œè¿°èªã¨ã—ã¦ã®é£Ÿã¹ãŸã€ã‚’æŠ½å‡º\n",
    "- **Attentionè¨ˆç®—**: ã€Œä¸»èªã®çŒ«ã€ã¨ã€Œè¿°èªã®é£Ÿã¹ãŸã€ãŒå¼·ãé–¢é€£\n",
    "- **Valueå¤‰æ›**: å®Ÿéš›ã«ä¼é”ã™ã‚‹æƒ…å ±ã‚’æœ€é©åŒ–\n",
    "\n",
    "ã“ã‚Œã«ã‚ˆã‚Šã€æ–‡æ³•æ§‹é€ ã‚„æ„å‘³çš„ãªé–¢ä¿‚æ€§ã‚’å­¦ç¿’ã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efb43ab",
   "metadata": {},
   "source": [
    "#### æ•°å­¦çš„ãªè¦–ç‚¹\n",
    "\n",
    "**ç·šå½¢å¤‰æ›ã®å½¹å‰²**:\n",
    "\n",
    "$$\n",
    "Q = XW_q, \\quad K = XW_k, \\quad V = XW_v\n",
    "$$\n",
    "\n",
    "- $X$: å…¥åŠ›ï¼ˆå…¨å˜èªã§åŒã˜ï¼‰\n",
    "- $W_q, W_k, W_v$: ç•°ãªã‚‹é‡ã¿è¡Œåˆ—ï¼ˆå­¦ç¿’å¯èƒ½ï¼‰\n",
    "- ã“ã‚Œã«ã‚ˆã‚Šã€åŒã˜å…¥åŠ›ã‹ã‚‰ç•°ãªã‚‹è¡¨ç¾ã‚’ç”Ÿæˆ\n",
    "\n",
    "**Attentionè¨ˆç®—**:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\text{softmax}\\left(\\frac{(XW_q)(XW_k)^T}{\\sqrt{d_k}}\\right)(XW_v)\n",
    "$$\n",
    "\n",
    "$W_q, W_k, W_v$ãŒç•°ãªã‚‹ â†’ ã‚ˆã‚Šè±Šã‹ãªè¡¨ç¾ãŒå¯èƒ½"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0b461d",
   "metadata": {},
   "source": [
    "#### ã¾ã¨ã‚\n",
    "\n",
    "**Q, K, Vã«ç·šå½¢å¤‰æ›ã‚’é€šã™ç†ç”±**:\n",
    "\n",
    "1. **ç•°ãªã‚‹å½¹å‰²ã®å­¦ç¿’**\n",
    "   - Query: ã€Œä½•ã‚’æ¢ã™ã‹ã€\n",
    "   - Key: ã€Œä½•ã‚’æŒã£ã¦ã„ã‚‹ã‹ã€\n",
    "   - Value: ã€Œä½•ã‚’ä¼ãˆã‚‹ã‹ã€\n",
    "   - å„å¤‰æ›ãŒç‹¬ç«‹ã—ãŸé‡ã¿è¡Œåˆ—ã§ç•°ãªã‚‹ç‰¹å¾´ã‚’æŠ½å‡º\n",
    "\n",
    "2. **å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**\n",
    "   - ã‚¿ã‚¹ã‚¯ã«å¿œã˜ã¦æœ€é©ãªQ, K, Vã®ç”Ÿæˆæ–¹æ³•ã‚’å­¦ç¿’\n",
    "   - ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è‡ªå‹•çš„ã«æœ‰ç”¨ãªè¡¨ç¾ã‚’ç²å¾—\n",
    "\n",
    "3. **è¡¨ç¾åŠ›ã®å‘ä¸Š**\n",
    "   - ç·šå½¢å¤‰æ›ãªã—: å˜ç´”ãªè‡ªå·±ç›¸é–¢ã®ã¿\n",
    "   - ç·šå½¢å¤‰æ›ã‚ã‚Š: è¤‡é›‘ãªé–¢ä¿‚æ€§ã‚’å­¦ç¿’å¯èƒ½\n",
    "\n",
    "4. **æŸ”è»Ÿæ€§**\n",
    "   - æ¬¡å…ƒã®å¤‰æ›ãŒå¯èƒ½\n",
    "   - Multi-Head Attentionã§ç‰¹ã«é‡è¦\n",
    "\n",
    "**é¡ä¼¼ä¾‹**:\n",
    "ç•³ã¿è¾¼ã¿å±¤ã§ã‚‚ã€åŒã˜ç”»åƒã‹ã‚‰ç•°ãªã‚‹ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆedgeæ¤œå‡ºã€è‰²æ¤œå‡ºãªã©ï¼‰ã§ç•°ãªã‚‹ç‰¹å¾´ã‚’æŠ½å‡ºã™ã‚‹ã®ã¨åŒã˜è€ƒãˆæ–¹ã§ã™ã€‚\n",
    "\n",
    "**å®Ÿè£…**:\n",
    "```python\n",
    "# src/attention.py\n",
    "self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "```\n",
    "\n",
    "ç·šå½¢å¤‰æ›ã¯ã€AttentionãŒæŸ”è»Ÿã§å¼·åŠ›ãªå­¦ç¿’æ©Ÿæ§‹ã¨ãªã‚‹ãŸã‚ã®**æ ¸å¿ƒçš„ãªä»•çµ„ã¿**ã§ã™ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93ee672",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q13: Multi-Head Attentionã§ã•ã‚‰ã«Q, K, Vã‚’ç·šå½¢å¤‰æ›ã™ã‚‹ã®ã¯ãªãœï¼Ÿ\n",
    "\n",
    "**è³ªå•æ—¥**: 2025å¹´11æœˆ17æ—¥\n",
    "\n",
    "### è³ªå•\n",
    "Multi-Head Attentionã§ã¯ã€ã™ã§ã«Q, K, VãŒç”Ÿæˆã•ã‚ŒãŸå¾Œã€ã•ã‚‰ã«å„headã”ã¨ã«ç·šå½¢å¤‰æ›å±¤ï¼ˆ`W_q`, `W_k`, `W_v`ï¼‰ã«å…¥åŠ›ã—ã¦ã„ã¾ã™ã€‚Q, K, Vã¯ã™ã§ã«ç•°ãªã‚‹ã‚‚ã®ãªã®ã«ã€ãªãœã•ã‚‰ã«ç·šå½¢å¤‰æ›ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã®ã§ã™ã‹ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d894915",
   "metadata": {},
   "source": [
    "### å›ç­”\n",
    "\n",
    "Multi-Head Attentionã®ç·šå½¢å¤‰æ›ã¯ã€**å„headãŒç•°ãªã‚‹è¡¨ç¾éƒ¨åˆ†ç©ºé–“ã‚’å­¦ç¿’ã™ã‚‹**ãŸã‚ã®ä»•çµ„ã¿ã§ã™ã€‚\n",
    "\n",
    "#### æ§‹é€ ã®ç†è§£\n",
    "\n",
    "å®Ÿã¯ã€å®Ÿè£…æ–¹æ³•ã«ã‚ˆã£ã¦è¦‹ãˆæ–¹ãŒç•°ãªã‚Šã¾ã™ãŒã€æ¦‚å¿µçš„ã«ã¯åŒã˜ã“ã¨ã‚’ã—ã¦ã„ã¾ã™ï¼š\n",
    "\n",
    "##### ãƒ‘ã‚¿ãƒ¼ãƒ³1: 1ã¤ã®å¤§ããªç·šå½¢å¤‰æ› â†’ åˆ†å‰²ï¼ˆæˆ‘ã€…ã®å®Ÿè£…ï¼‰\n",
    "\n",
    "```python\n",
    "# 1ã¤ã®å¤§ããªW_q (d_model â†’ d_model)\n",
    "Q = self.W_q(x)  # [batch, seq_len, d_model]\n",
    "\n",
    "# è¤‡æ•°headã«åˆ†å‰²\n",
    "Q = self.split_heads(Q, batch_size)  # [batch, num_heads, seq_len, d_k]\n",
    "```\n",
    "\n",
    "##### ãƒ‘ã‚¿ãƒ¼ãƒ³2: å„headã”ã¨ã«ç‹¬ç«‹ã—ãŸå°ã•ãªç·šå½¢å¤‰æ›\n",
    "\n",
    "```python\n",
    "# å„headãŒç‹¬ç«‹ã—ãŸW_q_i (d_model â†’ d_k)\n",
    "head_1 = self.W_q_1(x)  # [batch, seq_len, d_k]\n",
    "head_2 = self.W_q_2(x)  # [batch, seq_len, d_k]\n",
    "...\n",
    "head_8 = self.W_q_8(x)  # [batch, seq_len, d_k]\n",
    "```\n",
    "\n",
    "**é‡è¦**: ã©ã¡ã‚‰ã‚‚æ•°å­¦çš„ã«ã¯åŒã˜ï¼ãƒ‘ã‚¿ãƒ¼ãƒ³1ã¯è¨ˆç®—åŠ¹ç‡ã®ãŸã‚ã«å…¨headã‚’ã¾ã¨ã‚ã¦å‡¦ç†ã—ã¦ã„ã‚‹ã ã‘ã§ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3f31c1",
   "metadata": {},
   "source": [
    "#### ãªãœå„headã«ç•°ãªã‚‹ç·šå½¢å¤‰æ›ãŒå¿…è¦ã‹\n",
    "\n",
    "##### 1. **ç•°ãªã‚‹è¡¨ç¾éƒ¨åˆ†ç©ºé–“ã‚’å­¦ç¿’**\n",
    "\n",
    "å„headã¯ã€**åŒã˜å…¥åŠ›ã‹ã‚‰ç•°ãªã‚‹è¦–ç‚¹ã§æƒ…å ±ã‚’æŠ½å‡º**ã—ã¾ã™ï¼š\n",
    "\n",
    "- **Head 1**: æ–‡æ³•çš„ãªé–¢ä¿‚æ€§ã‚’æ‰ãˆã‚‹ï¼ˆä¸»èª-è¿°èªãªã©ï¼‰\n",
    "- **Head 2**: æ„å‘³çš„ãªé¡ä¼¼æ€§ã‚’æ‰ãˆã‚‹\n",
    "- **Head 3**: ä½ç½®çš„ãªè¿‘ã•ã‚’æ‰ãˆã‚‹\n",
    "- ...\n",
    "\n",
    "å„headãŒç‹¬ç«‹ã—ãŸé‡ã¿è¡Œåˆ—ã‚’æŒã¤ã“ã¨ã§ã€ç•°ãªã‚‹ç¨®é¡ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã§ãã¾ã™ã€‚\n",
    "\n",
    "##### 2. **åŒã˜å¤‰æ›ã‚’ä½¿ã†ã¨æ„å‘³ãŒãªã„**\n",
    "\n",
    "ã‚‚ã—å…¨headãŒåŒã˜ç·šå½¢å¤‰æ›ã‚’ä½¿ã£ãŸã‚‰ï¼š\n",
    "\n",
    "```python\n",
    "# å…¨headãŒåŒã˜W_qã‚’ä½¿ã†å ´åˆ\n",
    "Q = self.W_q(x)  # å…¨headã§åŒã˜å¤‰æ›\n",
    "# â†“ ã“ã‚Œã§ã¯å…¨headãŒåŒã˜æƒ…å ±ã—ã‹è¦‹ã‚‰ã‚Œãªã„ï¼\n",
    "```\n",
    "\n",
    "â†’ Multi-Headã«ã™ã‚‹æ„å‘³ãŒãªããªã‚Šã¾ã™\n",
    "\n",
    "##### 3. **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç‹¬ç«‹æ€§**\n",
    "\n",
    "å„headã¯**ç‹¬ç«‹ã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**ã‚’æŒã¡ã¾ã™ï¼š\n",
    "\n",
    "```\n",
    "Head 1: W_q[0:64, :]ã®éƒ¨åˆ†ã‚’ä½¿ç”¨\n",
    "Head 2: W_q[64:128, :]ã®éƒ¨åˆ†ã‚’ä½¿ç”¨\n",
    "...\n",
    "Head 8: W_q[448:512, :]ã®éƒ¨åˆ†ã‚’ä½¿ç”¨\n",
    "```\n",
    "\n",
    "å¤§ããª1ã¤ã®è¡Œåˆ—ã¨ã—ã¦å®Ÿè£…ã—ã¦ã„ã¾ã™ãŒã€å„headã¯ç•°ãªã‚‹éƒ¨åˆ†ã‚’ä½¿ã†ãŸã‚ã€å®Ÿè³ªçš„ã«ç‹¬ç«‹ã—ãŸå¤‰æ›ã«ãªã‚Šã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1193a3",
   "metadata": {},
   "source": [
    "#### ã‚³ãƒ¼ãƒ‰ä¾‹: é‡ã¿è¡Œåˆ—ã®åˆ†å‰²ã‚’ç¢ºèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d290f7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_qã®å½¢çŠ¶: torch.Size([512, 512])\n",
      "  - å…¥åŠ›æ¬¡å…ƒ: 512\n",
      "  - å‡ºåŠ›æ¬¡å…ƒ: 512\n",
      "\n",
      "å¤‰æ›å¾Œã®Q: torch.Size([1, 5, 512])\n",
      "åˆ†å‰²å¾Œã®Q: torch.Size([1, 8, 5, 64])\n",
      "\n",
      "å„headãŒä½¿ç”¨ã™ã‚‹é‡ã¿è¡Œåˆ—ã®éƒ¨åˆ†:\n",
      "  Head 1: W_q[0:64, :] ã®éƒ¨åˆ†\n",
      "           å‡ºåŠ›ã®0ï½63æ¬¡å…ƒç›®ã‚’ç”Ÿæˆ\n",
      "  Head 2: W_q[64:128, :] ã®éƒ¨åˆ†\n",
      "           å‡ºåŠ›ã®64ï½127æ¬¡å…ƒç›®ã‚’ç”Ÿæˆ\n",
      "  Head 3: W_q[128:192, :] ã®éƒ¨åˆ†\n",
      "           å‡ºåŠ›ã®128ï½191æ¬¡å…ƒç›®ã‚’ç”Ÿæˆ\n",
      "  Head 4: W_q[192:256, :] ã®éƒ¨åˆ†\n",
      "           å‡ºåŠ›ã®192ï½255æ¬¡å…ƒç›®ã‚’ç”Ÿæˆ\n",
      "  Head 5: W_q[256:320, :] ã®éƒ¨åˆ†\n",
      "           å‡ºåŠ›ã®256ï½319æ¬¡å…ƒç›®ã‚’ç”Ÿæˆ\n",
      "  Head 6: W_q[320:384, :] ã®éƒ¨åˆ†\n",
      "           å‡ºåŠ›ã®320ï½383æ¬¡å…ƒç›®ã‚’ç”Ÿæˆ\n",
      "  Head 7: W_q[384:448, :] ã®éƒ¨åˆ†\n",
      "           å‡ºåŠ›ã®384ï½447æ¬¡å…ƒç›®ã‚’ç”Ÿæˆ\n",
      "  Head 8: W_q[448:512, :] ã®éƒ¨åˆ†\n",
      "           å‡ºåŠ›ã®448ï½511æ¬¡å…ƒç›®ã‚’ç”Ÿæˆ\n",
      "\n",
      "ğŸ’¡ ãƒã‚¤ãƒ³ãƒˆ:\n",
      "  - 1ã¤ã®å¤§ããªW_q (512Ã—512) ã‚’ä½¿ç”¨\n",
      "  - ã—ã‹ã—å„headã¯ç•°ãªã‚‹å‡ºåŠ›æ¬¡å…ƒï¼ˆ64æ¬¡å…ƒåˆ†ï¼‰ã‚’æ‹…å½“\n",
      "  - çµæœçš„ã«ã€å„headã¯ç‹¬ç«‹ã—ãŸå¤‰æ›ã‚’è¡Œã†\n",
      "  - ã“ã‚Œã«ã‚ˆã‚Šå„headãŒç•°ãªã‚‹è¡¨ç¾éƒ¨åˆ†ç©ºé–“ã‚’å­¦ç¿’\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_k = d_model // num_heads  # 64\n",
    "\n",
    "# Multi-Head Attentionã®W_q\n",
    "W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "print(\"W_qã®å½¢çŠ¶:\", W_q.weight.shape)\n",
    "print(f\"  - å…¥åŠ›æ¬¡å…ƒ: {d_model}\")\n",
    "print(f\"  - å‡ºåŠ›æ¬¡å…ƒ: {d_model}\")\n",
    "print()\n",
    "\n",
    "# å…¥åŠ›\n",
    "x = torch.randn(1, 5, d_model)  # [batch=1, seq_len=5, d_model=512]\n",
    "\n",
    "# ç·šå½¢å¤‰æ›\n",
    "Q = W_q(x)  # [1, 5, 512]\n",
    "print(\"å¤‰æ›å¾Œã®Q:\", Q.shape)\n",
    "\n",
    "# å„headã«åˆ†å‰²\n",
    "Q = Q.view(1, 5, num_heads, d_k)  # [1, 5, 8, 64]\n",
    "Q = Q.transpose(1, 2)  # [1, 8, 5, 64]\n",
    "print(\"åˆ†å‰²å¾Œã®Q:\", Q.shape)\n",
    "print()\n",
    "\n",
    "# å„headãŒä½¿ã†é‡ã¿ã®éƒ¨åˆ†ã‚’ç¢ºèª\n",
    "print(\"å„headãŒä½¿ç”¨ã™ã‚‹é‡ã¿è¡Œåˆ—ã®éƒ¨åˆ†:\")\n",
    "for i in range(num_heads):\n",
    "    start = i * d_k\n",
    "    end = (i + 1) * d_k\n",
    "    print(f\"  Head {i+1}: W_q[{start}:{end}, :] ã®éƒ¨åˆ†\")\n",
    "    print(f\"           å‡ºåŠ›ã®{start}ï½{end-1}æ¬¡å…ƒç›®ã‚’ç”Ÿæˆ\")\n",
    "\n",
    "print()\n",
    "print(\"ğŸ’¡ ãƒã‚¤ãƒ³ãƒˆ:\")\n",
    "print(\"  - 1ã¤ã®å¤§ããªW_q (512Ã—512) ã‚’ä½¿ç”¨\")\n",
    "print(\"  - ã—ã‹ã—å„headã¯ç•°ãªã‚‹å‡ºåŠ›æ¬¡å…ƒï¼ˆ64æ¬¡å…ƒåˆ†ï¼‰ã‚’æ‹…å½“\")\n",
    "print(\"  - çµæœçš„ã«ã€å„headã¯ç‹¬ç«‹ã—ãŸå¤‰æ›ã‚’è¡Œã†\")\n",
    "print(\"  - ã“ã‚Œã«ã‚ˆã‚Šå„headãŒç•°ãªã‚‹è¡¨ç¾éƒ¨åˆ†ç©ºé–“ã‚’å­¦ç¿’\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0be20c2",
   "metadata": {},
   "source": [
    "#### æ•°å­¦çš„ãªèª¬æ˜\n",
    "\n",
    "Multi-Head Attentionã®æ•°å¼ï¼š\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O\n",
    "$$\n",
    "\n",
    "å„headã¯ï¼š\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "$$\n",
    "\n",
    "**é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ**:\n",
    "\n",
    "1. **$W_i^Q$ã¯å„headã”ã¨ã«ç•°ãªã‚‹**\n",
    "   - Head 1: $W_1^Q$ (d_model â†’ d_k)\n",
    "   - Head 2: $W_2^Q$ (d_model â†’ d_k)\n",
    "   - ...\n",
    "   - Head 8: $W_8^Q$ (d_model â†’ d_k)\n",
    "\n",
    "2. **å®Ÿè£…ä¸Šã¯1ã¤ã®å¤§ããªè¡Œåˆ—**\n",
    "   - å®Ÿè£…: $W^Q$ (d_model â†’ d_model)\n",
    "   - åˆ†å‰²å¾Œ: å„headãŒç•°ãªã‚‹éƒ¨åˆ†ã‚’ä½¿ç”¨\n",
    "\n",
    "3. **ãªãœã“ã‚ŒãŒå¿…è¦ã‹**\n",
    "   - åŒã˜å…¥åŠ› $Q$ ã‹ã‚‰ã€å„headãŒç•°ãªã‚‹å°„å½±ã‚’ä½œã‚‹\n",
    "   - å„headãŒç•°ãªã‚‹ã€Œè¦–ç‚¹ã€ã§æƒ…å ±ã‚’è¦‹ã‚‹\n",
    "   - ã“ã‚Œã«ã‚ˆã‚Šå¤šæ§˜ãªé–¢ä¿‚æ€§ã‚’ä¸¦åˆ—ã«å­¦ç¿’"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a353d66a",
   "metadata": {},
   "source": [
    "#### å…·ä½“ä¾‹: ç”»åƒå‡¦ç†ã¨ã®é¡ä¼¼æ€§\n",
    "\n",
    "ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆCNNï¼‰ã¨ä¼¼ã¦ã„ã¾ã™ï¼š\n",
    "\n",
    "**CNNã®å ´åˆ**:\n",
    "- åŒã˜å…¥åŠ›ç”»åƒã«è¤‡æ•°ã®ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨\n",
    "- Filter 1: ã‚¨ãƒƒã‚¸æ¤œå‡º\n",
    "- Filter 2: è‰²æ¤œå‡º\n",
    "- Filter 3: ãƒ†ã‚¯ã‚¹ãƒãƒ£æ¤œå‡º\n",
    "- å„ãƒ•ã‚£ãƒ«ã‚¿ã¯ç•°ãªã‚‹ç‰¹å¾´ã‚’æŠ½å‡º\n",
    "\n",
    "**Multi-Head Attentionã®å ´åˆ**:\n",
    "- åŒã˜å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«è¤‡æ•°ã®headï¼ˆå¤‰æ›ï¼‰ã‚’é©ç”¨\n",
    "- Head 1: æ–‡æ³•é–¢ä¿‚ã‚’æ‰ãˆã‚‹\n",
    "- Head 2: æ„å‘³é–¢ä¿‚ã‚’æ‰ãˆã‚‹\n",
    "- Head 3: ä½ç½®é–¢ä¿‚ã‚’æ‰ãˆã‚‹\n",
    "- å„headã¯ç•°ãªã‚‹é–¢ä¿‚æ€§ã‚’å­¦ç¿’\n",
    "\n",
    "ã©ã¡ã‚‰ã‚‚ã€ŒåŒã˜å…¥åŠ›ã‹ã‚‰ã€è¤‡æ•°ã®ç•°ãªã‚‹è¦–ç‚¹ã§ç‰¹å¾´ã‚’æŠ½å‡ºã™ã‚‹ã€ã¨ã„ã†å…±é€šã®è€ƒãˆæ–¹ã§ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b941eb42",
   "metadata": {},
   "source": [
    "#### ã¾ã¨ã‚\n",
    "\n",
    "**Multi-Head Attentionã§å„headã«ç•°ãªã‚‹ç·šå½¢å¤‰æ›ãŒå¿…è¦ãªç†ç”±**:\n",
    "\n",
    "1. **ç•°ãªã‚‹è¡¨ç¾éƒ¨åˆ†ç©ºé–“ã®å­¦ç¿’**\n",
    "   - å„headãŒç‹¬ç«‹ã—ãŸè¦–ç‚¹ã§æƒ…å ±ã‚’æŠ½å‡º\n",
    "   - æ–‡æ³•ã€æ„å‘³ã€ä½ç½®ãªã©ã€ç•°ãªã‚‹ç¨®é¡ã®é–¢ä¿‚æ€§ã‚’ä¸¦åˆ—ã«å­¦ç¿’\n",
    "\n",
    "2. **å®Ÿè£…ã®å·¥å¤«**\n",
    "   - 1ã¤ã®å¤§ããªç·šå½¢å¤‰æ› (d_model â†’ d_model)\n",
    "   - å‡ºåŠ›ã‚’è¤‡æ•°headã«åˆ†å‰²\n",
    "   - å„headã¯ç•°ãªã‚‹å‡ºåŠ›æ¬¡å…ƒã‚’æ‹…å½“ â†’ å®Ÿè³ªçš„ã«ç•°ãªã‚‹å¤‰æ›\n",
    "\n",
    "3. **åŒã˜å¤‰æ›ã‚’ä½¿ã†ã¨æ„å‘³ãŒãªã„**\n",
    "   - å…¨headãŒåŒã˜å¤‰æ› â†’ åŒã˜æƒ…å ±ã—ã‹è¦‹ã‚‰ã‚Œãªã„\n",
    "   - Multi-Headã«ã™ã‚‹åˆ©ç‚¹ãŒæ¶ˆå¤±\n",
    "\n",
    "4. **æ•°å­¦çš„ã«ã¯**\n",
    "   - å„head $i$ ã¯ç‹¬ç«‹ã—ãŸ $W_i^Q, W_i^K, W_i^V$ ã‚’æŒã¤\n",
    "   - å®Ÿè£…ä¸Šã¯åŠ¹ç‡åŒ–ã®ãŸã‚1ã¤ã®å¤§ããªè¡Œåˆ—ã¨ã—ã¦ã¾ã¨ã‚ã¦ã„ã‚‹\n",
    "   - çµæœã¯åŒã˜\n",
    "\n",
    "**å…·ä½“çš„ãªã‚¤ãƒ¡ãƒ¼ã‚¸**:\n",
    "```\n",
    "å…¥åŠ› x â†’ W_q â†’ [Head1ã®64æ¬¡å…ƒ | Head2ã®64æ¬¡å…ƒ | ... | Head8ã®64æ¬¡å…ƒ]\n",
    "         â†‘         â†‘              â†‘                      â†‘\n",
    "      512Ã—512   ç•°ãªã‚‹é‡ã¿    ç•°ãªã‚‹é‡ã¿          ç•°ãªã‚‹é‡ã¿\n",
    "```\n",
    "\n",
    "å„headã¯**ç‹¬ç«‹ã—ãŸå­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**ã‚’æŒã¡ã€**ç•°ãªã‚‹ç‰¹å¾´**ã‚’æŠ½å‡ºã—ã¾ã™ã€‚ã“ã‚ŒãŒMulti-Head Attentionã®æœ¬è³ªã§ã™ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9e540a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q14: Multi-Head Attentionã®Q, K, Vã‚‚å…¥åŠ›ã®ç·šå½¢å¤‰æ›ã¨è€ƒãˆã¦è‰¯ã„ã‹ï¼Ÿ\n",
    "\n",
    "**è³ªå•æ—¥**: 2025å¹´11æœˆ18æ—¥\n",
    "\n",
    "### è³ªå•\n",
    "Multi-Head Attentionã§ä½¿ã†Q, K, Vã‚‚ã€01_self_attention_demo.ipynbã§å­¦ã‚“ã ã‚ˆã†ã«ã€å…¥åŠ›ã‚’ç·šå½¢å¤‰æ›ã—ãŸã‚‚ã®ã¨è€ƒãˆã¦è‰¯ã„ã®ã§ã—ã‚‡ã†ã‹ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5b2299",
   "metadata": {},
   "source": [
    "### å›ç­”\n",
    "\n",
    "**ã¯ã„ã€ãã®é€šã‚Šã§ã™ï¼** Multi-Head Attentionã®Q, K, Vã‚‚ã€Self-Attentionã¨å…¨ãåŒã˜ã‚ˆã†ã«ã€**å…¥åŠ›ã‚’ç·šå½¢å¤‰æ›ã—ãŸã‚‚ã®**ã§ã™ã€‚\n",
    "\n",
    "#### Self-Attention (01) ã¨ Multi-Head Attention (02) ã®é–¢ä¿‚\n",
    "\n",
    "##### Self-Attention (01ã®å®Ÿè£…):\n",
    "\n",
    "```python\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # å…¥åŠ›xã‹ã‚‰ç·šå½¢å¤‰æ›ã§Q, K, Vã‚’ç”Ÿæˆ\n",
    "        Q = self.W_q(x)  # [batch, seq_len, d_model]\n",
    "        K = self.W_k(x)  # [batch, seq_len, d_model]\n",
    "        V = self.W_v(x)  # [batch, seq_len, d_model]\n",
    "        \n",
    "        # Attentionã‚’è¨ˆç®—\n",
    "        output = self.attention(Q, K, V)\n",
    "        return output\n",
    "```\n",
    "\n",
    "##### Multi-Head Attention (02ã®å®Ÿè£…):\n",
    "\n",
    "```python\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    def forward(self, x, x, x):\n",
    "        # å…¥åŠ›xã‹ã‚‰ç·šå½¢å¤‰æ›ã§Q, K, Vã‚’ç”Ÿæˆï¼ˆSelf-Attentionã¨åŒã˜ï¼ï¼‰\n",
    "        Q = self.W_q(x)  # [batch, seq_len, d_model]\n",
    "        K = self.W_k(x)  # [batch, seq_len, d_model]\n",
    "        V = self.W_v(x)  # [batch, seq_len, d_model]\n",
    "        \n",
    "        # ã“ã“ã‹ã‚‰ãŒMulti-Headç‰¹æœ‰ã®å‡¦ç†\n",
    "        # è¤‡æ•°headã«åˆ†å‰²\n",
    "        Q = self.split_heads(Q)  # [batch, num_heads, seq_len, d_k]\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "        \n",
    "        # å„headã§Attentionã‚’è¨ˆç®—\n",
    "        output = self.attention(Q, K, V)\n",
    "        return output\n",
    "```\n",
    "\n",
    "**é•ã„ã¯åˆ†å‰²ã™ã‚‹ã‹ã©ã†ã‹ã ã‘ï¼**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb0b71b",
   "metadata": {},
   "source": [
    "#### é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ\n",
    "\n",
    "##### 1. **åŸºæœ¬ã¯åŒã˜**\n",
    "- ã©ã¡ã‚‰ã‚‚å…¥åŠ› `x` ã‚’ç·šå½¢å¤‰æ›ã—ã¦ Q, K, V ã‚’ç”Ÿæˆ\n",
    "- `W_q`, `W_k`, `W_v` ã¨ã„ã†å­¦ç¿’å¯èƒ½ãªé‡ã¿è¡Œåˆ—ã‚’ä½¿ç”¨\n",
    "- ç·šå½¢å¤‰æ›ã®æ•°å¼ã‚‚åŒã˜: $Q = xW_q$, $K = xW_k$, $V = xW_v$\n",
    "\n",
    "##### 2. **é•ã„ã¯å¾Œå‡¦ç†**\n",
    "- **Self-Attention**: Q, K, Vã‚’ãã®ã¾ã¾ä½¿ã†\n",
    "- **Multi-Head Attention**: Q, K, Vã‚’è¤‡æ•°headã«åˆ†å‰²ã—ã¦ã‹ã‚‰ä½¿ã†\n",
    "\n",
    "##### 3. **æ¦‚å¿µçš„ãªæµã‚Œ**\n",
    "\n",
    "```\n",
    "å…¥åŠ› x\n",
    "  â†“\n",
    "ç·šå½¢å¤‰æ› (W_q, W_k, W_v) â† ã“ã“ã¯01ã‚‚02ã‚‚åŒã˜ï¼\n",
    "  â†“\n",
    "Q, K, V ã‚’ç”Ÿæˆ\n",
    "  â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 01: Self    â”‚ 02: Multi   â”‚\n",
    "â”‚             â”‚             â”‚\n",
    "â”‚ ãã®ã¾ã¾    â”‚ åˆ†å‰²        â”‚\n",
    "â”‚ Attention   â”‚ â†“           â”‚\n",
    "â”‚             â”‚ å„headã§    â”‚\n",
    "â”‚             â”‚ Attention   â”‚\n",
    "â”‚             â”‚ â†“           â”‚\n",
    "â”‚             â”‚ çµåˆ        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71211e55",
   "metadata": {},
   "source": [
    "#### ã‚³ãƒ¼ãƒ‰ä¾‹: 01ã¨02ã®æ¯”è¼ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "98e87063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "01: Self-Attention\n",
      "======================================================================\n",
      "å…¥åŠ› x: torch.Size([2, 5, 512])\n",
      "Q: torch.Size([2, 5, 512])\n",
      "K: torch.Size([2, 5, 512])\n",
      "V: torch.Size([2, 5, 512])\n",
      "â†’ ã“ã®ã¾ã¾Attentionã«ä½¿ã†\n",
      "\n",
      "======================================================================\n",
      "02: Multi-Head Attention\n",
      "======================================================================\n",
      "å…¥åŠ› x: torch.Size([2, 5, 512])\n",
      "Q (å¤‰æ›ç›´å¾Œ): torch.Size([2, 5, 512])\n",
      "K (å¤‰æ›ç›´å¾Œ): torch.Size([2, 5, 512])\n",
      "V (å¤‰æ›ç›´å¾Œ): torch.Size([2, 5, 512])\n",
      "â†’ ã“ã“ã¾ã§ã¯01ã¨å…¨ãåŒã˜ï¼\n",
      "\n",
      "Q (åˆ†å‰²å¾Œ): torch.Size([2, 8, 5, 64])\n",
      "K (åˆ†å‰²å¾Œ): torch.Size([2, 8, 5, 64])\n",
      "V (åˆ†å‰²å¾Œ): torch.Size([2, 8, 5, 64])\n",
      "â†’ è¤‡æ•°headã«åˆ†å‰²ã—ã¦ã‹ã‚‰å„headã§Attentionã‚’è¨ˆç®—\n",
      "\n",
      "======================================================================\n",
      "çµè«–\n",
      "======================================================================\n",
      "âœ“ Q, K, Vã®ç”Ÿæˆæ–¹æ³•ã¯01ã‚‚02ã‚‚åŒã˜ï¼ˆå…¥åŠ›xã®ç·šå½¢å¤‰æ›ï¼‰\n",
      "âœ“ é•ã„ã¯ã€ãã®å¾Œã«åˆ†å‰²ã™ã‚‹ã‹ã©ã†ã‹ã ã‘\n",
      "âœ“ Multi-Headã¯ã€Self-Attentionã®æ‹¡å¼µç‰ˆ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "\n",
    "# å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆ01ã‚‚02ã‚‚åŒã˜å…¥åŠ›ã‚’ä½¿ã†ï¼‰\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"01: Self-Attention\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Self-Attentionã®ç·šå½¢å¤‰æ›å±¤\n",
    "W_q_self = nn.Linear(d_model, d_model, bias=False)\n",
    "W_k_self = nn.Linear(d_model, d_model, bias=False)\n",
    "W_v_self = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "# Q, K, Vã®ç”Ÿæˆ\n",
    "Q_self = W_q_self(x)\n",
    "K_self = W_k_self(x)\n",
    "V_self = W_v_self(x)\n",
    "\n",
    "print(f\"å…¥åŠ› x: {x.shape}\")\n",
    "print(f\"Q: {Q_self.shape}\")\n",
    "print(f\"K: {K_self.shape}\")\n",
    "print(f\"V: {V_self.shape}\")\n",
    "print(\"â†’ ã“ã®ã¾ã¾Attentionã«ä½¿ã†\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"02: Multi-Head Attention\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Multi-Head Attentionã®ç·šå½¢å¤‰æ›å±¤\n",
    "W_q_multi = nn.Linear(d_model, d_model, bias=False)\n",
    "W_k_multi = nn.Linear(d_model, d_model, bias=False)\n",
    "W_v_multi = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "# Q, K, Vã®ç”Ÿæˆï¼ˆSelf-Attentionã¨åŒã˜ï¼ï¼‰\n",
    "Q_multi = W_q_multi(x)\n",
    "K_multi = W_k_multi(x)\n",
    "V_multi = W_v_multi(x)\n",
    "\n",
    "print(f\"å…¥åŠ› x: {x.shape}\")\n",
    "print(f\"Q (å¤‰æ›ç›´å¾Œ): {Q_multi.shape}\")\n",
    "print(f\"K (å¤‰æ›ç›´å¾Œ): {K_multi.shape}\")\n",
    "print(f\"V (å¤‰æ›ç›´å¾Œ): {V_multi.shape}\")\n",
    "print(\"â†’ ã“ã“ã¾ã§ã¯01ã¨å…¨ãåŒã˜ï¼\")\n",
    "\n",
    "# ã“ã“ã‹ã‚‰Multi-Headç‰¹æœ‰ã®å‡¦ç†\n",
    "d_k = d_model // num_heads\n",
    "Q_multi = Q_multi.view(batch_size, seq_len, num_heads, d_k).transpose(1, 2)\n",
    "K_multi = K_multi.view(batch_size, seq_len, num_heads, d_k).transpose(1, 2)\n",
    "V_multi = V_multi.view(batch_size, seq_len, num_heads, d_k).transpose(1, 2)\n",
    "\n",
    "print(f\"\\nQ (åˆ†å‰²å¾Œ): {Q_multi.shape}\")\n",
    "print(f\"K (åˆ†å‰²å¾Œ): {K_multi.shape}\")\n",
    "print(f\"V (åˆ†å‰²å¾Œ): {V_multi.shape}\")\n",
    "print(\"â†’ è¤‡æ•°headã«åˆ†å‰²ã—ã¦ã‹ã‚‰å„headã§Attentionã‚’è¨ˆç®—\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"çµè«–\")\n",
    "print(\"=\" * 70)\n",
    "print(\"âœ“ Q, K, Vã®ç”Ÿæˆæ–¹æ³•ã¯01ã‚‚02ã‚‚åŒã˜ï¼ˆå…¥åŠ›xã®ç·šå½¢å¤‰æ›ï¼‰\")\n",
    "print(\"âœ“ é•ã„ã¯ã€ãã®å¾Œã«åˆ†å‰²ã™ã‚‹ã‹ã©ã†ã‹ã ã‘\")\n",
    "print(\"âœ“ Multi-Headã¯ã€Self-Attentionã®æ‹¡å¼µç‰ˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003a24bd",
   "metadata": {},
   "source": [
    "#### src/attention.pyã§ã®å®Ÿè£…ã‚’ç¢ºèª\n",
    "\n",
    "å®Ÿéš›ã®`src/attention.py`ã‚’è¦‹ã‚‹ã¨ã€ä¸¡æ–¹ã¨ã‚‚åŒã˜æ§‹é€ ã«ãªã£ã¦ã„ã¾ã™ï¼š\n",
    "\n",
    "##### SelfAttentionã‚¯ãƒ©ã‚¹:\n",
    "```python\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        # å…¥åŠ›ã‹ã‚‰Query, Key, Valueã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®ç·šå½¢å¤‰æ›\n",
    "        self.query_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.key_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.value_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # å…¥åŠ›ã‹ã‚‰ Q, K, V ã‚’ç·šå½¢å¤‰æ›ã§ç”Ÿæˆ\n",
    "        query = self.query_linear(x)\n",
    "        key = self.key_linear(x)\n",
    "        value = self.value_linear(x)\n",
    "        \n",
    "        # Attentionã‚’é©ç”¨\n",
    "        attention_output, attention_weights = self.attention(query, key, value, mask)\n",
    "```\n",
    "\n",
    "##### MultiHeadAttentionã‚¯ãƒ©ã‚¹:\n",
    "```python\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        # Q, K, Vç”¨ã®ç·šå½¢å¤‰æ›å±¤ï¼ˆå…¨headã‚’ã¾ã¨ã‚ã¦å‡¦ç†ï¼‰\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # 1. ç·šå½¢å¤‰æ›ï¼ˆSelf-Attentionã¨åŒã˜ï¼ï¼‰\n",
    "        Q = self.W_q(query)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "        \n",
    "        # 2. è¤‡æ•°headã«åˆ†å‰²ï¼ˆMulti-Headç‰¹æœ‰ã®å‡¦ç†ï¼‰\n",
    "        Q = self.split_heads(Q, batch_size)\n",
    "        K = self.split_heads(K, batch_size)\n",
    "        V = self.split_heads(V, batch_size)\n",
    "        \n",
    "        # 3. å„headã§Attentionã‚’è¨ˆç®—\n",
    "        attention_output, attention_weights = self.attention(Q, K, V, mask)\n",
    "```\n",
    "\n",
    "**å¤‰æ•°åãŒé•ã†ã ã‘ã§ã€ã‚„ã£ã¦ã„ã‚‹ã“ã¨ã¯åŒã˜ï¼**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e87269c",
   "metadata": {},
   "source": [
    "#### ã¾ã¨ã‚\n",
    "\n",
    "**Q: Multi-Head Attentionã®Q, K, Vã‚‚å…¥åŠ›ã®ç·šå½¢å¤‰æ›ã¨è€ƒãˆã¦è‰¯ã„ã‹ï¼Ÿ**\n",
    "\n",
    "**A: ã¯ã„ã€å…¨ããã®é€šã‚Šã§ã™ï¼**\n",
    "\n",
    "##### å…±é€šç‚¹ï¼ˆ01ã‚‚02ã‚‚åŒã˜ï¼‰:\n",
    "1. å…¥åŠ› `x` ã‹ã‚‰ç·šå½¢å¤‰æ›ã§ Q, K, V ã‚’ç”Ÿæˆ\n",
    "2. `nn.Linear` ã‚’ä½¿ã£ãŸå­¦ç¿’å¯èƒ½ãªé‡ã¿è¡Œåˆ—\n",
    "3. æ•°å¼: $Q = xW_q$, $K = xW_k$, $V = xW_v$\n",
    "4. Q, K, Vã¯ãã‚Œãã‚Œç•°ãªã‚‹å½¹å‰²ã‚’æŒã¤\n",
    "\n",
    "##### é•ã„ï¼ˆ02ã ã‘ã®ç‰¹å¾´ï¼‰:\n",
    "- ç”Ÿæˆã—ãŸ Q, K, V ã‚’è¤‡æ•°headã«åˆ†å‰²\n",
    "- å„headã§ç‹¬ç«‹ã«Attentionã‚’è¨ˆç®—\n",
    "- æœ€å¾Œã«å…¨headã®çµæœã‚’çµåˆ\n",
    "\n",
    "##### ç†è§£ã®ãƒã‚¤ãƒ³ãƒˆ:\n",
    "```\n",
    "Self-Attention (01):\n",
    "  å…¥åŠ› â†’ ç·šå½¢å¤‰æ› â†’ Q, K, V â†’ Attention â†’ å‡ºåŠ›\n",
    "\n",
    "Multi-Head Attention (02):\n",
    "  å…¥åŠ› â†’ ç·šå½¢å¤‰æ› â†’ Q, K, V â†’ åˆ†å‰² â†’ å„headã§Attention â†’ çµåˆ â†’ å‡ºåŠ›\n",
    "         â””â”€ ã“ã“ã¾ã§01ã¨åŒã˜ â”€â”˜\n",
    "```\n",
    "\n",
    "**Multi-Head Attentionã¯ã€Self-Attentionã®è‡ªç„¶ãªæ‹¡å¼µ**ã§ã™ã€‚Q, K, Vã®ç”Ÿæˆæ–¹æ³•ã¯å…¨ãåŒã˜ã§ã€ãã®å¾Œã®å‡¦ç†ãŒç•°ãªã‚‹ã ã‘ã§ã™ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6d15f9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q15: Attentionã¯æœ¬è³ªçš„ã«Q, K, Vã®é–¢æ•°ã§ã‚ã‚Šã€å…¥åŠ›xã¯å¿…è¦ãªã„ï¼Ÿ\n",
    "\n",
    "**è³ªå•æ—¥**: 2025å¹´11æœˆ18æ—¥\n",
    "\n",
    "### è³ªå•\n",
    "Attentionæ©Ÿæ§‹ã‚„Multi-Head Attentionã¯ã€æ•°å¼ä¸Šã‚‚å®Ÿè£…ä¸Šã‚‚ `Attention(Q, K, V)` ã¨ã—ã¦Q, K, Vã®é–¢æ•°ã¨ã—ã¦è¨˜è¿°ã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œã¾ã§ã®èª¬æ˜ã§ã¯å…¥åŠ›xã‹ã‚‰ç·šå½¢å¤‰æ›ã—ã¦Q, K, Vã‚’ç”Ÿæˆã™ã‚‹ã¨èª¬æ˜ã•ã‚Œã¦ã„ã¾ã™ãŒã€æœ¬è³ªçš„ã«ã¯å…¥åŠ›xã®è©±ã¯å¿…è¦ãªãã€ã€ŒQ, K, VãŒä¸ãˆã‚‰ã‚Œã‚Œã°Attentionã¯è¨ˆç®—ã§ãã‚‹ã€ã¨ç†è§£ã™ã¹ãã§ã—ã‚‡ã†ã‹ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2358c829",
   "metadata": {},
   "source": [
    "### å›ç­”\n",
    "\n",
    "**ãã®é€šã‚Šã§ã™ï¼éå¸¸ã«é‹­ã„æŒ‡æ‘˜ã§ã™ã€‚**\n",
    "\n",
    "Attentionæ©Ÿæ§‹ã¯æœ¬è³ªçš„ã«**Q, K, Vã®é–¢æ•°**ã§ã‚ã‚Šã€å…¥åŠ›xãŒã©ã“ã‹ã‚‰æ¥ãŸã‹ã¯é–¢ä¿‚ã‚ã‚Šã¾ã›ã‚“ã€‚\n",
    "\n",
    "#### 2ã¤ã®è¦–ç‚¹ã®åŒºåˆ¥\n",
    "\n",
    "##### 1. **Attentionæ©Ÿæ§‹ãã®ã‚‚ã®ï¼ˆã‚³ã‚¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰**\n",
    "\n",
    "æ•°å¼:\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "- **å…¥åŠ›**: Q, K, Vï¼ˆ3ã¤ã®è¡Œåˆ—ï¼‰\n",
    "- **å‡ºåŠ›**: Attentioné©ç”¨å¾Œã®è¡Œåˆ—\n",
    "- **ç‰¹å¾´**: Q, K, VãŒã©ã“ã‹ã‚‰æ¥ãŸã‹ã¯çŸ¥ã‚‰ãªã„ãƒ»æ°—ã«ã—ãªã„\n",
    "- **å½¹å‰²**: ç´”ç²‹ãªè¨ˆç®—ãƒ¡ã‚«ãƒ‹ã‚ºãƒ \n",
    "\n",
    "##### 2. **Self-Attentionï¼ˆAttentionã®ä½¿ã„æ–¹ã®ä¸€ä¾‹ï¼‰**\n",
    "\n",
    "æ•°å¼:\n",
    "$$\n",
    "Q = xW_q, \\quad K = xW_k, \\quad V = xW_v\n",
    "$$\n",
    "$$\n",
    "\\text{SelfAttention}(x) = \\text{Attention}(xW_q, xW_k, xW_v)\n",
    "$$\n",
    "\n",
    "- **å…¥åŠ›**: xï¼ˆ1ã¤ã®å…¥åŠ›ï¼‰\n",
    "- **ç‰¹å¾´**: åŒã˜å…¥åŠ›xã‹ã‚‰Q, K, Vã‚’ç”Ÿæˆ\n",
    "- **å½¹å‰²**: Attentionã®å…·ä½“çš„ãªå¿œç”¨æ–¹æ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34198b40",
   "metadata": {},
   "source": [
    "#### å®Ÿè£…ã§ã®åˆ†é›¢\n",
    "\n",
    "`src/attention.py`ã§ã‚‚ã€ã“ã®2ã¤ã¯æ˜ç¢ºã«åˆ†é›¢ã•ã‚Œã¦ã„ã¾ã™ï¼š\n",
    "\n",
    "##### ScaledDotProductAttentionï¼ˆã‚³ã‚¢ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ï¼‰\n",
    "\n",
    "```python\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: Queryè¡Œåˆ— [batch_size, seq_len, d_k]\n",
    "            key: Keyè¡Œåˆ— [batch_size, seq_len, d_k]\n",
    "            value: Valueè¡Œåˆ— [batch_size, seq_len, d_k]\n",
    "        \"\"\"\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        return output, attention_weights\n",
    "```\n",
    "\n",
    "**ãƒã‚¤ãƒ³ãƒˆ**: \n",
    "- Q, K, VãŒã©ã“ã‹ã‚‰æ¥ãŸã‹å…¨ãçŸ¥ã‚‰ãªã„\n",
    "- ãŸã è¨ˆç®—ã™ã‚‹ã ã‘\n",
    "- æ±ç”¨çš„ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ \n",
    "\n",
    "##### SelfAttentionï¼ˆAttentionã®å¿œç”¨ï¼‰\n",
    "\n",
    "```python\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        self.query_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.key_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.value_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.attention = ScaledDotProductAttention()  # â† ã‚³ã‚¢ã‚’åˆ©ç”¨\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # xã‹ã‚‰Q, K, Vã‚’ç”Ÿæˆï¼ˆSelf-Attentionç‰¹æœ‰ã®å‡¦ç†ï¼‰\n",
    "        query = self.query_linear(x)\n",
    "        key = self.key_linear(x)\n",
    "        value = self.value_linear(x)\n",
    "        \n",
    "        # Attentionã‚’å‘¼ã³å‡ºã™ï¼ˆæ±ç”¨ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ï¼‰\n",
    "        output, weights = self.attention(query, key, value)\n",
    "        return output, weights\n",
    "```\n",
    "\n",
    "**ãƒã‚¤ãƒ³ãƒˆ**:\n",
    "- å…¥åŠ›xã®å‡¦ç†ã‚’æ‹…å½“\n",
    "- Q, K, Vã‚’æº–å‚™\n",
    "- Attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’å‘¼ã³å‡ºã™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb486a4",
   "metadata": {},
   "source": [
    "#### Q, K, VãŒç•°ãªã‚‹å…¥åŠ›ã‹ã‚‰æ¥ã‚‹ä¾‹: Cross-Attention\n",
    "\n",
    "Self-Attentionã§ã¯åŒã˜xã‹ã‚‰Q, K, Vã‚’ç”Ÿæˆã—ã¾ã™ãŒã€**ç•°ãªã‚‹å…¥åŠ›ã‹ã‚‰ç”Ÿæˆã™ã‚‹å ´åˆã‚‚ã‚ã‚Šã¾ã™**ã€‚\n",
    "\n",
    "##### Cross-Attentionï¼ˆTransformerã®ãƒ‡ã‚³ãƒ¼ãƒ€ã§ä½¿ç”¨ï¼‰\n",
    "\n",
    "```python\n",
    "def cross_attention(encoder_output, decoder_input):\n",
    "    \"\"\"\n",
    "    Q: ãƒ‡ã‚³ãƒ¼ãƒ€ã®å…¥åŠ›ã‹ã‚‰ç”Ÿæˆ\n",
    "    K, V: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ›ã‹ã‚‰ç”Ÿæˆ\n",
    "    \"\"\"\n",
    "    Q = W_q(decoder_input)      # ãƒ‡ã‚³ãƒ¼ãƒ€å´\n",
    "    K = W_k(encoder_output)     # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å´\n",
    "    V = W_v(encoder_output)     # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å´\n",
    "    \n",
    "    # Attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã¯åŒã˜ï¼\n",
    "    output = Attention(Q, K, V)\n",
    "    return output\n",
    "```\n",
    "\n",
    "**é‡è¦ãªç‚¹**:\n",
    "- Q, K, VãŒ**ç•°ãªã‚‹ã‚½ãƒ¼ã‚¹**ã‹ã‚‰æ¥ã¦ã„ã‚‹\n",
    "- ã§ã‚‚Attentionæ©Ÿæ§‹ã¯å…¨ãåŒã˜\n",
    "- Attentionã¯ã€ŒQ, K, Vã®é–¢ä¿‚ã‚’è¨ˆç®—ã™ã‚‹ã ã‘ã€ã§ã€å‡ºæ‰€ã¯é–¢ä¿‚ãªã„\n",
    "\n",
    "##### å¿œç”¨ä¾‹: ç¿»è¨³ã‚¿ã‚¹ã‚¯\n",
    "\n",
    "```\n",
    "è‹±èªæ–‡: \"I love cats\"ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼‰\n",
    "æ—¥æœ¬èªæ–‡: \"ç§ã¯\"ï¼ˆãƒ‡ã‚³ãƒ¼ãƒ€ã§ç”Ÿæˆä¸­ï¼‰\n",
    "\n",
    "Q: ã€Œç§ã¯ã€ã®æ¬¡ã«ä½•ã‚’ç”Ÿæˆã™ã¹ãã‹æ¢ã—ã¦ã„ã‚‹ï¼ˆãƒ‡ã‚³ãƒ¼ãƒ€å´ï¼‰\n",
    "K, V: è‹±èªæ–‡ã®æƒ…å ±ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å´ï¼‰\n",
    "\n",
    "Attentionè¨ˆç®—: ã€Œç§ã¯ã€ãŒè‹±èªæ–‡ã®ã©ã“ã«æ³¨ç›®ã™ã¹ãã‹\n",
    "çµæœ: \"love\" ã«æ³¨ç›® â†’ æ¬¡ã¯ã€Œæ„›ã—ã¦ã„ã‚‹ã€ã‚’ç”Ÿæˆ\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109c3114",
   "metadata": {},
   "source": [
    "#### ã‚³ãƒ¼ãƒ‰ä¾‹: Attentionã®æ±ç”¨æ€§ã‚’ç¢ºèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "27608d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ã‚±ãƒ¼ã‚¹1: Self-Attention\n",
      "======================================================================\n",
      "å…¥åŠ›: åŒã˜x torch.Size([1, 3, 4])\n",
      "Q, K, V: å…¨ã¦xã‹ã‚‰ç”Ÿæˆ\n",
      "å‡ºåŠ›: torch.Size([1, 3, 4])\n",
      "\n",
      "======================================================================\n",
      "ã‚±ãƒ¼ã‚¹2: Cross-Attention\n",
      "======================================================================\n",
      "Q: decoder_input torch.Size([1, 2, 4]) ã‹ã‚‰ç”Ÿæˆ\n",
      "K, V: encoder_output torch.Size([1, 3, 4]) ã‹ã‚‰ç”Ÿæˆ\n",
      "å‡ºåŠ›: torch.Size([1, 2, 4])\n",
      "\n",
      "======================================================================\n",
      "ã‚±ãƒ¼ã‚¹3: ãƒ©ãƒ³ãƒ€ãƒ ãªQ, K, Vï¼ˆã©ã“ã‹ã‚‰æ¥ãŸã‹ã‚ã‹ã‚‰ãªã„ï¼‰\n",
      "======================================================================\n",
      "Q: torch.Size([1, 5, 4])\n",
      "K: torch.Size([1, 5, 4])\n",
      "V: torch.Size([1, 5, 4])\n",
      "å‡ºåŠ›: torch.Size([1, 5, 4])\n",
      "\n",
      "======================================================================\n",
      "çµè«–\n",
      "======================================================================\n",
      "âœ“ Attentioné–¢æ•°ã¯ã€ã©ã®ã‚±ãƒ¼ã‚¹ã§ã‚‚åŒã˜ã‚ˆã†ã«å‹•ä½œ\n",
      "âœ“ Q, K, VãŒã©ã“ã‹ã‚‰æ¥ãŸã‹ã¯å…¨ãé–¢ä¿‚ãªã„\n",
      "âœ“ Attentionã¯ç´”ç²‹ã«ã€ŒQ, K, Vã®é–¢æ•°ã€ã¨ã—ã¦å®šç¾©ã•ã‚Œã‚‹\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "def attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    æ±ç”¨çš„ãªAttentionæ©Ÿæ§‹\n",
    "    Q, K, VãŒã©ã“ã‹ã‚‰æ¥ãŸã‹ã¯æ°—ã«ã—ãªã„\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    return output, attention_weights\n",
    "\n",
    "# ã‚±ãƒ¼ã‚¹1: Self-Attentionï¼ˆåŒã˜xã‹ã‚‰ç”Ÿæˆï¼‰\n",
    "print(\"=\" * 70)\n",
    "print(\"ã‚±ãƒ¼ã‚¹1: Self-Attention\")\n",
    "print(\"=\" * 70)\n",
    "x = torch.randn(1, 3, 4)\n",
    "W_q = torch.randn(4, 4)\n",
    "W_k = torch.randn(4, 4)\n",
    "W_v = torch.randn(4, 4)\n",
    "\n",
    "Q1 = torch.matmul(x, W_q)\n",
    "K1 = torch.matmul(x, W_k)\n",
    "V1 = torch.matmul(x, W_v)\n",
    "\n",
    "print(f\"å…¥åŠ›: åŒã˜x {x.shape}\")\n",
    "print(f\"Q, K, V: å…¨ã¦xã‹ã‚‰ç”Ÿæˆ\")\n",
    "output1, weights1 = attention(Q1, K1, V1)\n",
    "print(f\"å‡ºåŠ›: {output1.shape}\")\n",
    "\n",
    "# ã‚±ãƒ¼ã‚¹2: Cross-Attentionï¼ˆç•°ãªã‚‹å…¥åŠ›ã‹ã‚‰ç”Ÿæˆï¼‰\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ã‚±ãƒ¼ã‚¹2: Cross-Attention\")\n",
    "print(\"=\" * 70)\n",
    "encoder_output = torch.randn(1, 3, 4)\n",
    "decoder_input = torch.randn(1, 2, 4)\n",
    "\n",
    "Q2 = torch.matmul(decoder_input, W_q)  # ãƒ‡ã‚³ãƒ¼ãƒ€å…¥åŠ›ã‹ã‚‰\n",
    "K2 = torch.matmul(encoder_output, W_k)  # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å‡ºåŠ›ã‹ã‚‰\n",
    "V2 = torch.matmul(encoder_output, W_v)  # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å‡ºåŠ›ã‹ã‚‰\n",
    "\n",
    "print(f\"Q: decoder_input {decoder_input.shape} ã‹ã‚‰ç”Ÿæˆ\")\n",
    "print(f\"K, V: encoder_output {encoder_output.shape} ã‹ã‚‰ç”Ÿæˆ\")\n",
    "output2, weights2 = attention(Q2, K2, V2)\n",
    "print(f\"å‡ºåŠ›: {output2.shape}\")\n",
    "\n",
    "# ã‚±ãƒ¼ã‚¹3: å®Œå…¨ã«ãƒ©ãƒ³ãƒ€ãƒ ãªQ, K, V\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ã‚±ãƒ¼ã‚¹3: ãƒ©ãƒ³ãƒ€ãƒ ãªQ, K, Vï¼ˆã©ã“ã‹ã‚‰æ¥ãŸã‹ã‚ã‹ã‚‰ãªã„ï¼‰\")\n",
    "print(\"=\" * 70)\n",
    "Q3 = torch.randn(1, 5, 4)\n",
    "K3 = torch.randn(1, 5, 4)\n",
    "V3 = torch.randn(1, 5, 4)\n",
    "\n",
    "print(f\"Q: {Q3.shape}\")\n",
    "print(f\"K: {K3.shape}\")\n",
    "print(f\"V: {V3.shape}\")\n",
    "output3, weights3 = attention(Q3, K3, V3)\n",
    "print(f\"å‡ºåŠ›: {output3.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"çµè«–\")\n",
    "print(\"=\" * 70)\n",
    "print(\"âœ“ Attentioné–¢æ•°ã¯ã€ã©ã®ã‚±ãƒ¼ã‚¹ã§ã‚‚åŒã˜ã‚ˆã†ã«å‹•ä½œ\")\n",
    "print(\"âœ“ Q, K, VãŒã©ã“ã‹ã‚‰æ¥ãŸã‹ã¯å…¨ãé–¢ä¿‚ãªã„\")\n",
    "print(\"âœ“ Attentionã¯ç´”ç²‹ã«ã€ŒQ, K, Vã®é–¢æ•°ã€ã¨ã—ã¦å®šç¾©ã•ã‚Œã‚‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4a7a28",
   "metadata": {},
   "source": [
    "#### ãªãœå…¥åŠ›xã®è©±ã‚’ã™ã‚‹ã®ã‹\n",
    "\n",
    "ã§ã¯ã€ãªãœã“ã‚Œã¾ã§ã€Œå…¥åŠ›xã‹ã‚‰ç·šå½¢å¤‰æ›ã€ã¨ã„ã†èª¬æ˜ã‚’ã—ã¦ããŸã®ã§ã—ã‚‡ã†ã‹ï¼Ÿ\n",
    "\n",
    "##### ç†ç”±1: **å­¦ç¿’ã®ãŸã‚**\n",
    "\n",
    "Attentionæ©Ÿæ§‹è‡ªä½“ã«ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å­¦ç¿’ã™ã‚‹ã«ã¯ï¼š\n",
    "- ç·šå½¢å¤‰æ›ã®é‡ã¿ $W_q, W_k, W_v$ ãŒå¿…è¦\n",
    "- ã“ã‚Œã‚‰ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€ã‚¿ã‚¹ã‚¯ã«é©ã—ãŸQ, K, Vã®ç”Ÿæˆæ–¹æ³•ã‚’ç¿’å¾—\n",
    "\n",
    "##### ç†ç”±2: **Self-AttentionãŒæœ€ã‚‚ä¸€èˆ¬çš„**\n",
    "\n",
    "Transformerã§ã¯ã€Self-AttentionãŒä¸»è¦ãªæ§‹æˆè¦ç´ ï¼š\n",
    "- ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€: Self-Attention\n",
    "- ãƒ‡ã‚³ãƒ¼ãƒ€: Self-Attention + Cross-Attention\n",
    "\n",
    "ã€ŒåŒã˜xã‹ã‚‰Q, K, Vã‚’ç”Ÿæˆã€ã¨ã„ã†ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒé »å‡ºã™ã‚‹ãŸã‚ã€èª¬æ˜ã®ä¸­å¿ƒã«ãªã‚‹\n",
    "\n",
    "##### ç†ç”±3: **ç†è§£ã®ã—ã‚„ã™ã•**\n",
    "\n",
    "åˆå­¦è€…ã«ã¨ã£ã¦ï¼š\n",
    "- ã€Œå…¥åŠ›xãŒã‚ã‚‹ã€ã¨ã„ã†å…·ä½“çš„ãªå‡ºç™ºç‚¹ãŒã‚ã‚‹ã¨ç†è§£ã—ã‚„ã™ã„\n",
    "- æŠ½è±¡çš„ãªã€ŒQ, K, VãŒä¸ãˆã‚‰ã‚Œã‚‹ã€ã‚ˆã‚Šã‚¤ãƒ¡ãƒ¼ã‚¸ã—ã‚„ã™ã„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d9c978",
   "metadata": {},
   "source": [
    "#### ã¾ã¨ã‚\n",
    "\n",
    "**Q: Attentionã¯æœ¬è³ªçš„ã«Q, K, Vã®é–¢æ•°ã§ã‚ã‚Šã€å…¥åŠ›xã¯å¿…è¦ãªã„ï¼Ÿ**\n",
    "\n",
    "**A: ã¯ã„ã€å®Œå…¨ã«æ­£ã—ã„ç†è§£ã§ã™ï¼**\n",
    "\n",
    "##### Attentionæ©Ÿæ§‹ã®æœ¬è³ª\n",
    "\n",
    "```\n",
    "Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V\n",
    "```\n",
    "\n",
    "- **ç´”ç²‹ãªæ•°å­¦çš„é–¢æ•°**: Q, K, V â†’ å‡ºåŠ›\n",
    "- **æ±ç”¨çš„ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **: ã©ã“ã‹ã‚‰æ¥ãŸQ, K, Vã§ã‚‚åŒã˜ã‚ˆã†ã«å‡¦ç†\n",
    "- **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãªã—**: è¨ˆç®—ã™ã‚‹ã ã‘\n",
    "\n",
    "##### å…¥åŠ›xã®å½¹å‰²\n",
    "\n",
    "å…¥åŠ›xã¯ã€**Q, K, Vã‚’æº–å‚™ã™ã‚‹æ–¹æ³•ã®ä¸€ä¾‹**ï¼š\n",
    "\n",
    "- **Self-Attention**: åŒã˜xã‹ã‚‰Q, K, Vã‚’ç”Ÿæˆ\n",
    "- **Cross-Attention**: ç•°ãªã‚‹å…¥åŠ›ã‹ã‚‰Q, K, Vã‚’ç”Ÿæˆ\n",
    "- **ãã®ä»–**: Q, K, Vã¯ä»»æ„ã®æ–¹æ³•ã§æº–å‚™å¯èƒ½\n",
    "\n",
    "##### ç†è§£ã®ãƒã‚¤ãƒ³ãƒˆ\n",
    "\n",
    "| è¦–ç‚¹ | å†…å®¹ |\n",
    "|------|------|\n",
    "| **Attentionã®ã‚³ã‚¢** | Q, K, Vã®é–¢æ•°ï¼ˆæ±ç”¨ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ï¼‰ |\n",
    "| **Self-Attentionãªã©** | Attentionã®ä½¿ã„æ–¹ï¼ˆå¿œç”¨ä¾‹ï¼‰ |\n",
    "| **å…¥åŠ›xã®å½¹å‰²** | Q, K, Vã‚’ç”Ÿæˆã™ã‚‹æ‰‹æ®µã®1ã¤ |\n",
    "\n",
    "##### é¡ä¼¼ä¾‹\n",
    "\n",
    "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®é–¢æ•°ã¨åŒã˜ï¼š\n",
    "\n",
    "```python\n",
    "def sort(array):\n",
    "    # é…åˆ—ãŒã©ã“ã‹ã‚‰æ¥ãŸã‹ã¯é–¢ä¿‚ãªã„\n",
    "    return sorted(array)\n",
    "\n",
    "# ä½¿ã„æ–¹ã®ä¾‹\n",
    "list1 = [3, 1, 2]\n",
    "sort(list1)  # ãƒªã‚¹ãƒˆã‹ã‚‰\n",
    "\n",
    "dict1 = {'a': 3, 'b': 1}\n",
    "sort(dict1.values())  # è¾æ›¸ã‹ã‚‰\n",
    "\n",
    "# sorté–¢æ•°è‡ªä½“ã¯æ±ç”¨çš„\n",
    "```\n",
    "\n",
    "**Attentionæ©Ÿæ§‹ã‚‚åŒã˜**: Q, K, VãŒã‚ã‚Œã°è¨ˆç®—ã§ãã‚‹ã€‚ãã‚Œã‚‰ã‚’ã©ã†æº–å‚™ã™ã‚‹ã‹ã¯åˆ¥ã®è©±ã€‚\n",
    "\n",
    "ã“ã®ç†è§£ã¯ã€ä»Šå¾ŒCross-Attentionã‚„Transformerã®å…¨ä½“æ§‹é€ ã‚’å­¦ã¶ã¨ãã«éå¸¸ã«é‡è¦ã«ãªã‚Šã¾ã™ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc108420",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q16: Attention Weightsã¨Vã®ç©ã§ã€è¡Œã¨åˆ—ã®ã©ã¡ã‚‰ã«æ„å‘³ãŒã‚ã‚‹ã®ã‹ï¼Ÿ\n",
    "\n",
    "**è³ªå•æ—¥**: 2025å¹´11æœˆ18æ—¥\n",
    "\n",
    "### è³ªå•\n",
    "Attentionè¨ˆç®—ã®æœ€å¾Œã« `Attention_Weights Ã— V` ã¨ã„ã†è¡Œåˆ—ã®ç©ã‚’è¨ˆç®—ã—ã¾ã™ã€‚\n",
    "\n",
    "é€šå¸¸ã®ç·šå½¢å¤‰æ› `y = Wx` ã§ã¯ã€è¡Œåˆ—Wã®åˆ—ãƒ™ã‚¯ãƒˆãƒ«ãŒæ„å‘³ã‚’æŒã¡ã€çµæœyã®å„è¦ç´ ã¯ã€ŒWã®å„åˆ—ã¨ã®å†…ç©ã€ã¨ã„ã†è§£é‡ˆã«ãªã‚Šã¾ã™ã€‚\n",
    "\n",
    "ã—ã‹ã—Transformerã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ã®**è¡Œ**ã«æ„å‘³ãŒã‚ã‚‹ã‚ˆã†ã«è¦‹ãˆã¾ã™ï¼ˆå„è¡ŒãŒ1ã¤ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¡¨ã™ï¼‰ã€‚ã“ã‚ŒãŒæ··ä¹±ã®åŸå› ãªã®ã§ã™ãŒã€ã©ã†ç†è§£ã™ã‚Œã°ã‚ˆã„ã§ã—ã‚‡ã†ã‹ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac9904a",
   "metadata": {},
   "source": [
    "### å›ç­”\n",
    "\n",
    "ç´ æ™´ã‚‰ã—ã„è³ªå•ã§ã™ï¼ã“ã‚Œã¯è¡Œåˆ—ã®ç©ã®ç†è§£ã«ãŠã„ã¦é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã§ã™ã€‚\n",
    "\n",
    "**çµè«–**: Transformerã§ã¯ã€**è¡Œã«æ„å‘³ãŒã‚ã‚‹è¡Œåˆ—åŒå£«ã®ç©**ã‚’è¨ˆç®—ã—ã¦ã„ã¾ã™ã€‚é€šå¸¸ã®ç·šå½¢å¤‰æ›`y = Wx`ã¨ã¯**è»¢ç½®ã®é–¢ä¿‚**ã«ãªã£ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "#### 2ã¤ã®è¡Œåˆ—ã®ç©ã®è§£é‡ˆ\n",
    "\n",
    "##### ãƒ‘ã‚¿ãƒ¼ãƒ³1: é€šå¸¸ã®ç·šå½¢å¤‰æ›ï¼ˆåˆ—ãƒ™ã‚¯ãƒˆãƒ«ã«æ„å‘³ï¼‰\n",
    "\n",
    "```python\n",
    "# ãƒ™ã‚¯ãƒˆãƒ«ã®å¤‰æ›: y = Wx\n",
    "W = [[w11, w12],    # 2Ã—3 è¡Œåˆ—\n",
    "     [w21, w22],\n",
    "     [w31, w32]]\n",
    "x = [x1, x2, x3]    # 3æ¬¡å…ƒåˆ—ãƒ™ã‚¯ãƒˆãƒ«\n",
    "y = Wx              # 2æ¬¡å…ƒåˆ—ãƒ™ã‚¯ãƒˆãƒ«\n",
    "```\n",
    "\n",
    "è§£é‡ˆ:\n",
    "- xã¯**åˆ—ãƒ™ã‚¯ãƒˆãƒ«** (ç¸¦æ–¹å‘)\n",
    "- Wã®å„**åˆ—**ãŒãƒ™ã‚¯ãƒˆãƒ«\n",
    "- yã®å„è¦ç´ ã¯ã€Wã®å„**è¡Œ**ã¨xã®å†…ç©\n",
    "\n",
    "##### ãƒ‘ã‚¿ãƒ¼ãƒ³2: Transformerã®ãƒãƒƒãƒå‡¦ç†ï¼ˆè¡Œãƒ™ã‚¯ãƒˆãƒ«ã«æ„å‘³ï¼‰\n",
    "\n",
    "```python\n",
    "# ãƒãƒƒãƒå‡¦ç†: Y = XW\n",
    "X = [[x11, x12, x13],    # 2Ã—3 è¡Œåˆ— (2ã‚µãƒ³ãƒ—ãƒ«)\n",
    "     [x21, x22, x23]]\n",
    "W = [[w11, w12],         # 3Ã—2 è¡Œåˆ—\n",
    "     [w21, w22],\n",
    "     [w31, w32]]\n",
    "Y = XW                   # 2Ã—2 è¡Œåˆ—\n",
    "```\n",
    "\n",
    "è§£é‡ˆ:\n",
    "- Xã®å„**è¡Œ**ãŒ1ã¤ã®ã‚µãƒ³ãƒ—ãƒ« (æ¨ªæ–¹å‘)\n",
    "- Wã®å„**åˆ—**ãŒãƒ™ã‚¯ãƒˆãƒ«\n",
    "- Yã®å„**è¡Œ**ãŒçµæœ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f846178",
   "metadata": {},
   "source": [
    "#### Attentionè¨ˆç®—ã§ã®è¡Œåˆ—ã®å½¢çŠ¶\n",
    "\n",
    "Transformerã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚’**[seq_len, d_model]** ã®å½¢ã§æ‰±ã„ã¾ã™ï¼š\n",
    "\n",
    "```\n",
    "V = [[v1_1, v1_2, ..., v1_d],    # ãƒˆãƒ¼ã‚¯ãƒ³1ã®ãƒ™ã‚¯ãƒˆãƒ«\n",
    "     [v2_1, v2_2, ..., v2_d],    # ãƒˆãƒ¼ã‚¯ãƒ³2ã®ãƒ™ã‚¯ãƒˆãƒ«\n",
    "     [v3_1, v3_2, ..., v3_d]]    # ãƒˆãƒ¼ã‚¯ãƒ³3ã®ãƒ™ã‚¯ãƒˆãƒ«\n",
    "```\n",
    "\n",
    "- **å„è¡Œ**: 1ã¤ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«\n",
    "- **å„åˆ—**: ç‰¹å¾´æ¬¡å…ƒã®1ã¤\n",
    "\n",
    "##### Attention Weightsã¨ã®ç©\n",
    "\n",
    "```python\n",
    "attention_weights = [[0.7, 0.2, 0.1],     # ãƒˆãƒ¼ã‚¯ãƒ³1ã®æ³¨ç›®åˆ†å¸ƒ\n",
    "                     [0.1, 0.8, 0.1],     # ãƒˆãƒ¼ã‚¯ãƒ³2ã®æ³¨ç›®åˆ†å¸ƒ\n",
    "                     [0.2, 0.2, 0.6]]     # ãƒˆãƒ¼ã‚¯ãƒ³3ã®æ³¨ç›®åˆ†å¸ƒ\n",
    "                     # [seq_len, seq_len]\n",
    "\n",
    "V = [[v1_1, v1_2, ..., v1_d],     # ãƒˆãƒ¼ã‚¯ãƒ³1\n",
    "     [v2_1, v2_2, ..., v2_d],     # ãƒˆãƒ¼ã‚¯ãƒ³2\n",
    "     [v3_1, v3_2, ..., v3_d]]     # ãƒˆãƒ¼ã‚¯ãƒ³3\n",
    "     # [seq_len, d_model]\n",
    "\n",
    "output = attention_weights @ V\n",
    "# [seq_len, seq_len] @ [seq_len, d_model] = [seq_len, d_model]\n",
    "```\n",
    "\n",
    "**é‡è¦**: \n",
    "- `attention_weights`ã®å„**è¡Œ**ï¼šå„ãƒˆãƒ¼ã‚¯ãƒ³ãŒä»–ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«æ³¨ç›®ã™ã‚‹é‡ã¿\n",
    "- `V`ã®å„**è¡Œ**ï¼šå„ãƒˆãƒ¼ã‚¯ãƒ³ã®å€¤ãƒ™ã‚¯ãƒˆãƒ«\n",
    "- `output`ã®å„**è¡Œ**ï¼šVã®è¡Œã®é‡ã¿ä»˜ãå’Œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4694d6",
   "metadata": {},
   "source": [
    "#### å…·ä½“ä¾‹ã§ç†è§£ã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8d432721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V (Valueè¡Œåˆ—):\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.]])\n",
      "å½¢çŠ¶: torch.Size([3, 4]) - å„è¡ŒãŒ1ã¤ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ™ã‚¯ãƒˆãƒ«\n",
      "\n",
      "Attention Weights:\n",
      "tensor([[0.7000, 0.2000, 0.1000],\n",
      "        [0.1000, 0.8000, 0.1000],\n",
      "        [0.2000, 0.2000, 0.6000]])\n",
      "å½¢çŠ¶: torch.Size([3, 3])\n",
      "å„è¡Œã®å’Œ: tensor([1., 1., 1.])\n",
      "\n",
      "Output = Attention_Weights @ V:\n",
      "tensor([[0.7000, 0.2000, 0.1000, 0.0000],\n",
      "        [0.1000, 0.8000, 0.1000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.6000, 0.0000]])\n",
      "å½¢çŠ¶: torch.Size([3, 4])\n",
      "\n",
      "============================================================\n",
      "1è¡Œç›®ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³1ã®å‡ºåŠ›ï¼‰ã‚’è©³ã—ãè¨ˆç®—:\n",
      "============================================================\n",
      "attention_weights[0] = [0.7 0.2 0.1]\n",
      "\n",
      "é‡ã¿ä»˜ãå’Œã®è¨ˆç®—:\n",
      "  0.7 Ã— V[0] = 0.7 Ã— [1. 0. 0. 0.] = [0.7 0.  0.  0. ]\n",
      "  0.2 Ã— V[1] = 0.2 Ã— [0. 1. 0. 0.] = [0.  0.2 0.  0. ]\n",
      "  0.1 Ã— V[2] = 0.1 Ã— [0. 0. 1. 0.] = [0.  0.  0.1 0. ]\n",
      "  åˆè¨ˆ      = [0.7 0.2 0.1 0. ]\n",
      "\n",
      "ğŸ’¡ ãƒã‚¤ãƒ³ãƒˆ: å‡ºåŠ›ã®å„è¡Œã¯ã€Vã®è¡Œãƒ™ã‚¯ãƒˆãƒ«ã®é‡ã¿ä»˜ãå’Œ\n",
      "   åˆ—ãƒ™ã‚¯ãƒˆãƒ«ã§ã¯ãªãã€è¡Œãƒ™ã‚¯ãƒˆãƒ«ã‚’æ··ãœåˆã‚ã›ã¦ã„ã‚‹ï¼\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 3ã¤ã®ãƒˆãƒ¼ã‚¯ãƒ³ã€å„ãƒˆãƒ¼ã‚¯ãƒ³ã¯4æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«\n",
    "seq_len = 3\n",
    "d_model = 4\n",
    "\n",
    "# Vã®å„è¡ŒãŒ1ã¤ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¡¨ã™\n",
    "V = torch.tensor([\n",
    "    [1.0, 0.0, 0.0, 0.0],  # ãƒˆãƒ¼ã‚¯ãƒ³1: [1,0,0,0]\n",
    "    [0.0, 1.0, 0.0, 0.0],  # ãƒˆãƒ¼ã‚¯ãƒ³2: [0,1,0,0]\n",
    "    [0.0, 0.0, 1.0, 0.0]   # ãƒˆãƒ¼ã‚¯ãƒ³3: [0,0,1,0]\n",
    "])\n",
    "\n",
    "print(\"V (Valueè¡Œåˆ—):\")\n",
    "print(V)\n",
    "print(f\"å½¢çŠ¶: {V.shape} - å„è¡ŒãŒ1ã¤ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ™ã‚¯ãƒˆãƒ«\")\n",
    "print()\n",
    "\n",
    "# Attention Weightsã®å„è¡ŒãŒã€å„ãƒˆãƒ¼ã‚¯ãƒ³ã®æ³¨ç›®åˆ†å¸ƒ\n",
    "attention_weights = torch.tensor([\n",
    "    [0.7, 0.2, 0.1],  # ãƒˆãƒ¼ã‚¯ãƒ³1ã¯ã€è‡ªåˆ†ã«70%, ãƒˆãƒ¼ã‚¯ãƒ³2ã«20%, ãƒˆãƒ¼ã‚¯ãƒ³3ã«10%\n",
    "    [0.1, 0.8, 0.1],  # ãƒˆãƒ¼ã‚¯ãƒ³2ã¯ã€è‡ªåˆ†ã«80%\n",
    "    [0.2, 0.2, 0.6]   # ãƒˆãƒ¼ã‚¯ãƒ³3ã¯ã€è‡ªåˆ†ã«60%\n",
    "])\n",
    "\n",
    "print(\"Attention Weights:\")\n",
    "print(attention_weights)\n",
    "print(f\"å½¢çŠ¶: {attention_weights.shape}\")\n",
    "print(\"å„è¡Œã®å’Œ:\", attention_weights.sum(dim=1))  # å„è¡Œã®å’Œã¯1\n",
    "print()\n",
    "\n",
    "# è¡Œåˆ—ã®ç©ã‚’è¨ˆç®—\n",
    "output = torch.matmul(attention_weights, V)\n",
    "\n",
    "print(\"Output = Attention_Weights @ V:\")\n",
    "print(output)\n",
    "print(f\"å½¢çŠ¶: {output.shape}\")\n",
    "print()\n",
    "\n",
    "# 1è¡Œç›®ã‚’è©³ã—ãè¦‹ã‚‹\n",
    "print(\"=\" * 60)\n",
    "print(\"1è¡Œç›®ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³1ã®å‡ºåŠ›ï¼‰ã‚’è©³ã—ãè¨ˆç®—:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"attention_weights[0] =\", attention_weights[0].numpy())\n",
    "print()\n",
    "print(\"é‡ã¿ä»˜ãå’Œã®è¨ˆç®—:\")\n",
    "print(f\"  0.7 Ã— V[0] = 0.7 Ã— {V[0].numpy()} = {(0.7 * V[0]).numpy()}\")\n",
    "print(f\"  0.2 Ã— V[1] = 0.2 Ã— {V[1].numpy()} = {(0.2 * V[1]).numpy()}\")\n",
    "print(f\"  0.1 Ã— V[2] = 0.1 Ã— {V[2].numpy()} = {(0.1 * V[2]).numpy()}\")\n",
    "print(f\"  åˆè¨ˆ      = {output[0].numpy()}\")\n",
    "print()\n",
    "print(\"ğŸ’¡ ãƒã‚¤ãƒ³ãƒˆ: å‡ºåŠ›ã®å„è¡Œã¯ã€Vã®è¡Œãƒ™ã‚¯ãƒˆãƒ«ã®é‡ã¿ä»˜ãå’Œ\")\n",
    "print(\"   åˆ—ãƒ™ã‚¯ãƒˆãƒ«ã§ã¯ãªãã€è¡Œãƒ™ã‚¯ãƒˆãƒ«ã‚’æ··ãœåˆã‚ã›ã¦ã„ã‚‹ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddc1c49",
   "metadata": {},
   "source": [
    "#### æ•°å­¦çš„ãªèª¬æ˜\n",
    "\n",
    "##### è¡Œåˆ—ã®ç©ã®ä¸€èˆ¬å½¢\n",
    "\n",
    "$$\n",
    "C = AB\n",
    "$$\n",
    "\n",
    "ã“ã“ã§ $A$ ãŒ $[m \\times n]$ã€$B$ ãŒ $[n \\times p]$ ã®ã¨ãã€$C$ ã¯ $[m \\times p]$\n",
    "\n",
    "çµæœã®è¦ç´ :\n",
    "$$\n",
    "C_{ij} = \\sum_{k=1}^{n} A_{ik} B_{kj}\n",
    "$$\n",
    "\n",
    "##### Attentionã§ã®è§£é‡ˆ\n",
    "\n",
    "$$\n",
    "\\text{output} = \\text{attention\\_weights} \\times V\n",
    "$$\n",
    "\n",
    "- `attention_weights`: $[\\text{seq\\_len} \\times \\text{seq\\_len}]$\n",
    "- `V`: $[\\text{seq\\_len} \\times d_{\\text{model}}]$\n",
    "- `output`: $[\\text{seq\\_len} \\times d_{\\text{model}}]$\n",
    "\n",
    "å‡ºåŠ›ã®ç¬¬$i$è¡Œã€ç¬¬$j$åˆ—ã®è¦ç´ :\n",
    "\n",
    "$$\n",
    "\\text{output}_{ij} = \\sum_{k=1}^{\\text{seq\\_len}} \\text{attention\\_weights}_{ik} \\cdot V_{kj}\n",
    "$$\n",
    "\n",
    "**é‡è¦ãªæ´å¯Ÿ**:\n",
    "\n",
    "- $\\text{output}$ã®ç¬¬$i$**è¡Œ**ã¯ã€$V$ã®å…¨ã¦ã®**è¡Œ**ã®é‡ã¿ä»˜ãå’Œ\n",
    "- é‡ã¿ã¯$\\text{attention\\_weights}$ã®ç¬¬$i$è¡Œã‹ã‚‰æ¥ã‚‹\n",
    "- ã¤ã¾ã‚Šã€ã€Œå„ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆè¡Œï¼‰ãŒä»–ã®ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆè¡Œï¼‰ã‚’ã©ã†çµ„ã¿åˆã‚ã›ã‚‹ã‹ã€ã‚’è¨ˆç®—ã—ã¦ã„ã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02e8289",
   "metadata": {},
   "source": [
    "#### ãªãœé€šå¸¸ã®ç·šå½¢å¤‰æ›ã¨é•ã†ã®ã‹\n",
    "\n",
    "##### é€šå¸¸ã®ç·šå½¢å¤‰æ› (y = Wx)\n",
    "\n",
    "```python\n",
    "# 1ã¤ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’å¤‰æ›\n",
    "x = [x1, x2, x3]     # åˆ—ãƒ™ã‚¯ãƒˆãƒ« (ç¸¦)\n",
    "W @ x = y            # å¤‰æ›å¾Œã®åˆ—ãƒ™ã‚¯ãƒˆãƒ«\n",
    "```\n",
    "\n",
    "- ãƒ‡ãƒ¼ã‚¿: **åˆ—ãƒ™ã‚¯ãƒˆãƒ«**ï¼ˆç¸¦æ–¹å‘ï¼‰\n",
    "- 1ã¤ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å¤‰æ›\n",
    "\n",
    "##### Transformerã®ãƒãƒƒãƒå‡¦ç† (Y = XW)\n",
    "\n",
    "```python\n",
    "# è¤‡æ•°ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’ã¾ã¨ã‚ã¦å¤‰æ›\n",
    "X = [[x1_1, x1_2, x1_3],    # ã‚µãƒ³ãƒ—ãƒ«1 (æ¨ª)\n",
    "     [x2_1, x2_2, x2_3]]    # ã‚µãƒ³ãƒ—ãƒ«2 (æ¨ª)\n",
    "X @ W = Y                    # å¤‰æ›å¾Œã®è¡Œåˆ—ï¼ˆå„è¡ŒãŒã‚µãƒ³ãƒ—ãƒ«ï¼‰\n",
    "```\n",
    "\n",
    "- ãƒ‡ãƒ¼ã‚¿: è¡Œåˆ—ã®**å„è¡Œ**ãŒã‚µãƒ³ãƒ—ãƒ«ï¼ˆæ¨ªæ–¹å‘ï¼‰\n",
    "- è¤‡æ•°ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ã¾ã¨ã‚ã¦å‡¦ç†ï¼ˆåŠ¹ç‡åŒ–ï¼‰\n",
    "\n",
    "##### è»¢ç½®ã®é–¢ä¿‚\n",
    "\n",
    "å®Ÿã¯åŒã˜ã“ã¨ã‚’ã—ã¦ã„ã¾ã™ï¼\n",
    "\n",
    "```python\n",
    "# é€šå¸¸ã®æ›¸ãæ–¹\n",
    "y = W @ x           # åˆ—ãƒ™ã‚¯ãƒˆãƒ«\n",
    "\n",
    "# è»¢ç½®ã‚’å–ã‚‹ã¨\n",
    "y^T = x^T @ W^T     # è¡Œãƒ™ã‚¯ãƒˆãƒ«\n",
    "```\n",
    "\n",
    "Transformerã¯åŠ¹ç‡åŒ–ã®ãŸã‚ã€**æœ€åˆã‹ã‚‰è»¢ç½®ã—ãŸå½¢**ï¼ˆè¡Œãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã§å‡¦ç†ã—ã¦ã„ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f811a63a",
   "metadata": {},
   "source": [
    "#### ã¾ã¨ã‚\n",
    "\n",
    "**Q: Attention Weightsã¨Vã®ç©ã§ã€è¡Œã¨åˆ—ã®ã©ã¡ã‚‰ã«æ„å‘³ãŒã‚ã‚‹ã®ã‹ï¼Ÿ**\n",
    "\n",
    "**A: Transformerã§ã¯ã€Œè¡Œã€ã«æ„å‘³ãŒã‚ã‚Šã¾ã™ï¼**\n",
    "\n",
    "##### é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ\n",
    "\n",
    "| è¦³ç‚¹ | é€šå¸¸ã®ç·šå½¢å¤‰æ› | Transformer |\n",
    "|------|--------------|------------|\n",
    "| **ãƒ‡ãƒ¼ã‚¿ã®å‘ã** | åˆ—ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆç¸¦ï¼‰ | è¡Œãƒ™ã‚¯ãƒˆãƒ«ï¼ˆæ¨ªï¼‰ |\n",
    "| **å½¢å¼** | $y = Wx$ | $Y = XW$ |\n",
    "| **ã‚µãƒ³ãƒ—ãƒ«** | 1ã¤ãšã¤ | ãƒãƒƒãƒï¼ˆè¤‡æ•°ã¾ã¨ã‚ã¦ï¼‰ |\n",
    "| **æ„å‘³ã®ã‚ã‚‹æ–¹å‘** | ç¸¦ï¼ˆåˆ—ï¼‰ | æ¨ªï¼ˆè¡Œï¼‰ |\n",
    "\n",
    "##### Attentionè¨ˆç®—ã®æ§‹é€ \n",
    "\n",
    "```\n",
    "attention_weights @ V\n",
    "[seq_len, seq_len] @ [seq_len, d_model] = [seq_len, d_model]\n",
    "     â†“                      â†“                     â†“\n",
    "  å„è¡ŒãŒæ³¨ç›®åˆ†å¸ƒ        å„è¡ŒãŒãƒˆãƒ¼ã‚¯ãƒ³        å„è¡ŒãŒå‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³\n",
    "```\n",
    "\n",
    "**å„è¡Œ**ãŒæ„å‘³ã‚’æŒã¤:\n",
    "- Vã®å„**è¡Œ**: å„ãƒˆãƒ¼ã‚¯ãƒ³ã®å€¤ãƒ™ã‚¯ãƒˆãƒ«\n",
    "- attention_weightsã®å„**è¡Œ**: å„ãƒˆãƒ¼ã‚¯ãƒ³ã®æ³¨ç›®åˆ†å¸ƒ  \n",
    "- outputã®å„**è¡Œ**: Vã®è¡Œãƒ™ã‚¯ãƒˆãƒ«ã®é‡ã¿ä»˜ãå’Œ\n",
    "\n",
    "##### ãªãœã“ã®å½¢å¼ã‹\n",
    "\n",
    "1. **ãƒãƒƒãƒå‡¦ç†ã®åŠ¹ç‡åŒ–**: è¤‡æ•°ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¸¦åˆ—å‡¦ç†\n",
    "2. **å®Ÿè£…ã®çµ±ä¸€æ€§**: PyTorchãªã©ã®æ·±å±¤å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®æ…£ç¿’\n",
    "3. **æ•°å­¦çš„ã«ã¯åŒã˜**: è»¢ç½®ã‚’å–ã‚Œã°å¾“æ¥ã®åˆ—ãƒ™ã‚¯ãƒˆãƒ«å½¢å¼ã¨ç­‰ä¾¡\n",
    "\n",
    "##### ç†è§£ã®ã‚³ãƒ„\n",
    "\n",
    "**ã€Œè¡Œåˆ—ã®ç©ã¯ã€å·¦å´ã®è¡Œã¨å³å´ã®åˆ—ã®å†…ç©ã€**ã¨ã„ã†åŸºæœ¬ã‚’æ€ã„å‡ºã™:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "\\text{è¡Œ1} \\\\\n",
    "\\text{è¡Œ2} \\\\\n",
    "\\text{è¡Œ3}\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix} \n",
    "\\text{åˆ—1} & \\text{åˆ—2} & \\text{åˆ—3}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Transformerã§ã¯:\n",
    "- å·¦å´ï¼ˆattention_weightsï¼‰ã®**è¡Œ**: å„ãƒˆãƒ¼ã‚¯ãƒ³ã®æ³¨ç›®åˆ†å¸ƒ\n",
    "- å³å´ï¼ˆVï¼‰ã®**åˆ—**: ç‰¹å¾´æ¬¡å…ƒ\n",
    "- çµæœ: å„ãƒˆãƒ¼ã‚¯ãƒ³ãŒä»–ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®å€¤ã‚’é‡ã¿ä»˜ã‘ã—ã¦é›†ç´„\n",
    "\n",
    "ã“ã®ç†è§£ãŒã‚ã‚Œã°ã€Transformerã®è¨ˆç®—ãŒã™ã£ãã‚Šè¦‹ãˆã¦ãã¾ã™ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a88c740",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q17: Self-Attentionã®å³å¯†ãªå®šç¾©ã«ã¤ã„ã¦\n",
    "\n",
    "**è³ªå•æ—¥**: 2025å¹´11æœˆ21æ—¥\n",
    "\n",
    "### è³ªå•\n",
    "å³å¯†ã«è¨€ã†ã¨Self-Attentionã¨ã¯ã€å…¥åŠ›ã‹ã‚‰ç·šå½¢å¤‰æ›å±¤ã®å‡ºåŠ›ï¼ˆQ, K, Vï¼‰ã‚’Scaled Dot-product Attentionã«å…¥ã‚ŒãŸå‡ºåŠ›ã®ã“ã¨ã¨ç†è§£ã—ã¦è‰¯ã„ã‹ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a791f254",
   "metadata": {},
   "source": [
    "### å›ç­”\n",
    "\n",
    "ã¯ã„ã€ãã®ç†è§£ã§æ­£ç¢ºã§ã™ã€‚\n",
    "\n",
    "#### Self-Attentionã®å³å¯†ãªå®šç¾©\n",
    "\n",
    "**Self-Attention = ç·šå½¢å¤‰æ›(Q, K, V) + Scaled Dot-Product Attention + å‡ºåŠ›ç·šå½¢å¤‰æ›**\n",
    "\n",
    "ã¨ã„ã†æ§‹é€ ã§ã™ã€‚\n",
    "\n",
    "#### å…·ä½“çš„ãªå‡¦ç†ãƒ•ãƒ­ãƒ¼\n",
    "\n",
    "```\n",
    "å…¥åŠ› X (shape: [batch, seq_len, d_model])\n",
    "    â†“\n",
    "Q = X @ W_q  (Query ã¸ã®ç·šå½¢å¤‰æ›)\n",
    "K = X @ W_k  (Key ã¸ã®ç·šå½¢å¤‰æ›)  \n",
    "V = X @ W_v  (Value ã¸ã®ç·šå½¢å¤‰æ›)\n",
    "    â†“\n",
    "Attention(Q, K, V) = softmax(QK^T / âˆšd_k) @ V  (Scaled Dot-Product Attention)\n",
    "    â†“\n",
    "output = Attention @ W_o  (å‡ºåŠ›ç·šå½¢å¤‰æ›)\n",
    "```\n",
    "\n",
    "#### å®Ÿè£…ã§ã®å¯¾å¿œ\n",
    "\n",
    "`src/attention.py`ã®`SelfAttention`ã‚¯ãƒ©ã‚¹ã¯ã¾ã•ã«ã“ã®æ§‹é€ ã‚’å®Ÿè£…ã—ã¦ã„ã¾ã™:\n",
    "\n",
    "1. **ç·šå½¢å¤‰æ›å±¤** (`__init__`ã§å®šç¾©):\n",
    "   - `self.query_linear`, `self.key_linear`, `self.value_linear`\n",
    "\n",
    "2. **Scaled Dot-Product Attention** (`forward`å†…):\n",
    "   - `scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)`\n",
    "   - `attention_weights = F.softmax(scores, dim=-1)`\n",
    "   - `output = torch.matmul(attention_weights, V)`\n",
    "\n",
    "3. **å‡ºåŠ›ç·šå½¢å¤‰æ›**:\n",
    "   - `self.output_linear`\n",
    "\n",
    "#### \"Self\"ã®æ„å‘³\n",
    "\n",
    "\"Self\"ã¨ã„ã†ã®ã¯ã€Q, K, V **ã™ã¹ã¦ãŒåŒã˜å…¥åŠ›Xã‹ã‚‰ç”Ÿæˆã•ã‚Œã‚‹**ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ï¼ˆä»–ã®ç³»åˆ—ã‹ã‚‰ã§ã¯ãªãã€è‡ªåˆ†è‡ªèº«ã‹ã‚‰ï¼‰ã€‚\n",
    "\n",
    "ã“ã‚Œã«å¯¾ã—ã¦Cross-Attentionã§ã¯ã€Qã¨K/VãŒç•°ãªã‚‹å…¥åŠ›ã‹ã‚‰ç”Ÿæˆã•ã‚Œã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590d551b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q18: Self-Attentionã®å‡ºåŠ›å½¢çŠ¶ã«ã¤ã„ã¦\n",
    "\n",
    "**è³ªå•æ—¥**: 2025å¹´11æœˆ21æ—¥\n",
    "\n",
    "### è³ªå•\n",
    "Self-Attentionã¯å…¥åŠ›ã¨åŒã˜å½¢ã‚’ã—ã¦ã„ã‚‹ã¨è€ƒãˆã¦è‰¯ã„ã‹ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d39a6ef",
   "metadata": {},
   "source": [
    "### å›ç­”\n",
    "\n",
    "ã¯ã„ã€**Self-Attentionã®å‡ºåŠ›ã¯å…¥åŠ›ã¨åŒã˜å½¢**ã‚’ã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "#### å½¢çŠ¶ã®ä¸€è‡´\n",
    "\n",
    "- **å…¥åŠ›**: `[batch_size, seq_len, d_model]`\n",
    "- **å‡ºåŠ›**: `[batch_size, seq_len, d_model]`\n",
    "\n",
    "#### ãªãœåŒã˜å½¢çŠ¶ãªã®ã‹\n",
    "\n",
    "1. **æ®‹å·®æ¥ç¶š(Residual Connection)ãŒå¯èƒ½**: \n",
    "   - Transformerã§ã¯ `output = LayerNorm(x + SelfAttention(x))` ã®ã‚ˆã†ã«å…¥åŠ›ã‚’è¶³ã—åˆã‚ã›ã‚‹\n",
    "   - ãã®ãŸã‚ã€å½¢çŠ¶ãŒåŒã˜ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™\n",
    "\n",
    "2. **å±¤ã‚’ç©ã¿é‡ã­ã‚‰ã‚Œã‚‹**: \n",
    "   - å‡ºåŠ›ãŒå…¥åŠ›ã¨åŒã˜å½¢ãªã®ã§ã€Self-Attentionå±¤ã‚’ä½•å±¤ã‚‚é‡ã­ã‚‹ã“ã¨ãŒã§ãã¾ã™\n",
    "\n",
    "3. **ç³»åˆ—é•·ãŒä¿æŒã•ã‚Œã‚‹**: \n",
    "   - å„ä½ç½®ã®æƒ…å ±ãŒä¿æŒã•ã‚Œã€ç³»åˆ—å…¨ä½“ã®æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸæ–°ã—ã„è¡¨ç¾ã«å¤‰æ›ã•ã‚Œã¾ã™\n",
    "\n",
    "#### å†…éƒ¨ã§ã®æ¬¡å…ƒå¤‰åŒ–\n",
    "\n",
    "å†…éƒ¨ã§ã¯æ¬¡å…ƒãŒå¤‰åŒ–ã—ã¾ã™ãŒã€æœ€çµ‚çš„ã«å…ƒã«æˆ»ã‚Šã¾ã™:\n",
    "\n",
    "```\n",
    "å…¥åŠ› X: [batch, seq_len, d_model]\n",
    "    â†“ ç·šå½¢å¤‰æ›\n",
    "Q, K, V: [batch, seq_len, d_k]  (d_k ã¯é€šå¸¸ d_model ã¨åŒã˜ã‹å°ã•ã„)\n",
    "    â†“ Scaled Dot-Product Attention\n",
    "Attentionå‡ºåŠ›: [batch, seq_len, d_k]\n",
    "    â†“ å‡ºåŠ›ç·šå½¢å¤‰æ›(ã‚ã‚Œã°)\n",
    "æœ€çµ‚å‡ºåŠ›: [batch, seq_len, d_model]  (å…¥åŠ›ã¨åŒã˜!)\n",
    "```\n",
    "\n",
    "#### å®Ÿè£…ã§ã®ç¢ºèª\n",
    "\n",
    "`src/attention.py`ã®å®Ÿè£…ã§ã‚‚ã€`d_model`ã‚’å…¥åŠ›ãƒ»å‡ºåŠ›ã®ä¸¡æ–¹ã§ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ã“ã®å½¢çŠ¶ã®ä¸€è‡´ã‚’ä¿è¨¼ã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "```python\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.query_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.key_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.value_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.output_linear = nn.Linear(d_model, d_model)  # åŒã˜d_model\n",
    "```\n",
    "\n",
    "ã“ã®å…¥å‡ºåŠ›ã®å½¢çŠ¶ä¸€è‡´ãŒã€Transformerã®æ·±ã„æ§‹é€ ã‚’å¯èƒ½ã«ã—ã¦ã„ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ca3559",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q19: Multi-Head Attentionã®headåˆ†å‰²ã«ã¤ã„ã¦\n",
    "\n",
    "**è³ªå•æ—¥**: 2025å¹´11æœˆ21æ—¥\n",
    "\n",
    "### è³ªå•\n",
    "Multi-Head Attentionã«ã¤ã„ã¦ã€headåˆ†å‰²ã¨ã¯ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’åˆ†å‰²ã™ã‚‹ã“ã¨ã¨æ‰ãˆã¦ã„ã„ã‹ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9c6399",
   "metadata": {},
   "source": [
    "### å›ç­”\n",
    "\n",
    "ã¯ã„ã€ãã®ç†è§£ã§æ­£ç¢ºã§ã™ï¼\n",
    "\n",
    "**Multi-Head Attentionã®headåˆ†å‰² = ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆd_modelæ¬¡å…ƒï¼‰ã‚’åˆ†å‰²ã™ã‚‹ã“ã¨**\n",
    "\n",
    "#### å…·ä½“çš„ãªåˆ†å‰²ã®ä»•çµ„ã¿\n",
    "\n",
    "```\n",
    "å…¥åŠ›: [batch, seq_len, d_model=512]\n",
    "    â†“ ç·šå½¢å¤‰æ› (W_q)\n",
    "Q: [batch, seq_len, d_model=512]\n",
    "    â†“ headåˆ†å‰²\n",
    "Q: [batch, num_heads=8, seq_len, d_k=64]\n",
    "```\n",
    "\n",
    "ä¾‹ãˆã° `d_model=512`, `num_heads=8` ã®å ´åˆ:\n",
    "- å„headã¯ **d_k = 512 / 8 = 64æ¬¡å…ƒ** ã‚’æ‹…å½“\n",
    "- 512æ¬¡å…ƒã®ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’ã€8å€‹ã®64æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã«åˆ†å‰²\n",
    "- å„headã¯ **ç‰¹å¾´é‡ç©ºé–“ã®ç•°ãªã‚‹éƒ¨åˆ†** ã‚’è¦‹ã‚‹\n",
    "\n",
    "#### é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ\n",
    "\n",
    "1. **åˆ†å‰²ã¯ç‰¹å¾´é‡æ¬¡å…ƒ**: \n",
    "   - ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ï¼ˆseq_lenï¼‰ã¯åˆ†å‰²ã•ã‚Œãšã€ãã®ã¾ã¾ä¿æŒ\n",
    "   - ç‰¹å¾´é‡æ¬¡å…ƒï¼ˆd_modelï¼‰ã ã‘ãŒåˆ†å‰²ã•ã‚Œã‚‹\n",
    "\n",
    "2. **ç‹¬ç«‹ã—ãŸéƒ¨åˆ†ç©ºé–“**: \n",
    "   - å„headã¯ç‰¹å¾´é‡ã®ç•°ãªã‚‹å´é¢ã‚’å­¦ç¿’\n",
    "   - Head 1: æ–‡æ³•çš„ãªé–¢ä¿‚ã‚’æ‰ãˆã‚‹\n",
    "   - Head 2: æ„å‘³çš„ãªé¡ä¼¼æ€§ã‚’æ‰ãˆã‚‹\n",
    "   - Head 3: ä½ç½®é–¢ä¿‚ã‚’æ‰ãˆã‚‹ï¼ˆãªã©ï¼‰\n",
    "\n",
    "3. **æœ€å¾Œã«çµåˆ**: \n",
    "   - å…¨headã®å‡ºåŠ›ã‚’çµåˆï¼ˆconcatï¼‰ã—ã¦å…ƒã®`d_model`æ¬¡å…ƒã«æˆ»ã™\n",
    "   - `[batch, num_heads, seq_len, d_k]` â†’ `[batch, seq_len, d_model]`\n",
    "\n",
    "#### å®Ÿè£…ã§ã®ç¢ºèª\n",
    "\n",
    "`src/attention.py`ã®`MultiHeadAttention`ã‚¯ãƒ©ã‚¹ã®`split_heads`ãƒ¡ã‚½ãƒƒãƒ‰ãŒã¾ã•ã«ã“ã®åˆ†å‰²å‡¦ç†ã‚’ã—ã¦ã„ã¾ã™:\n",
    "\n",
    "```python\n",
    "def split_heads(self, x, batch_size):\n",
    "    \"\"\"\n",
    "    å…¥åŠ›ã‚’è¤‡æ•°ã®headã«åˆ†å‰²\n",
    "    \n",
    "    Args:\n",
    "        x: shape [batch_size, seq_len, d_model]\n",
    "    Returns:\n",
    "        shape [batch_size, num_heads, seq_len, d_k]\n",
    "    \"\"\"\n",
    "    # [batch, seq_len, d_model] -> [batch, seq_len, num_heads, d_k]\n",
    "    x = x.view(batch_size, -1, self.num_heads, self.d_k)\n",
    "    # [batch, seq_len, num_heads, d_k] -> [batch, num_heads, seq_len, d_k]\n",
    "    return x.transpose(1, 2)\n",
    "```\n",
    "\n",
    "ã“ã‚Œã§512æ¬¡å…ƒã®ç‰¹å¾´é‡ã‚’8å€‹ã®64æ¬¡å…ƒã«åˆ†å‰²ã—ã€å„headã§ä¸¦åˆ—å‡¦ç†ã—ã¦ã„ã¾ã™ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a229df2",
   "metadata": {},
   "source": [
    "#### ã‚³ãƒ¼ãƒ‰ä¾‹: headåˆ†å‰²ã®å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "70adfc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å…ƒã®å…¥åŠ›å½¢çŠ¶: torch.Size([2, 5, 512])\n",
      "  - batch_size: 2\n",
      "  - seq_len: 5\n",
      "  - d_model: 512\n",
      "\n",
      "reshapeå¾Œ: torch.Size([2, 5, 8, 64])\n",
      "transposeå¾Œï¼ˆheadåˆ†å‰²å®Œäº†ï¼‰: torch.Size([2, 8, 5, 64])\n",
      "  - batch_size: 2\n",
      "  - num_heads: 8 (8å€‹ã®headã«åˆ†å‰²)\n",
      "  - seq_len: 5 (ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã¯ä¿æŒ)\n",
      "  - d_k: 64 (å„headã¯64æ¬¡å…ƒã‚’æ‹…å½“)\n",
      "\n",
      "ğŸ’¡ ç‰¹å¾´é‡ã®512æ¬¡å…ƒãŒã€8å€‹ã®64æ¬¡å…ƒã«åˆ†å‰²ã•ã‚Œã¾ã—ãŸï¼\n",
      "   å„headãŒç‰¹å¾´é‡ç©ºé–“ã®ç•°ãªã‚‹éƒ¨åˆ†ã‚’ä¸¦åˆ—ã«å‡¦ç†ã—ã¾ã™ã€‚\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_k = d_model // num_heads  # 64\n",
    "\n",
    "# ã‚µãƒ³ãƒ—ãƒ«å…¥åŠ›\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "print(f\"å…ƒã®å…¥åŠ›å½¢çŠ¶: {x.shape}\")\n",
    "print(f\"  - batch_size: {batch_size}\")\n",
    "print(f\"  - seq_len: {seq_len}\")\n",
    "print(f\"  - d_model: {d_model}\")\n",
    "\n",
    "# headåˆ†å‰²ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "# [batch, seq_len, d_model] -> [batch, seq_len, num_heads, d_k]\n",
    "x_reshaped = x.view(batch_size, seq_len, num_heads, d_k)\n",
    "print(f\"\\nreshapeå¾Œ: {x_reshaped.shape}\")\n",
    "\n",
    "# [batch, seq_len, num_heads, d_k] -> [batch, num_heads, seq_len, d_k]\n",
    "x_split = x_reshaped.transpose(1, 2)\n",
    "print(f\"transposeå¾Œï¼ˆheadåˆ†å‰²å®Œäº†ï¼‰: {x_split.shape}\")\n",
    "print(f\"  - batch_size: {x_split.shape[0]}\")\n",
    "print(f\"  - num_heads: {x_split.shape[1]} (8å€‹ã®headã«åˆ†å‰²)\")\n",
    "print(f\"  - seq_len: {x_split.shape[2]} (ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã¯ä¿æŒ)\")\n",
    "print(f\"  - d_k: {x_split.shape[3]} (å„headã¯{d_k}æ¬¡å…ƒã‚’æ‹…å½“)\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ ç‰¹å¾´é‡ã®{d_model}æ¬¡å…ƒãŒã€{num_heads}å€‹ã®{d_k}æ¬¡å…ƒã«åˆ†å‰²ã•ã‚Œã¾ã—ãŸï¼\")\n",
    "print(f\"   å„headãŒç‰¹å¾´é‡ç©ºé–“ã®ç•°ãªã‚‹éƒ¨åˆ†ã‚’ä¸¦åˆ—ã«å‡¦ç†ã—ã¾ã™ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bbfc92",
   "metadata": {},
   "source": [
    "#### çµåˆã®ä»•çµ„ã¿\n",
    "\n",
    "åˆ†å‰²ã¨é€†ã®æ“ä½œã§ã€å…¨headã®å‡ºåŠ›ã‚’çµåˆã—ã¾ã™:\n",
    "\n",
    "```python\n",
    "def combine_heads(self, x, batch_size):\n",
    "    \"\"\"\n",
    "    è¤‡æ•°ã®headã‚’çµåˆ\n",
    "    \n",
    "    Args:\n",
    "        x: shape [batch_size, num_heads, seq_len, d_k]\n",
    "    Returns:\n",
    "        shape [batch_size, seq_len, d_model]\n",
    "    \"\"\"\n",
    "    # [batch, num_heads, seq_len, d_k] -> [batch, seq_len, num_heads, d_k]\n",
    "    x = x.transpose(1, 2).contiguous()\n",
    "    # [batch, seq_len, num_heads, d_k] -> [batch, seq_len, d_model]\n",
    "    return x.view(batch_size, -1, self.d_model)\n",
    "```\n",
    "\n",
    "ã“ã®åˆ†å‰²ã¨çµåˆã«ã‚ˆã‚Šã€Multi-Head Attentionã¯è¤‡æ•°ã®è¡¨ç¾éƒ¨åˆ†ç©ºé–“ã‹ã‚‰ä¸¦åˆ—ã«æƒ…å ±ã‚’å­¦ç¿’ã§ãã¾ã™ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3affc73",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q17: Self-Attentionã®å³å¯†ãªå®šç¾©ã«ã¤ã„ã¦\n",
    "\n",
    "**è³ªå•æ—¥**: 2025å¹´11æœˆ21æ—¥\n",
    "\n",
    "### è³ªå•\n",
    "å³å¯†ã«è¨€ã†ã¨Self-Attentionã¨ã¯ã€å…¥åŠ›ã‹ã‚‰ç·šå½¢å¤‰æ›å±¤ã®å‡ºåŠ›ï¼ˆQ, K, Vï¼‰ã‚’Scaled Dot-product Attentionã«å…¥ã‚ŒãŸå‡ºåŠ›ã®ã“ã¨ã¨ç†è§£ã—ã¦è‰¯ã„ã‹ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebb1919",
   "metadata": {},
   "source": [
    "### å›ç­”\n",
    "\n",
    "ã¯ã„ã€ãã®ç†è§£ã§æ­£ç¢ºã§ã™ã€‚\n",
    "\n",
    "#### Self-Attentionã®å³å¯†ãªå®šç¾©\n",
    "\n",
    "**Self-Attention = ç·šå½¢å¤‰æ›(Q, K, V) + Scaled Dot-Product Attention + å‡ºåŠ›ç·šå½¢å¤‰æ›**\n",
    "\n",
    "ã¨ã„ã†æ§‹é€ ã§ã™ã€‚\n",
    "\n",
    "#### å…·ä½“çš„ãªå‡¦ç†ãƒ•ãƒ­ãƒ¼\n",
    "\n",
    "```\n",
    "å…¥åŠ› X (shape: [batch, seq_len, d_model])\n",
    "    â†“\n",
    "Q = X @ W_q  (Query ã¸ã®ç·šå½¢å¤‰æ›)\n",
    "K = X @ W_k  (Key ã¸ã®ç·šå½¢å¤‰æ›)  \n",
    "V = X @ W_v  (Value ã¸ã®ç·šå½¢å¤‰æ›)\n",
    "    â†“\n",
    "Attention(Q, K, V) = softmax(QK^T / âˆšd_k) @ V  (Scaled Dot-Product Attention)\n",
    "    â†“\n",
    "output = Attention @ W_o  (å‡ºåŠ›ç·šå½¢å¤‰æ›)\n",
    "```\n",
    "\n",
    "#### å®Ÿè£…ã§ã®å¯¾å¿œ\n",
    "\n",
    "`src/attention.py`ã®`SelfAttention`ã‚¯ãƒ©ã‚¹ã¯ã¾ã•ã«ã“ã®æ§‹é€ ã‚’å®Ÿè£…ã—ã¦ã„ã¾ã™:\n",
    "\n",
    "1. **ç·šå½¢å¤‰æ›å±¤** (`__init__`ã§å®šç¾©):\n",
    "   - `self.query_linear`, `self.key_linear`, `self.value_linear`\n",
    "\n",
    "2. **Scaled Dot-Product Attention** (`forward`å†…):\n",
    "   - `scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)`\n",
    "   - `attention_weights = F.softmax(scores, dim=-1)`\n",
    "   - `output = torch.matmul(attention_weights, V)`\n",
    "\n",
    "3. **å‡ºåŠ›ç·šå½¢å¤‰æ›**:\n",
    "   - `self.output_linear`\n",
    "\n",
    "#### \"Self\"ã®æ„å‘³\n",
    "\n",
    "\"Self\"ã¨ã„ã†ã®ã¯ã€Q, K, V **ã™ã¹ã¦ãŒåŒã˜å…¥åŠ›Xã‹ã‚‰ç”Ÿæˆã•ã‚Œã‚‹**ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ï¼ˆä»–ã®ç³»åˆ—ã‹ã‚‰ã§ã¯ãªãã€è‡ªåˆ†è‡ªèº«ã‹ã‚‰ï¼‰ã€‚\n",
    "\n",
    "ã“ã‚Œã«å¯¾ã—ã¦Cross-Attentionã§ã¯ã€Qã¨K/VãŒç•°ãªã‚‹å…¥åŠ›ã‹ã‚‰ç”Ÿæˆã•ã‚Œã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bbacfd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q18: Self-Attentionã®å‡ºåŠ›å½¢çŠ¶ã«ã¤ã„ã¦\n",
    "\n",
    "**è³ªå•æ—¥**: 2025å¹´11æœˆ21æ—¥\n",
    "\n",
    "### è³ªå•\n",
    "Self-Attentionã¯å…¥åŠ›ã¨åŒã˜å½¢ã‚’ã—ã¦ã„ã‚‹ã¨è€ƒãˆã¦è‰¯ã„ã‹ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aecc52d",
   "metadata": {},
   "source": [
    "### å›ç­”\n",
    "\n",
    "ã¯ã„ã€**Self-Attentionã®å‡ºåŠ›ã¯å…¥åŠ›ã¨åŒã˜å½¢**ã‚’ã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "#### å½¢çŠ¶ã®ä¸€è‡´\n",
    "\n",
    "- **å…¥åŠ›**: `[batch_size, seq_len, d_model]`\n",
    "- **å‡ºåŠ›**: `[batch_size, seq_len, d_model]`\n",
    "\n",
    "#### ãªãœåŒã˜å½¢çŠ¶ãªã®ã‹\n",
    "\n",
    "1. **æ®‹å·®æ¥ç¶š(Residual Connection)ãŒå¯èƒ½**: \n",
    "   - Transformerã§ã¯ `output = LayerNorm(x + SelfAttention(x))` ã®ã‚ˆã†ã«å…¥åŠ›ã‚’è¶³ã—åˆã‚ã›ã‚‹\n",
    "   - ãã®ãŸã‚ã€å½¢çŠ¶ãŒåŒã˜ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™\n",
    "\n",
    "2. **å±¤ã‚’ç©ã¿é‡ã­ã‚‰ã‚Œã‚‹**: \n",
    "   - å‡ºåŠ›ãŒå…¥åŠ›ã¨åŒã˜å½¢ãªã®ã§ã€Self-Attentionå±¤ã‚’ä½•å±¤ã‚‚é‡ã­ã‚‹ã“ã¨ãŒã§ãã¾ã™\n",
    "\n",
    "3. **ç³»åˆ—é•·ãŒä¿æŒã•ã‚Œã‚‹**: \n",
    "   - å„ä½ç½®ã®æƒ…å ±ãŒä¿æŒã•ã‚Œã€ç³»åˆ—å…¨ä½“ã®æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸæ–°ã—ã„è¡¨ç¾ã«å¤‰æ›ã•ã‚Œã¾ã™\n",
    "\n",
    "#### å†…éƒ¨ã§ã®æ¬¡å…ƒå¤‰åŒ–\n",
    "\n",
    "å†…éƒ¨ã§ã¯æ¬¡å…ƒãŒå¤‰åŒ–ã—ã¾ã™ãŒã€æœ€çµ‚çš„ã«å…ƒã«æˆ»ã‚Šã¾ã™:\n",
    "\n",
    "```\n",
    "å…¥åŠ› X: [batch, seq_len, d_model]\n",
    "    â†“ ç·šå½¢å¤‰æ›\n",
    "Q, K, V: [batch, seq_len, d_k]  (d_k ã¯é€šå¸¸ d_model ã¨åŒã˜ã‹å°ã•ã„)\n",
    "    â†“ Scaled Dot-Product Attention\n",
    "Attentionå‡ºåŠ›: [batch, seq_len, d_k]\n",
    "    â†“ å‡ºåŠ›ç·šå½¢å¤‰æ›(ã‚ã‚Œã°)\n",
    "æœ€çµ‚å‡ºåŠ›: [batch, seq_len, d_model]  (å…¥åŠ›ã¨åŒã˜!)\n",
    "```\n",
    "\n",
    "#### å®Ÿè£…ã§ã®ç¢ºèª\n",
    "\n",
    "`src/attention.py`ã®å®Ÿè£…ã§ã‚‚ã€`d_model`ã‚’å…¥åŠ›ãƒ»å‡ºåŠ›ã®ä¸¡æ–¹ã§ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ã“ã®å½¢çŠ¶ã®ä¸€è‡´ã‚’ä¿è¨¼ã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "```python\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.query_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.key_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.value_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.output_linear = nn.Linear(d_model, d_model)  # åŒã˜d_model\n",
    "```\n",
    "\n",
    "ã“ã®å…¥å‡ºåŠ›ã®å½¢çŠ¶ä¸€è‡´ãŒã€Transformerã®æ·±ã„æ§‹é€ ã‚’å¯èƒ½ã«ã—ã¦ã„ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44376693",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q19: Multi-Head Attentionã®headåˆ†å‰²ã«ã¤ã„ã¦\n",
    "\n",
    "**è³ªå•æ—¥**: 2025å¹´11æœˆ21æ—¥\n",
    "\n",
    "### è³ªå•\n",
    "Multi-Head Attentionã«ã¤ã„ã¦ã€headåˆ†å‰²ã¨ã¯ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’åˆ†å‰²ã™ã‚‹ã“ã¨ã¨æ‰ãˆã¦ã„ã„ã‹ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c04f96",
   "metadata": {},
   "source": [
    "### å›ç­”\n",
    "\n",
    "ã¯ã„ã€ãã®ç†è§£ã§æ­£ç¢ºã§ã™ï¼\n",
    "\n",
    "**Multi-Head Attentionã®headåˆ†å‰² = ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆd_modelæ¬¡å…ƒï¼‰ã‚’åˆ†å‰²ã™ã‚‹ã“ã¨**\n",
    "\n",
    "#### å…·ä½“çš„ãªåˆ†å‰²ã®ä»•çµ„ã¿\n",
    "\n",
    "```\n",
    "å…¥åŠ›: [batch, seq_len, d_model=512]\n",
    "    â†“ ç·šå½¢å¤‰æ› (W_q)\n",
    "Q: [batch, seq_len, d_model=512]\n",
    "    â†“ headåˆ†å‰²\n",
    "Q: [batch, num_heads=8, seq_len, d_k=64]\n",
    "```\n",
    "\n",
    "ä¾‹ãˆã° `d_model=512`, `num_heads=8` ã®å ´åˆ:\n",
    "- å„headã¯ **d_k = 512 / 8 = 64æ¬¡å…ƒ** ã‚’æ‹…å½“\n",
    "- 512æ¬¡å…ƒã®ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’ã€8å€‹ã®64æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã«åˆ†å‰²\n",
    "- å„headã¯ **ç‰¹å¾´é‡ç©ºé–“ã®ç•°ãªã‚‹éƒ¨åˆ†** ã‚’è¦‹ã‚‹\n",
    "\n",
    "#### é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ\n",
    "\n",
    "1. **åˆ†å‰²ã¯ç‰¹å¾´é‡æ¬¡å…ƒ**: \n",
    "   - ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ï¼ˆseq_lenï¼‰ã¯åˆ†å‰²ã•ã‚Œãšã€ãã®ã¾ã¾ä¿æŒ\n",
    "   - ç‰¹å¾´é‡æ¬¡å…ƒï¼ˆd_modelï¼‰ã ã‘ãŒåˆ†å‰²ã•ã‚Œã‚‹\n",
    "\n",
    "2. **ç‹¬ç«‹ã—ãŸéƒ¨åˆ†ç©ºé–“**: \n",
    "   - å„headã¯ç‰¹å¾´é‡ã®ç•°ãªã‚‹å´é¢ã‚’å­¦ç¿’\n",
    "   - Head 1: æ–‡æ³•çš„ãªé–¢ä¿‚ã‚’æ‰ãˆã‚‹\n",
    "   - Head 2: æ„å‘³çš„ãªé¡ä¼¼æ€§ã‚’æ‰ãˆã‚‹\n",
    "   - Head 3: ä½ç½®é–¢ä¿‚ã‚’æ‰ãˆã‚‹ï¼ˆãªã©ï¼‰\n",
    "\n",
    "3. **æœ€å¾Œã«çµåˆ**: \n",
    "   - å…¨headã®å‡ºåŠ›ã‚’çµåˆï¼ˆconcatï¼‰ã—ã¦å…ƒã®`d_model`æ¬¡å…ƒã«æˆ»ã™\n",
    "   - `[batch, num_heads, seq_len, d_k]` â†’ `[batch, seq_len, d_model]`\n",
    "\n",
    "#### å®Ÿè£…ã§ã®ç¢ºèª\n",
    "\n",
    "`src/attention.py`ã®`MultiHeadAttention`ã‚¯ãƒ©ã‚¹ã®`split_heads`ãƒ¡ã‚½ãƒƒãƒ‰ãŒã¾ã•ã«ã“ã®åˆ†å‰²å‡¦ç†ã‚’ã—ã¦ã„ã¾ã™:\n",
    "\n",
    "```python\n",
    "def split_heads(self, x, batch_size):\n",
    "    \"\"\"\n",
    "    å…¥åŠ›ã‚’è¤‡æ•°ã®headã«åˆ†å‰²\n",
    "    \n",
    "    Args:\n",
    "        x: shape [batch_size, seq_len, d_model]\n",
    "    Returns:\n",
    "        shape [batch_size, num_heads, seq_len, d_k]\n",
    "    \"\"\"\n",
    "    # [batch, seq_len, d_model] -> [batch, seq_len, num_heads, d_k]\n",
    "    x = x.view(batch_size, -1, self.num_heads, self.d_k)\n",
    "    # [batch, seq_len, num_heads, d_k] -> [batch, num_heads, seq_len, d_k]\n",
    "    return x.transpose(1, 2)\n",
    "```\n",
    "\n",
    "ã“ã‚Œã§512æ¬¡å…ƒã®ç‰¹å¾´é‡ã‚’8å€‹ã®64æ¬¡å…ƒã«åˆ†å‰²ã—ã€å„headã§ä¸¦åˆ—å‡¦ç†ã—ã¦ã„ã¾ã™ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ee0358",
   "metadata": {},
   "source": [
    "#### ã‚³ãƒ¼ãƒ‰ä¾‹: headåˆ†å‰²ã®å¯è¦–åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "36a4634f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å…ƒã®å…¥åŠ›å½¢çŠ¶: torch.Size([2, 5, 512])\n",
      "  - batch_size: 2\n",
      "  - seq_len: 5\n",
      "  - d_model: 512\n",
      "\n",
      "reshapeå¾Œ: torch.Size([2, 5, 8, 64])\n",
      "transposeå¾Œï¼ˆheadåˆ†å‰²å®Œäº†ï¼‰: torch.Size([2, 8, 5, 64])\n",
      "  - batch_size: 2\n",
      "  - num_heads: 8 (8å€‹ã®headã«åˆ†å‰²)\n",
      "  - seq_len: 5 (ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã¯ä¿æŒ)\n",
      "  - d_k: 64 (å„headã¯64æ¬¡å…ƒã‚’æ‹…å½“)\n",
      "\n",
      "ğŸ’¡ ç‰¹å¾´é‡ã®512æ¬¡å…ƒãŒã€8å€‹ã®64æ¬¡å…ƒã«åˆ†å‰²ã•ã‚Œã¾ã—ãŸï¼\n",
      "   å„headãŒç‰¹å¾´é‡ç©ºé–“ã®ç•°ãªã‚‹éƒ¨åˆ†ã‚’ä¸¦åˆ—ã«å‡¦ç†ã—ã¾ã™ã€‚\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_k = d_model // num_heads  # 64\n",
    "\n",
    "# ã‚µãƒ³ãƒ—ãƒ«å…¥åŠ›\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "print(f\"å…ƒã®å…¥åŠ›å½¢çŠ¶: {x.shape}\")\n",
    "print(f\"  - batch_size: {batch_size}\")\n",
    "print(f\"  - seq_len: {seq_len}\")\n",
    "print(f\"  - d_model: {d_model}\")\n",
    "\n",
    "# headåˆ†å‰²ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "# [batch, seq_len, d_model] -> [batch, seq_len, num_heads, d_k]\n",
    "x_reshaped = x.view(batch_size, seq_len, num_heads, d_k)\n",
    "print(f\"\\nreshapeå¾Œ: {x_reshaped.shape}\")\n",
    "\n",
    "# [batch, seq_len, num_heads, d_k] -> [batch, num_heads, seq_len, d_k]\n",
    "x_split = x_reshaped.transpose(1, 2)\n",
    "print(f\"transposeå¾Œï¼ˆheadåˆ†å‰²å®Œäº†ï¼‰: {x_split.shape}\")\n",
    "print(f\"  - batch_size: {x_split.shape[0]}\")\n",
    "print(f\"  - num_heads: {x_split.shape[1]} (8å€‹ã®headã«åˆ†å‰²)\")\n",
    "print(f\"  - seq_len: {x_split.shape[2]} (ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã¯ä¿æŒ)\")\n",
    "print(f\"  - d_k: {x_split.shape[3]} (å„headã¯{d_k}æ¬¡å…ƒã‚’æ‹…å½“)\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ ç‰¹å¾´é‡ã®{d_model}æ¬¡å…ƒãŒã€{num_heads}å€‹ã®{d_k}æ¬¡å…ƒã«åˆ†å‰²ã•ã‚Œã¾ã—ãŸï¼\")\n",
    "print(f\"   å„headãŒç‰¹å¾´é‡ç©ºé–“ã®ç•°ãªã‚‹éƒ¨åˆ†ã‚’ä¸¦åˆ—ã«å‡¦ç†ã—ã¾ã™ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a400b392",
   "metadata": {},
   "source": [
    "#### çµåˆã®ä»•çµ„ã¿\n",
    "\n",
    "åˆ†å‰²ã¨é€†ã®æ“ä½œã§ã€å…¨headã®å‡ºåŠ›ã‚’çµåˆã—ã¾ã™:\n",
    "\n",
    "```python\n",
    "def combine_heads(self, x, batch_size):\n",
    "    \"\"\"\n",
    "    è¤‡æ•°ã®headã‚’çµåˆ\n",
    "    \n",
    "    Args:\n",
    "        x: shape [batch_size, num_heads, seq_len, d_k]\n",
    "    Returns:\n",
    "        shape [batch_size, seq_len, d_model]\n",
    "    \"\"\"\n",
    "    # [batch, num_heads, seq_len, d_k] -> [batch, seq_len, num_heads, d_k]\n",
    "    x = x.transpose(1, 2).contiguous()\n",
    "    # [batch, seq_len, num_heads, d_k] -> [batch, seq_len, d_model]\n",
    "    return x.view(batch_size, -1, self.d_model)\n",
    "```\n",
    "\n",
    "ã“ã®åˆ†å‰²ã¨çµåˆã«ã‚ˆã‚Šã€Multi-Head Attentionã¯è¤‡æ•°ã®è¡¨ç¾éƒ¨åˆ†ç©ºé–“ã‹ã‚‰ä¸¦åˆ—ã«æƒ…å ±ã‚’å­¦ç¿’ã§ãã¾ã™ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5475aa85",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q20: Multi-Head Attentionã§ç‰¹å¾´é‡ã‚’éƒ¨åˆ†ç©ºé–“ã«åˆ†å‰²ã™ã‚‹åˆ©ç‚¹\n",
    "\n",
    "**è³ªå•æ—¥**: 2025å¹´11æœˆ21æ—¥\n",
    "\n",
    "### è³ªå•\n",
    "ç‰¹å¾´é‡ã‚’éƒ¨åˆ†ç©ºé–“ã«ã™ã‚‹ã“ã¨ã§ã©ã‚“ãªè‰¯ã„æ‰€ãŒã‚ã‚‹ã®ã‹ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1932c1a",
   "metadata": {},
   "source": [
    "### å›ç­”\n",
    "\n",
    "Multi-Head Attentionã§ç‰¹å¾´é‡ã‚’éƒ¨åˆ†ç©ºé–“ã«åˆ†å‰²ã™ã‚‹ã“ã¨ã«ã¯ã€**è¤‡æ•°ã®é‡è¦ãªåˆ©ç‚¹**ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "\n",
    "#### 1. ç•°ãªã‚‹ç¨®é¡ã®é–¢ä¿‚æ€§ã‚’ä¸¦åˆ—ã«å­¦ç¿’ã§ãã‚‹\n",
    "\n",
    "å„headãŒç•°ãªã‚‹è¡¨ç¾éƒ¨åˆ†ç©ºé–“ã‚’æ‹…å½“ã™ã‚‹ã“ã¨ã§ã€**å¤šæ§˜ãªè¦–ç‚¹**ã‹ã‚‰æƒ…å ±ã‚’æ‰ãˆã‚‰ã‚Œã¾ã™ã€‚\n",
    "\n",
    "**ä¾‹**: è‡ªç„¶è¨€èªå‡¦ç†ã®å ´åˆ\n",
    "- **Head 1**: æ–‡æ³•çš„ãªä¾å­˜é–¢ä¿‚ï¼ˆä¸»èª-è¿°èªãªã©ï¼‰\n",
    "- **Head 2**: æ„å‘³çš„ãªé¡ä¼¼æ€§ï¼ˆåŒç¾©èªã€é–¢é€£èªï¼‰\n",
    "- **Head 3**: ä½ç½®çš„ãªè¿‘æ¥æ€§ï¼ˆéš£æ¥ã™ã‚‹å˜èªï¼‰\n",
    "- **Head 4**: é•·è·é›¢ä¾å­˜ï¼ˆæ–‡ã‚’ã¾ãŸãå‚ç…§é–¢ä¿‚ï¼‰\n",
    "- **Head 5-8**: ãã®ä»–ã®æŠ½è±¡çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³\n",
    "\n",
    "ã“ã‚Œã‚‰ã‚’**åŒæ™‚ã«**å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€å˜ä¸€ã®Attentionã§ã¯æ‰ãˆãã‚Œãªã„è¤‡é›‘ãªé–¢ä¿‚æ€§ã‚’è¡¨ç¾ã§ãã¾ã™ã€‚\n",
    "\n",
    "#### 2. è¨ˆç®—åŠ¹ç‡ãŒå¤‰ã‚ã‚‰ãªã„ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå¢—ãˆãªã„ï¼‰\n",
    "\n",
    "é‡è¦ãªç‚¹ã¨ã—ã¦ã€headã‚’å¢—ã‚„ã—ã¦ã‚‚**ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¯å¤‰ã‚ã‚Šã¾ã›ã‚“**:\n",
    "\n",
    "- **Single-Head** (d_model=512): \n",
    "  - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° = 4 Ã— (512 Ã— 512) = 1,048,576\n",
    "  \n",
    "- **Multi-Head** (d_model=512, num_heads=8, d_k=64):\n",
    "  - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° = 4 Ã— (512 Ã— 512) = 1,048,576\n",
    "\n",
    "å„headã®æ¬¡å…ƒãŒå°ã•ããªã‚‹ï¼ˆd_k = d_model / num_headsï¼‰ãŸã‚ã€ç·è¨ˆç®—é‡ã¯åŒã˜ã§ã™ã€‚\n",
    "\n",
    "#### 3. éå­¦ç¿’ã®ãƒªã‚¹ã‚¯ã‚’åˆ†æ•£\n",
    "\n",
    "1ã¤ã®å¤§ããªç©ºé–“ã§å­¦ç¿’ã™ã‚‹ã‚ˆã‚Šã‚‚ã€è¤‡æ•°ã®å°ã•ãªéƒ¨åˆ†ç©ºé–“ã§å­¦ç¿’ã™ã‚‹ã“ã¨ã§:\n",
    "\n",
    "- **ç‰¹åŒ–ã—ãŸå­¦ç¿’**: å„headãŒç‰¹å®šã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ç‰¹åŒ–\n",
    "- **å†—é•·æ€§ã®ç¢ºä¿**: ä¸€éƒ¨ã®headãŒå¤±æ•—ã—ã¦ã‚‚ä»–ãŒã‚«ãƒãƒ¼\n",
    "- **æ±åŒ–æ€§èƒ½ã®å‘ä¸Š**: ç•°ãªã‚‹è¦–ç‚¹ã®çµ„ã¿åˆã‚ã›ã§é ‘å¥ã«\n",
    "\n",
    "#### 4. è§£é‡ˆå¯èƒ½æ€§ã®å‘ä¸Š\n",
    "\n",
    "å„headã®å½¹å‰²ã‚’å¯è¦–åŒ–ãƒ»åˆ†æã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ãŒ**ä½•ã‚’å­¦ç¿’ã—ã¦ã„ã‚‹ã‹**ãŒç†è§£ã—ã‚„ã™ããªã‚Šã¾ã™ã€‚\n",
    "\n",
    "å®Ÿéš›ã®ç ”ç©¶ã§ã¯ã€å„headãŒä»¥ä¸‹ã®ã‚ˆã†ãªå½¹å‰²ã‚’å­¦ç¿’ã—ã¦ã„ã‚‹ã“ã¨ãŒè¦³å¯Ÿã•ã‚Œã¦ã„ã¾ã™:\n",
    "- ä½ç½®çš„æ³¨æ„ï¼ˆç›´å‰ãƒ»ç›´å¾Œã®å˜èªã«æ³¨ç›®ï¼‰\n",
    "- æ§‹æ–‡çš„æ³¨æ„ï¼ˆæ–‡æ³•æ§‹é€ ã«æ²¿ã£ãŸæ³¨ç›®ï¼‰\n",
    "- æ„å‘³çš„æ³¨æ„ï¼ˆæ„å‘³çš„ã«é–¢é€£ã™ã‚‹å˜èªã«æ³¨ç›®ï¼‰\n",
    "\n",
    "#### 5. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åŠ¹æœ\n",
    "\n",
    "è¤‡æ•°ã®headã‚’æŒã¤ã“ã¨ã¯ã€ã‚ã‚‹ç¨®ã®**ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’**ã¨ä¼¼ãŸåŠ¹æœãŒã‚ã‚Šã¾ã™:\n",
    "\n",
    "- ç•°ãªã‚‹è¦–ç‚¹ã‹ã‚‰ã®äºˆæ¸¬ã‚’çµ±åˆ\n",
    "- ã‚ˆã‚Šå®‰å®šã—ãŸè¡¨ç¾ã®ç²å¾—\n",
    "- å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã§ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®åˆ©ç‚¹ã‚’äº«å—"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43caff6",
   "metadata": {},
   "source": [
    "#### ã‚³ãƒ¼ãƒ‰ä¾‹: Single-Head vs Multi-Headã®æ¯”è¼ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5c6fcf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Single-Head Attention\n",
      "======================================================================\n",
      "ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 1,048,576\n",
      "å‡ºåŠ›å½¢çŠ¶: torch.Size([2, 10, 512])\n",
      "Attentioné‡ã¿å½¢çŠ¶: torch.Size([2, 1, 10, 10])\n",
      "  - 1ã¤ã®headã§å…¨ä½“ã‚’è¦‹ã‚‹\n",
      "  - d_k = 512 (= d_model)\n",
      "\n",
      "======================================================================\n",
      "Multi-Head Attention (8 heads)\n",
      "======================================================================\n",
      "ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 1,048,576\n",
      "å‡ºåŠ›å½¢çŠ¶: torch.Size([2, 10, 512])\n",
      "Attentioné‡ã¿å½¢çŠ¶: torch.Size([2, 8, 10, 10])\n",
      "  - 8ã¤ã®headã§ç•°ãªã‚‹è¦–ç‚¹ã‹ã‚‰è¦‹ã‚‹\n",
      "  - d_k = 64 (= d_model / 8)\n",
      "\n",
      "======================================================================\n",
      "æ¯”è¼ƒçµæœ\n",
      "======================================================================\n",
      "ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®å·®: 0 (ã»ã¼åŒã˜)\n",
      "å‡ºåŠ›å½¢çŠ¶: ä¸¡æ–¹ã¨ã‚‚ torch.Size([2, 10, 512]) (åŒã˜)\n",
      "\n",
      "ğŸ’¡ é‡è¦ãªé•ã„:\n",
      "  - Single-Head: 1ã¤ã®å¤§ããªç©ºé–“ã§é–¢ä¿‚æ€§ã‚’å­¦ç¿’\n",
      "  - Multi-Head: 8ã¤ã®ç‹¬ç«‹ã—ãŸéƒ¨åˆ†ç©ºé–“ã§ç•°ãªã‚‹é–¢ä¿‚æ€§ã‚’ä¸¦åˆ—å­¦ç¿’\n",
      "\n",
      "Multi-Headã®åˆ©ç‚¹:\n",
      "  âœ“ å¤šæ§˜ãªè¦–ç‚¹ã‹ã‚‰æƒ…å ±ã‚’æ‰ãˆã‚‹\n",
      "  âœ“ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¯åŒã˜ã¾ã¾è¡¨ç¾åŠ›ãŒå‘ä¸Š\n",
      "  âœ“ ã‚ˆã‚Šè¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’å¯èƒ½\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "import torch\n",
    "from attention import SelfAttention, MultiHeadAttention\n",
    "\n",
    "d_model = 512\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆå…¥åŠ›\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Single-Head Attention\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Single-Head (å®Ÿè³ªçš„ã«ã¯num_heads=1ã®Multi-Head)\n",
    "single_head = MultiHeadAttention(d_model, num_heads=1)\n",
    "output_single, attn_single = single_head(x, x, x)\n",
    "\n",
    "single_params = sum(p.numel() for p in single_head.parameters())\n",
    "print(f\"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {single_params:,}\")\n",
    "print(f\"å‡ºåŠ›å½¢çŠ¶: {output_single.shape}\")\n",
    "print(f\"Attentioné‡ã¿å½¢çŠ¶: {attn_single.shape}\")\n",
    "print(f\"  - 1ã¤ã®headã§å…¨ä½“ã‚’è¦‹ã‚‹\")\n",
    "print(f\"  - d_k = {single_head.d_k} (= d_model)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Multi-Head Attention (8 heads)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Multi-Head\n",
    "multi_head = MultiHeadAttention(d_model, num_heads=8)\n",
    "output_multi, attn_multi = multi_head(x, x, x)\n",
    "\n",
    "multi_params = sum(p.numel() for p in multi_head.parameters())\n",
    "print(f\"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {multi_params:,}\")\n",
    "print(f\"å‡ºåŠ›å½¢çŠ¶: {output_multi.shape}\")\n",
    "print(f\"Attentioné‡ã¿å½¢çŠ¶: {attn_multi.shape}\")\n",
    "print(f\"  - 8ã¤ã®headã§ç•°ãªã‚‹è¦–ç‚¹ã‹ã‚‰è¦‹ã‚‹\")\n",
    "print(f\"  - d_k = {multi_head.d_k} (= d_model / 8)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"æ¯”è¼ƒçµæœ\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®å·®: {multi_params - single_params:,} (ã»ã¼åŒã˜)\")\n",
    "print(f\"å‡ºåŠ›å½¢çŠ¶: ä¸¡æ–¹ã¨ã‚‚ {output_single.shape} (åŒã˜)\")\n",
    "print(f\"\\nğŸ’¡ é‡è¦ãªé•ã„:\")\n",
    "print(f\"  - Single-Head: 1ã¤ã®å¤§ããªç©ºé–“ã§é–¢ä¿‚æ€§ã‚’å­¦ç¿’\")\n",
    "print(f\"  - Multi-Head: 8ã¤ã®ç‹¬ç«‹ã—ãŸéƒ¨åˆ†ç©ºé–“ã§ç•°ãªã‚‹é–¢ä¿‚æ€§ã‚’ä¸¦åˆ—å­¦ç¿’\")\n",
    "print(f\"\\nMulti-Headã®åˆ©ç‚¹:\")\n",
    "print(f\"  âœ“ å¤šæ§˜ãªè¦–ç‚¹ã‹ã‚‰æƒ…å ±ã‚’æ‰ãˆã‚‹\")\n",
    "print(f\"  âœ“ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¯åŒã˜ã¾ã¾è¡¨ç¾åŠ›ãŒå‘ä¸Š\")\n",
    "print(f\"  âœ“ ã‚ˆã‚Šè¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’å¯èƒ½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a63a6e",
   "metadata": {},
   "source": [
    "#### è¦–è¦šçš„ãªç†è§£\n",
    "\n",
    "**Single-Head Attention**:\n",
    "```\n",
    "[512æ¬¡å…ƒã®ç‰¹å¾´é‡ç©ºé–“]\n",
    "    â†“\n",
    "  å…¨ä½“ã‚’ä¸€åº¦ã«è¦‹ã‚‹\n",
    "    â†“\n",
    "  1ã¤ã®è¦–ç‚¹\n",
    "```\n",
    "\n",
    "**Multi-Head Attention (8 heads)**:\n",
    "```\n",
    "[512æ¬¡å…ƒã®ç‰¹å¾´é‡ç©ºé–“]\n",
    "    â†“ åˆ†å‰²\n",
    "[64æ¬¡å…ƒ] [64æ¬¡å…ƒ] [64æ¬¡å…ƒ] [64æ¬¡å…ƒ] [64æ¬¡å…ƒ] [64æ¬¡å…ƒ] [64æ¬¡å…ƒ] [64æ¬¡å…ƒ]\n",
    "  Head1    Head2    Head3    Head4    Head5    Head6    Head7    Head8\n",
    "    â†“        â†“        â†“        â†“        â†“        â†“        â†“        â†“\n",
    "  æ–‡æ³•çš„   æ„å‘³çš„   ä½ç½®çš„   é•·è·é›¢   ...     ...     ...     ...\n",
    "  é–¢ä¿‚     é¡ä¼¼æ€§   è¿‘æ¥æ€§   ä¾å­˜\n",
    "    â†“\n",
    "  çµåˆã—ã¦çµ±åˆ\n",
    "    â†“\n",
    "[512æ¬¡å…ƒã®è±Šã‹ãªè¡¨ç¾]\n",
    "```\n",
    "\n",
    "#### ã¾ã¨ã‚\n",
    "\n",
    "éƒ¨åˆ†ç©ºé–“ã¸ã®åˆ†å‰²ã¯ã€**è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å¢—ã‚„ã•ãšã«è¡¨ç¾åŠ›ã‚’é«˜ã‚ã‚‹**å·§å¦™ãªè¨­è¨ˆã§ã™:\n",
    "\n",
    "1. **å¤šæ§˜æ€§**: ç•°ãªã‚‹è¦–ç‚¹ã‹ã‚‰æƒ…å ±ã‚’æ‰ãˆã‚‹\n",
    "2. **åŠ¹ç‡æ€§**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’å¢—ã‚„ã•ãªã„\n",
    "3. **å°‚é–€åŒ–**: å„headãŒç‰¹å®šã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ç‰¹åŒ–\n",
    "4. **çµ±åˆ**: è¤‡æ•°ã®è¦–ç‚¹ã‚’çµ„ã¿åˆã‚ã›ã¦è±Šã‹ãªè¡¨ç¾ã‚’ç²å¾—\n",
    "5. **è§£é‡ˆæ€§**: å„headã®å½¹å‰²ãŒå¯è¦–åŒ–ãƒ»åˆ†æå¯èƒ½\n",
    "\n",
    "ã“ã‚ŒãŒMulti-Head AttentionãŒ**Transformerã®æˆåŠŸã®éµ**ã¨ãªã£ãŸç†ç”±ã®1ã¤ã§ã™ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}