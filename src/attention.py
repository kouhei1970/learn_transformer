"""
Self-Attentionæ©Ÿæ§‹ã®å®Ÿè£…

Transformerã®æ ¸ã¨ãªã‚‹Self-Attentionï¼ˆè‡ªå·±æ³¨æ„æ©Ÿæ§‹ï¼‰ã‚’å®Ÿè£…ã—ã¾ã™ã€‚
Attentionæ©Ÿæ§‹ã¯ã€å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®å„è¦ç´ ãŒä»–ã®è¦ç´ ã¨ã©ã‚Œã ã‘é–¢é€£ã—ã¦ã„ã‚‹ã‹ã‚’è¨ˆç®—ã—ã€
ãã‚Œã«åŸºã¥ã„ã¦é‡ã¿ä»˜ãå’Œã‚’å–ã‚‹ä»•çµ„ã¿ã§ã™ã€‚

å‚è€ƒ: "Attention is All You Need" (https://arxiv.org/abs/1706.03762)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math


class ScaledDotProductAttention(nn.Module):
    """
    Scaled Dot-Product Attention
    
    Attentionã®åŸºæœ¬ã¨ãªã‚‹æ©Ÿæ§‹ã§ã€ä»¥ä¸‹ã®è¨ˆç®—ã‚’è¡Œã„ã¾ã™:
    Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V
    
    ã“ã“ã§:
    - Q (Query): ã€Œä½•ã‚’æ¢ã—ã¦ã„ã‚‹ã‹ã€ã‚’è¡¨ã™ãƒ™ã‚¯ãƒˆãƒ«
    - K (Key): ã€Œä½•ã‚’æŒã£ã¦ã„ã‚‹ã‹ã€ã‚’è¡¨ã™ãƒ™ã‚¯ãƒˆãƒ«
    - V (Value): å®Ÿéš›ã®æƒ…å ±ã‚’æŒã¤ãƒ™ã‚¯ãƒˆãƒ«
    - d_k: Keyã®æ¬¡å…ƒæ•°ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°ã¨ã—ã¦ä½¿ç”¨ï¼‰
    """
    
    def __init__(self, dropout=0.1):
        """
        Args:
            dropout (float): Attentioné‡ã¿ã«é©ç”¨ã™ã‚‹ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
        """
        super(ScaledDotProductAttention, self).__init__()
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, query, key, value, mask=None):
        """
        Scaled Dot-Product Attentionã®é †ä¼æ’­
        
        Args:
            query (torch.Tensor): Queryè¡Œåˆ— [batch_size, seq_len, d_k]
            key (torch.Tensor): Keyè¡Œåˆ— [batch_size, seq_len, d_k]
            value (torch.Tensor): Valueè¡Œåˆ— [batch_size, seq_len, d_v]
            mask (torch.Tensor, optional): ãƒã‚¹ã‚¯ [batch_size, seq_len, seq_len]
                                          Trueã®ä½ç½®ã¯-infã«ãƒã‚¹ã‚¯ã•ã‚Œã‚‹
        
        Returns:
            output (torch.Tensor): Attentioné©ç”¨å¾Œã®å‡ºåŠ› [batch_size, seq_len, d_v]
            attention_weights (torch.Tensor): Attentioné‡ã¿ [batch_size, seq_len, seq_len]
        """
        # Keyã®æ¬¡å…ƒæ•°ã‚’å–å¾—ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã«ä½¿ç”¨ï¼‰
        d_k = query.size(-1)
        
        # Step 1: Qã¨Kã®å†…ç©ã‚’è¨ˆç®—
        # query: [batch, seq_len, d_k]
        # key.transpose(-2, -1): [batch, d_k, seq_len]
        # scores: [batch, seq_len, seq_len]
        scores = torch.matmul(query, key.transpose(-2, -1))
        
        # Step 2: sqrt(d_k)ã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        # ã“ã‚Œã«ã‚ˆã‚Šã€å†…ç©ãŒå¤§ãããªã‚Šã™ãã¦softmaxã®å‹¾é…ãŒæ¶ˆå¤±ã™ã‚‹ã®ã‚’é˜²ã
        scores = scores / math.sqrt(d_k)
        
        # Step 3: ãƒã‚¹ã‚¯ã®é©ç”¨ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        # ä¾‹: æœªæ¥ã®æƒ…å ±ã‚’è¦‹ãªã„ã‚ˆã†ã«ã™ã‚‹ï¼ˆãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ï¼‰ã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç„¡è¦–ã™ã‚‹ã€ãªã©
        if mask is not None:
            # ãƒã‚¹ã‚¯ã•ã‚ŒãŸä½ç½®ã‚’-infã«ã™ã‚‹ã“ã¨ã§ã€softmaxå¾Œã«0ã«ãªã‚‹
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # Step 4: Softmaxã§æ­£è¦åŒ–ã—ã¦Attentioné‡ã¿ã‚’è¨ˆç®—
        # å„è¡Œï¼ˆå„Queryä½ç½®ï¼‰ã«ã¤ã„ã¦ã€å…¨ã¦ã®Keyä½ç½®ã¸ã®é‡ã¿ã®å’ŒãŒ1ã«ãªã‚‹
        attention_weights = F.softmax(scores, dim=-1)
        
        # Step 5: ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆé©ç”¨ï¼ˆå­¦ç¿’æ™‚ã®æ­£å‰‡åŒ–ï¼‰
        attention_weights = self.dropout(attention_weights)
        
        # Step 6: Attentioné‡ã¿ã¨Valueã®é‡ã¿ä»˜ãå’Œã‚’è¨ˆç®—
        # attention_weights: [batch, seq_len, seq_len]
        # value: [batch, seq_len, d_v]
        # output: [batch, seq_len, d_v]
        output = torch.matmul(attention_weights, value)
        
        return output, attention_weights


class SelfAttention(nn.Module):
    """
    Self-Attention Layer
    
    å…¥åŠ›ã‹ã‚‰ç·šå½¢å¤‰æ›ã§Query, Key, Valueã‚’ç”Ÿæˆã—ã€
    Scaled Dot-Product Attentionã‚’é©ç”¨ã—ã¾ã™ã€‚
    
    ã€ŒSelfã€ã¨ã¯ã€åŒã˜å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‹ã‚‰ Q, K, V ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚
    ã“ã‚Œã«ã‚ˆã‚Šã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã®å„è¦ç´ ãŒä»–ã®å…¨è¦ç´ ã¨ã®é–¢ä¿‚æ€§ã‚’å­¦ç¿’ã§ãã¾ã™ã€‚
    """
    
    def __init__(self, d_model, dropout=0.1):
        """
        Args:
            d_model (int): ãƒ¢ãƒ‡ãƒ«ã®æ¬¡å…ƒæ•°ï¼ˆå…¥åŠ›ãƒ»å‡ºåŠ›ã®æ¬¡å…ƒï¼‰
            dropout (float): ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
        """
        super(SelfAttention, self).__init__()
        
        self.d_model = d_model
        
        # å…¥åŠ›ã‹ã‚‰Query, Key, Valueã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®ç·šå½¢å¤‰æ›
        # å„å¤‰æ›ã¯ç‹¬ç«‹ã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤
        # bias=False: æœ€æ–°ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã«å¾“ã„ã€ãƒã‚¤ã‚¢ã‚¹ãªã—ã§å®Ÿè£…
        # (Attentionã¯ç›¸å¯¾çš„ãªé–¢ä¿‚æ€§ã‚’æ‰ãˆã‚‹ãŸã‚ã€çµ¶å¯¾çš„ãªã‚ªãƒ•ã‚»ãƒƒãƒˆã¯ä¸è¦)
        self.query_linear = nn.Linear(d_model, d_model, bias=False)
        self.key_linear = nn.Linear(d_model, d_model, bias=False)
        self.value_linear = nn.Linear(d_model, d_model, bias=False)
        
        # Scaled Dot-Product Attention
        self.attention = ScaledDotProductAttention(dropout)
        
        # Attentioné©ç”¨å¾Œã®å‡ºåŠ›ã‚’å¤‰æ›ã™ã‚‹ç·šå½¢å±¤
        self.output_linear = nn.Linear(d_model, d_model)
        
    def forward(self, x, mask=None):
        """
        Self-Attentionã®é †ä¼æ’­
        
        Args:
            x (torch.Tensor): å…¥åŠ› [batch_size, seq_len, d_model]
            mask (torch.Tensor, optional): Attentionãƒã‚¹ã‚¯
        
        Returns:
            output (torch.Tensor): å‡ºåŠ› [batch_size, seq_len, d_model]
            attention_weights (torch.Tensor): Attentioné‡ã¿ [batch_size, seq_len, seq_len]
        """
        # Step 1: å…¥åŠ›ã‹ã‚‰ Q, K, V ã‚’ç·šå½¢å¤‰æ›ã§ç”Ÿæˆ
        # åŒã˜å…¥åŠ› x ã‹ã‚‰ç”Ÿæˆã™ã‚‹ãŸã‚ã€ŒSelfã€Attention
        query = self.query_linear(x)  # [batch, seq_len, d_model]
        key = self.key_linear(x)      # [batch, seq_len, d_model]
        value = self.value_linear(x)  # [batch, seq_len, d_model]
        
        # Step 2: Scaled Dot-Product Attentionã‚’é©ç”¨
        attention_output, attention_weights = self.attention(
            query, key, value, mask
        )
        
        # Step 3: å‡ºåŠ›ã‚’ç·šå½¢å¤‰æ›
        output = self.output_linear(attention_output)
        
        return output, attention_weights


class MultiHeadAttention(nn.Module):
    """
    Multi-Head Attention
    
    è¤‡æ•°ã®Attention headã‚’ä¸¦åˆ—ã«å®Ÿè¡Œã™ã‚‹ã“ã¨ã§ã€
    ç•°ãªã‚‹è¡¨ç¾éƒ¨åˆ†ç©ºé–“ã‹ã‚‰æƒ…å ±ã‚’æ‰ãˆã‚‹ä»•çµ„ã¿ã§ã™ã€‚++
    
    å„headã¯ç‹¬ç«‹ã—ãŸQ, K, Vå¤‰æ›ã‚’æŒã¡ã€ç•°ãªã‚‹ç¨®é¡ã®é–¢ä¿‚æ€§ã‚’å­¦ç¿’ã—ã¾ã™ã€‚
    å…¨headã®å‡ºåŠ›ã‚’çµåˆã—ã€æœ€çµ‚çš„ãªç·šå½¢å¤‰æ›ã‚’é©ç”¨ã—ã¾ã™ã€‚
    
    æ•°å¼:
        MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O
        head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
    
    ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¯ Single-Head ã¨åŒã˜:
        4 Ã— (d_model Ã— d_model)
    """
    
    def __init__(self, d_model, num_heads, dropout=0.1):
        """
        Args:
            d_model (int): ãƒ¢ãƒ‡ãƒ«ã®æ¬¡å…ƒæ•°ï¼ˆå…¥åŠ›ãƒ»å‡ºåŠ›ã®æ¬¡å…ƒï¼‰
            num_heads (int): Attention headã®æ•°
            dropout (float): ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
        """
        super(MultiHeadAttention, self).__init__()
        
        # d_modelãŒnum_headsã§å‰²ã‚Šåˆ‡ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
        assert d_model % num_heads == 0, \
            f"d_model ({d_model}) must be divisible by num_heads ({num_heads})"
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads  # å„headã®æ¬¡å…ƒ
        
        # Q, K, Vç”¨ã®ç·šå½¢å¤‰æ›å±¤ï¼ˆå…¨headã‚’ã¾ã¨ã‚ã¦å‡¦ç†ï¼‰
        # bias=False: æœ€æ–°ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã«å¾“ã†
        self.W_q = nn.Linear(d_model, d_model, bias=False)
        self.W_k = nn.Linear(d_model, d_model, bias=False)
        self.W_v = nn.Linear(d_model, d_model, bias=False)
        
        # Scaled Dot-Product Attention
        self.attention = ScaledDotProductAttention(dropout)
        
        # å‡ºåŠ›ç”¨ã®ç·šå½¢å¤‰æ›å±¤
        self.W_o = nn.Linear(d_model, d_model, bias=False)
        
    def split_heads(self, x, batch_size):
        """
        å…¥åŠ›ã‚’è¤‡æ•°ã®headã«åˆ†å‰²
        
        Args:
            x: shape [batch_size, seq_len, d_model]
        Returns:
            shape [batch_size, num_heads, seq_len, d_k]
        """
        # [batch, seq_len, d_model] -> [batch, seq_len, num_heads, d_k]
        x = x.view(batch_size, -1, self.num_heads, self.d_k)
        # [batch, seq_len, num_heads, d_k] -> [batch, num_heads, seq_len, d_k]
        return x.transpose(1, 2)
    
    def combine_heads(self, x, batch_size):
        """
        è¤‡æ•°ã®headã‚’çµåˆ
        
        Args:
            x: shape [batch_size, num_heads, seq_len, d_k]
        Returns:
            shape [batch_size, seq_len, d_model]
        """
        # [batch, num_heads, seq_len, d_k] -> [batch, seq_len, num_heads, d_k]
        x = x.transpose(1, 2).contiguous()
        # [batch, seq_len, num_heads, d_k] -> [batch, seq_len, d_model]
        return x.view(batch_size, -1, self.d_model)
    
    def forward(self, query, key, value, mask=None):
        """
        Multi-Head Attentionã®é †ä¼æ’­
        
        Args:
            query (torch.Tensor): Query [batch_size, seq_len, d_model]
            key (torch.Tensor): Key [batch_size, seq_len, d_model]
            value (torch.Tensor): Value [batch_size, seq_len, d_model]
            mask (torch.Tensor, optional): Attentionãƒã‚¹ã‚¯
        
        Returns:
            output (torch.Tensor): å‡ºåŠ› [batch_size, seq_len, d_model]
            attention_weights (torch.Tensor): Attentioné‡ã¿ 
                                             [batch_size, num_heads, seq_len, seq_len]
        """
        batch_size = query.size(0)
        
        # 1. ç·šå½¢å¤‰æ›: [batch, seq_len, d_model]
        Q = self.W_q(query)
        K = self.W_k(key)
        V = self.W_v(value)
        
        # 2. è¤‡æ•°headã«åˆ†å‰²: [batch, num_heads, seq_len, d_k]
        Q = self.split_heads(Q, batch_size)
        K = self.split_heads(K, batch_size)
        V = self.split_heads(V, batch_size)
        
        # 3. ãƒã‚¹ã‚¯ã®æ¬¡å…ƒã‚’èª¿æ•´ï¼ˆå¿…è¦ãªå ´åˆï¼‰
        if mask is not None:
            # ãƒã‚¹ã‚¯ã«headæ¬¡å…ƒã‚’è¿½åŠ : [batch, 1, seq_len, seq_len]
            mask = mask.unsqueeze(1)
        
        # 4. å„headã§Scaled Dot-Product Attentionã‚’å®Ÿè¡Œ
        # Q, K, V: [batch, num_heads, seq_len, d_k]
        # attention_output: [batch, num_heads, seq_len, d_k]
        # attention_weights: [batch, num_heads, seq_len, seq_len]
        attention_output, attention_weights = self.attention(Q, K, V, mask)
        
        # 5. å…¨headã‚’çµåˆ: [batch, seq_len, d_model]
        attention_output = self.combine_heads(attention_output, batch_size)
        
        # 6. å‡ºåŠ›ç·šå½¢å¤‰æ›
        output = self.W_o(attention_output)
        
        return output, attention_weights


class PositionalEncoding(nn.Module):
    """
    ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° (Positional Encoding)
    
    Transformerã¯é †åºã‚’è€ƒæ…®ã—ãªã„ãŸã‚ã€ç³»åˆ—ã®ä½ç½®æƒ…å ±ã‚’æ˜ç¤ºçš„ã«ä¸ãˆã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
    Sin/Cosé–¢æ•°ã‚’ä½¿ã£ãŸå›ºå®šçš„ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ã€å„ä½ç½®ã«å›ºæœ‰ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    
    æ•°å¼:
        PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
        PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
    
    Args:
        d_model (int): ãƒ¢ãƒ‡ãƒ«ã®æ¬¡å…ƒæ•°
        max_len (int): ã‚µãƒãƒ¼ãƒˆã™ã‚‹æœ€å¤§ç³»åˆ—é•·
        dropout (float): ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
    """
    
    def __init__(self, d_model, max_len=5000, dropout=0.1):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        
        # ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¡Œåˆ—ã‚’ä½œæˆ
        # shape: [max_len, d_model]
        pe = torch.zeros(max_len, d_model)
        
        # ä½ç½®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: [0, 1, 2, ..., max_len-1]
        # shape: [max_len, 1]
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        
        # åˆ†æ¯ã®è¨ˆç®—: 10000^(2i/d_model)
        # exp(log(10000) * (-2i/d_model)) = 10000^(-2i/d_model)
        # shape: [d_model/2]
        div_term = torch.exp(
            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)
        )
        
        # å¶æ•°æ¬¡å…ƒã«ã¯sinã€å¥‡æ•°æ¬¡å…ƒã«ã¯cos
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        # ãƒãƒƒãƒæ¬¡å…ƒã‚’è¿½åŠ : [1, max_len, d_model]
        pe = pe.unsqueeze(0)
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã¯ãªããƒãƒƒãƒ•ã‚¡ã¨ã—ã¦ç™»éŒ²ï¼ˆå­¦ç¿’ã—ãªã„å®šæ•°ï¼‰
        # ãƒ¢ãƒ‡ãƒ«ã®çŠ¶æ…‹ã¨ã—ã¦ä¿å­˜ã•ã‚Œã‚‹ãŒã€å‹¾é…è¨ˆç®—ã®å¯¾è±¡å¤–
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        """
        å…¥åŠ›ã«ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’åŠ ç®—
        
        Args:
            x (torch.Tensor): å…¥åŠ› [batch_size, seq_len, d_model]
        
        Returns:
            torch.Tensor: ä½ç½®æƒ…å ±ãŒåŠ ç®—ã•ã‚ŒãŸå…¥åŠ› [batch_size, seq_len, d_model]
        """
        # å…¥åŠ›ã®ç³»åˆ—é•·åˆ†ã ã‘ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å–å¾—ã—ã¦åŠ ç®—
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)


# ãƒ†ã‚¹ãƒˆç”¨ã®ã‚·ãƒ³ãƒ—ãƒ«ãªä¾‹
if __name__ == "__main__":
    # ãƒ‡ãƒã‚¤ã‚¹ã®è¨­å®šï¼ˆmacOS GPUå¯¾å¿œï¼‰
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"Using device: CUDA GPU")
    elif torch.backends.mps.is_available():
        device = torch.device("mps")
        print(f"Using device: Apple Silicon GPU (MPS)")
    else:
        device = torch.device("cpu")
        print(f"Using device: CPU")
    
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    batch_size = 2    # ãƒãƒƒãƒã‚µã‚¤ã‚º
    seq_len = 5       # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
    d_model = 64      # ãƒ¢ãƒ‡ãƒ«ã®æ¬¡å…ƒæ•°
    
    print("=" * 70)
    print("Self-Attention Test")
    print("=" * 70)
    
    # ãƒ©ãƒ³ãƒ€ãƒ ãªå…¥åŠ›ã‚’ç”Ÿæˆ
    x = torch.randn(batch_size, seq_len, d_model).to(device)
    print(f"Input shape: {x.shape}")
    
    # Self-Attentionãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    self_attention = SelfAttention(d_model).to(device)
    
    # é †ä¼æ’­
    output, attention_weights = self_attention(x)
    
    print(f"Output shape: {output.shape}")
    print(f"Attention weights shape: {attention_weights.shape}")
    print(f"\nAttention weights (first sample, first 3x3):")
    print(attention_weights[0, :3, :3].detach().cpu().numpy())
    print(f"\nAttention weights sum per row (should be ~1.0):")
    print(attention_weights[0].sum(dim=-1).detach().cpu().numpy())
    
    print("\n" + "=" * 70)
    print("Multi-Head Attention Test")
    print("=" * 70)
    
    num_heads = 8
    
    # Multi-Head Attentionãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    multi_head_attention = MultiHeadAttention(d_model, num_heads).to(device)
    
    # é †ä¼æ’­
    output_mh, attention_weights_mh = multi_head_attention(x, x, x)
    
    print(f"Input shape: {x.shape}")
    print(f"Output shape: {output_mh.shape}")
    print(f"Attention weights shape: {attention_weights_mh.shape}")
    print(f"Number of heads: {num_heads}")
    print(f"d_k per head: {multi_head_attention.d_k}")
    
    print("\n" + "=" * 70)
    print("Parameter Comparison")
    print("=" * 70)
    
    self_attn_params = sum(p.numel() for p in self_attention.parameters())
    multi_head_params = sum(p.numel() for p in multi_head_attention.parameters())
    
    print(f"Self-Attention parameters: {self_attn_params:,}")
    print(f"Multi-Head Attention parameters: {multi_head_params:,}")
    print(f"\nğŸ’¡ Same number of parameters, but Multi-Head learns")
    print(f"   {num_heads} different representation subspaces!")
    
    print("\n" + "=" * 70)
    print("Positional Encoding Test")
    print("=" * 70)
    
    # Position Encodingã®ãƒ†ã‚¹ãƒˆ
    pos_encoding = PositionalEncoding(d_model, max_len=100, dropout=0.0)
    
    # åŒã˜åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å…¨ä½ç½®ã§ä½¿ç”¨
    same_embedding = torch.randn(1, d_model).to(device)
    repeated_embeddings = same_embedding.repeat(1, seq_len, 1)
    
    print(f"Input (same embedding repeated): {repeated_embeddings.shape}")
    print(f"Position 0 and Position 4 difference (before PE): "
          f"{(repeated_embeddings[0, 0] - repeated_embeddings[0, 4]).abs().sum().item():.6f}")
    
    # Position Encodingã‚’é©ç”¨
    with_pos = pos_encoding(repeated_embeddings.cpu()).to(device)
    
    print(f"\nAfter Positional Encoding: {with_pos.shape}")
    print(f"Position 0 and Position 4 difference (after PE): "
          f"{(with_pos[0, 0] - with_pos[0, 4]).abs().sum().item():.6f}")
    
    print(f"\nğŸ’¡ Position Encoding ã«ã‚ˆã‚Šã€åŒã˜åŸ‹ã‚è¾¼ã¿ã§ã‚‚")
    print(f"   ä½ç½®ãŒç•°ãªã‚Œã°ç•°ãªã‚‹è¡¨ç¾ã«ãªã‚Šã¾ã™ï¼")
