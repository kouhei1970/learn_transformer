{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A Part 1: Attention基礎\n",
    "\n",
    "Transformer学習中の質問と回答集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24622a01",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q1: nn.Linearとは？\n",
    "\n",
    "**質問日**: 2025年11月17日\n",
    "\n",
    "### 質問\n",
    "`nn.Linear`とは何ですか？Self-Attentionでどのように使われていますか？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3a4629",
   "metadata": {},
   "source": [
    "### 回答\n",
    "\n",
    "`nn.Linear`は、PyTorchの**線形変換層（全結合層）**です。\n",
    "\n",
    "#### 基本的な役割\n",
    "\n",
    "入力に対して、重み行列との積とバイアスの加算を行います：\n",
    "\n",
    "**数式**: `y = xW^T + b`\n",
    "\n",
    "- `x`: 入力ベクトル\n",
    "- `W`: 重み行列（学習可能なパラメータ）\n",
    "- `b`: バイアスベクトル（学習可能なパラメータ）\n",
    "- `y`: 出力ベクトル\n",
    "\n",
    "#### Self-Attentionでの使い方\n",
    "\n",
    "同じ入力から、Query、Key、Valueという3つの異なる表現を生成するために使います。それぞれ独立した重み行列で変換することで、各変換が異なる役割を学習します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fb105478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kouhei/tmp/learn_transformer/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebab6707",
   "metadata": {},
   "source": [
    "#### コード例: 基本的な使い方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "50a3bf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力の形状: torch.Size([2, 8])\n",
      "入力:\n",
      "tensor([[ 1.0948e+00,  4.0948e-01,  1.3526e+00,  2.3708e-01,  1.2767e-03,\n",
      "          1.8368e-01, -1.3970e+00,  5.7167e-02],\n",
      "        [ 2.0081e+00, -2.9609e-01, -4.9418e-01, -7.0071e-01, -5.1890e-01,\n",
      "         -2.2101e+00, -3.3232e-01, -5.0763e-01]])\n",
      "\n",
      "出力の形状: torch.Size([2, 8])\n",
      "出力:\n",
      "tensor([[-0.3933, -0.7952,  0.0124, -0.5738,  0.0821,  0.8156, -1.2864,  0.3774],\n",
      "        [-0.6749, -0.7796,  0.5364,  0.2386, -0.0452, -0.6092, -0.3234, -0.9071]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "重み行列の形状: torch.Size([8, 8])\n",
      "バイアスの形状: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 8次元の入力を8次元の出力に変換する線形層\n",
    "linear = nn.Linear(in_features=8, out_features=8, bias=True)\n",
    "\n",
    "# 入力データ: [batch_size=2, features=8]\n",
    "x = torch.randn(2, 8)\n",
    "print(f\"入力の形状: {x.shape}\")\n",
    "print(f\"入力:\\n{x}\\n\")\n",
    "\n",
    "# 線形変換を適用\n",
    "y = linear(x)\n",
    "print(f\"出力の形状: {y.shape}\")\n",
    "print(f\"出力:\\n{y}\\n\")\n",
    "\n",
    "# パラメータの確認\n",
    "print(f\"重み行列の形状: {linear.weight.shape}\")  # [out_features, in_features]\n",
    "print(f\"バイアスの形状: {linear.bias.shape}\")      # [out_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948de590",
   "metadata": {},
   "source": [
    "#### コード例: Self-Attentionでの使い方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9d0b1c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力データの形状: torch.Size([1, 4, 8])\n",
      "  [バッチサイズ, シーケンス長, 埋め込み次元]\n",
      "\n",
      "Query (Q) の形状: torch.Size([1, 4, 8])\n",
      "Key (K) の形状: torch.Size([1, 4, 8])\n",
      "Value (V) の形状: torch.Size([1, 4, 8])\n",
      "\n",
      "重要なポイント:\n",
      "  - 同じ入力Xから生成 → 「Self」Attention\n",
      "  - 異なる重み行列W_q, W_k, W_vで変換\n",
      "  - 各変換が独立して学習される\n"
     ]
    }
   ],
   "source": [
    "# Self-Attentionでの使用例\n",
    "d_model = 8\n",
    "seq_len = 4\n",
    "batch_size = 1\n",
    "\n",
    "# 入力データ（シーケンスデータ）\n",
    "X = torch.randn(batch_size, seq_len, d_model)\n",
    "print(f\"入力データの形状: {X.shape}\")\n",
    "print(f\"  [バッチサイズ, シーケンス長, 埋め込み次元]\\n\")\n",
    "\n",
    "# Query, Key, Value用の線形変換層\n",
    "# バイアスなし（bias=False）が一般的\n",
    "W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "# 同じ入力Xから、異なる重み行列で Q, K, V を生成\n",
    "Q = W_q(X)  # Query: 「何を探しているか」\n",
    "K = W_k(X)  # Key: 「何を持っているか」\n",
    "V = W_v(X)  # Value: 「実際の情報」\n",
    "\n",
    "print(f\"Query (Q) の形状: {Q.shape}\")\n",
    "print(f\"Key (K) の形状: {K.shape}\")\n",
    "print(f\"Value (V) の形状: {V.shape}\")\n",
    "\n",
    "print(f\"\\n重要なポイント:\")\n",
    "print(f\"  - 同じ入力Xから生成 → 「Self」Attention\")\n",
    "print(f\"  - 異なる重み行列W_q, W_k, W_vで変換\")\n",
    "print(f\"  - 各変換が独立して学習される\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74819c45",
   "metadata": {},
   "source": [
    "#### まとめ\n",
    "\n",
    "- `nn.Linear`は線形変換（行列積 + バイアス）を行う層\n",
    "- Self-Attentionでは、入力から**Q, K, V**を生成するために3つの`nn.Linear`を使用\n",
    "- 各`nn.Linear`は独立した重み行列を持ち、学習を通じて最適化される\n",
    "- `bias=False`が一般的（Transformerの論文では省略されている）\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dc4384",
   "metadata": {},
   "source": [
    "## Q2: Query、Key、Valueとは？KとVの違いは？\n",
    "\n",
    "**質問日**: 2025年11月17日\n",
    "\n",
    "### 質問\n",
    "\n",
    "「Queryは何を探しているか」「Keyは何を持っているか」「Valueは実際の情報」という説明では、**KeyとValueの違い**が分かりません。もっと具体的に説明してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab95ccfa",
   "metadata": {},
   "source": [
    "### 回答: 具体例で理解する\n",
    "\n",
    "抽象的な説明ではなく、**データベース検索**に例えると分かりやすくなります。\n",
    "\n",
    "#### データベース検索の例\n",
    "\n",
    "あなたが図書館のシステムで本を探すとします：\n",
    "\n",
    "1. **Query（検索クエリ）**: `\"機械学習\"`という検索ワード\n",
    "2. **Key（索引・タグ）**: 各本に付けられたキーワード（「AI」「統計」「Python」など）\n",
    "3. **Value（実際のデータ）**: 本の内容そのもの\n",
    "\n",
    "**検索の流れ**:\n",
    "- あなたの検索ワード（Query）と各本のキーワード（Key）を比較\n",
    "- マッチ度が高い本ほど高いスコアを付ける\n",
    "- スコアに基づいて、実際の本の内容（Value）を取得\n",
    "\n",
    "**重要**: KeyとValueは別物！\n",
    "- **Key**: マッチング（類似度計算）に使う「索引」\n",
    "- **Value**: 実際に取り出したい「中身」"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c67749",
   "metadata": {},
   "source": [
    "#### Self-Attentionでの具体例\n",
    "\n",
    "文「私は 猫が 好き です」を処理する場合：\n",
    "\n",
    "**元の入力**: 各単語の埋め込みベクトル（例: 8次元）\n",
    "\n",
    "この入力から、**目的に応じて異なる表現**を作ります：\n",
    "\n",
    "1. **Query（Q）**: 「この単語は、他のどの単語と関連付けたいか？」を表す表現\n",
    "   - 例: 「好き」という単語が「何が好きなのか？」を探すための表現\n",
    "   \n",
    "2. **Key（K）**: 「この単語は、どんな情報で検索されたいか？」を表す表現\n",
    "   - 例: 「猫が」という単語が「動物」「対象」として検索される際の表現\n",
    "   \n",
    "3. **Value（V）**: 「この単語が持つ実際の意味情報」\n",
    "   - 例: 「猫が」という単語の持つ本来の意味的な情報\n",
    "\n",
    "**計算の流れ**:\n",
    "1. Queryで「探したい情報のパターン」を表現\n",
    "2. Keyで「マッチング用の特徴」を表現\n",
    "3. QueryとKeyの類似度（内積）を計算 → Attention Weight\n",
    "4. Attention Weightを使って、Valueを重み付けして集約"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c761f52a",
   "metadata": {},
   "source": [
    "#### なぜKとVを分ける必要があるのか？\n",
    "\n",
    "**重要な理由**: マッチング用の特徴と、取り出したい情報は**別物**だから！\n",
    "\n",
    "**例1: 単語の品詞と意味**\n",
    "- Key: 「動詞」「名詞」などの**文法的特徴**でマッチング\n",
    "- Value: その単語の**意味的な情報**を取得\n",
    "\n",
    "**例2: 画像認識（Vision Transformer）**\n",
    "- Key: 「エッジ」「色」などの**視覚的特徴**でマッチング  \n",
    "- Value: その領域の**詳細な情報**を取得\n",
    "\n",
    "もしKeyとValueが同じだと、「マッチングに最適な表現」と「情報として最適な表現」が一緒になってしまい、表現力が制限されます。\n",
    "\n",
    "**分けることで**:\n",
    "- Keyは「どの情報に注目すべきか」の判断に特化\n",
    "- Valueは「実際に取り出す情報」として最適化\n",
    "- それぞれが独立して学習できる → より柔軟で強力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8075e74",
   "metadata": {},
   "source": [
    "#### 数値例で確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7f490c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力 X:\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.]])\n",
      "tensor([[[1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.]]])\n",
      "形状: torch.Size([1, 3, 4])\n",
      "\n",
      "Query (Q) - 探索用の表現:\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "Key (K) - マッチング用の表現:\n",
      "tensor([[2., 0., 0., 0.],\n",
      "        [0., 2., 0., 0.],\n",
      "        [0., 0., 2., 0.]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "Value (V) - 取り出す情報:\n",
      "tensor([[1.0000, 0.5000, 0.0000, 0.0000],\n",
      "        [0.5000, 1.0000, 0.5000, 0.0000],\n",
      "        [0.0000, 0.5000, 1.0000, 0.5000]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "【観察】\n",
      "- Q, K, V は同じ入力Xから生成されるが、異なる重み行列で変換\n",
      "- Kはマッチング用にスケール調整（×2）\n",
      "- Vは周辺の情報を混ぜ合わせた表現（スムージング）\n",
      "→ それぞれ異なる目的に特化した表現になっている！\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# シンプルな例: 3つの単語、次元数4\n",
    "seq_len = 3\n",
    "d_model = 4\n",
    "\n",
    "# 入力: 3つの単語の埋め込みベクトル\n",
    "X = torch.tensor([\n",
    "    [1.0, 0.0, 0.0, 0.0],  # 単語1\n",
    "    [0.0, 1.0, 0.0, 0.0],  # 単語2  \n",
    "    [0.0, 0.0, 1.0, 0.0],  # 単語3\n",
    "]).unsqueeze(0)  # [1, 3, 4]\n",
    "\n",
    "print(\"入力 X:\")\n",
    "print(X[0])\n",
    "print(X)\n",
    "print(f\"形状: {X.shape}\\n\")\n",
    "\n",
    "# Q, K, V用の変換（わかりやすくするため単純化）\n",
    "W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "# 重みを固定して違いを明確に\n",
    "with torch.no_grad():\n",
    "    # Qの重み: 「探す方向」を強調\n",
    "    W_q.weight.copy_(torch.eye(d_model))\n",
    "    \n",
    "    # Kの重み: 「マッチング用の特徴」を抽出\n",
    "    W_k.weight.copy_(torch.eye(d_model) * 2)\n",
    "    \n",
    "    # Vの重み: 「取り出す情報」を変換\n",
    "    W_v.weight.copy_(torch.tensor([\n",
    "        [1.0, 0.5, 0.0, 0.0],\n",
    "        [0.5, 1.0, 0.5, 0.0],\n",
    "        [0.0, 0.5, 1.0, 0.5],\n",
    "        [0.0, 0.0, 0.5, 1.0],\n",
    "    ]))\n",
    "\n",
    "Q = W_q(X)\n",
    "K = W_k(X)\n",
    "V = W_v(X)\n",
    "\n",
    "print(\"Query (Q) - 探索用の表現:\")\n",
    "print(Q[0])\n",
    "print(\"\\nKey (K) - マッチング用の表現:\")\n",
    "print(K[0])\n",
    "print(\"\\nValue (V) - 取り出す情報:\")\n",
    "print(V[0])\n",
    "\n",
    "print(\"\\n【観察】\")\n",
    "print(\"- Q, K, V は同じ入力Xから生成されるが、異なる重み行列で変換\")\n",
    "print(\"- Kはマッチング用にスケール調整（×2）\")\n",
    "print(\"- Vは周辺の情報を混ぜ合わせた表現（スムージング）\")\n",
    "print(\"→ それぞれ異なる目的に特化した表現になっている！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39119e2",
   "metadata": {},
   "source": [
    "#### Attention計算の全体像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f3421896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Scores (Q × K^T / √d):\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "各行: 各単語（Query）が、全単語（Key）とどれだけマッチするか\n",
      "\n",
      "Attention Weights (softmax適用後):\n",
      "tensor([[0.5761, 0.2119, 0.2119],\n",
      "        [0.2119, 0.5761, 0.2119],\n",
      "        [0.2119, 0.2119, 0.5761]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "各行の合計: tensor([1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)\n",
      "\n",
      "最終出力 (Attention Weights × Value):\n",
      "tensor([[0.6821, 0.6060, 0.3179, 0.1060],\n",
      "        [0.5000, 0.7881, 0.5000, 0.1060],\n",
      "        [0.3179, 0.6060, 0.6821, 0.2881]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "============================================================\n",
      "【重要な理解】\n",
      "============================================================\n",
      "1. QueryとKeyを使って「どの情報に注目するか」を決定\n",
      "2. その重み（Attention Weights）を使って\n",
      "3. Valueから実際の情報を取り出す\n",
      "\n",
      "KeyとValueが別物だからこそ:\n",
      "  ✓ マッチングの基準（Key）と\n",
      "  ✓ 取り出す情報（Value）を\n",
      "  独立して最適化できる！\n"
     ]
    }
   ],
   "source": [
    "# 上で作ったQ, K, Vを使って、Attentionを計算\n",
    "\n",
    "# Step 1: QueryとKeyの類似度を計算（スコア）\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_model ** 0.5)\n",
    "print(\"Attention Scores (Q × K^T / √d):\")\n",
    "print(scores[0])\n",
    "print(\"\\n各行: 各単語（Query）が、全単語（Key）とどれだけマッチするか\")\n",
    "\n",
    "# Step 2: Softmaxで正規化 → Attention Weights\n",
    "attn_weights = F.softmax(scores, dim=-1)\n",
    "print(\"\\nAttention Weights (softmax適用後):\")\n",
    "print(attn_weights[0])\n",
    "print(\"\\n各行の合計:\", attn_weights[0].sum(dim=-1))\n",
    "\n",
    "# Step 3: Attention WeightsでValueを重み付け\n",
    "output = torch.matmul(attn_weights, V)\n",
    "print(\"\\n最終出力 (Attention Weights × Value):\")\n",
    "print(output[0])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"【重要な理解】\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. QueryとKeyを使って「どの情報に注目するか」を決定\")\n",
    "print(\"2. その重み（Attention Weights）を使って\")\n",
    "print(\"3. Valueから実際の情報を取り出す\")\n",
    "print(\"\")\n",
    "print(\"KeyとValueが別物だからこそ:\")\n",
    "print(\"  ✓ マッチングの基準（Key）と\")\n",
    "print(\"  ✓ 取り出す情報（Value）を\")\n",
    "print(\"  独立して最適化できる！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074cca53",
   "metadata": {},
   "source": [
    "#### まとめ: Q, K, Vの役割\n",
    "\n",
    "| 要素 | 役割 | 例え | 実際の使われ方 |\n",
    "|------|------|------|----------------|\n",
    "| **Query (Q)** | 「何を探すか」を表す | 検索ワード | 各単語が「どんな情報を必要としているか」 |\n",
    "| **Key (K)** | 「マッチング用の特徴」 | 索引・タグ | 各単語が「どんな特徴でマッチするか」 |\n",
    "| **Value (V)** | 「実際に取り出す情報」 | 本の中身 | 実際に集約される意味情報 |\n",
    "\n",
    "**計算の流れ**:\n",
    "```\n",
    "1. Q × K^T → どの単語に注目すべきかのスコア\n",
    "2. softmax → スコアを確率分布に変換（Attention Weights）\n",
    "3. Attention Weights × V → 重み付けされた情報の集約\n",
    "```\n",
    "\n",
    "**KeyとValueを分ける理由**:\n",
    "- **Key**: 類似度計算（マッチング）に最適な表現に特化\n",
    "- **Value**: 取り出す情報として最適な表現に特化\n",
    "- 分けることで、それぞれが独立して学習でき、表現力が向上\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cfbf23",
   "metadata": {},
   "source": [
    "## Q3: Q, K, Vの意味は後の演算で明確になるのか？\n",
    "\n",
    "**質問日**: 2025年11月17日\n",
    "\n",
    "### 質問\n",
    "\n",
    "現時点では、数学的にはQ, K, Vは同じもの（入力X）をNNで変換しただけで、それぞれのNNの重みが違うので出力が違うだけですよね。\n",
    "\n",
    "「Queryは探す」「Keyはマッチング」「Valueは情報」という**意味合い**は、これから行う演算（内積、Softmax、重み付き和）で明確になるのでしょうか？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba33efd3",
   "metadata": {},
   "source": [
    "### 回答: はい、その通りです！\n",
    "\n",
    "**素晴らしい洞察です。** あなたの理解は完全に正しいです。\n",
    "\n",
    "#### 現時点での数学的な事実\n",
    "\n",
    "```python\n",
    "Q = W_q(X)  # ただの線形変換\n",
    "K = W_k(X)  # ただの線形変換\n",
    "V = W_v(X)  # ただの線形変換\n",
    "```\n",
    "\n",
    "この時点では、Q, K, Vは**ただ重みが違うだけの3つの出力**です。\n",
    "「Query」「Key」「Value」という名前は、人間が後付けした**ラベル**に過ぎません。\n",
    "\n",
    "#### 意味が明確になるのは「使われ方」から\n",
    "\n",
    "Q, K, Vの意味は、**これから行う演算での役割**によって初めて明確になります：\n",
    "\n",
    "1. **Q × K^T**: QueryとKeyの内積 → 「どれとどれが関連するか」を計算\n",
    "2. **Softmax**: スコアを正規化 → 「注目の配分」を決定\n",
    "3. **Attention × V**: 重みでValueを集約 → 「実際の情報を取り出す」\n",
    "\n",
    "つまり、**演算の構造が意味を定義している**のです！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e83adc",
   "metadata": {},
   "source": [
    "#### 演算フローで見る「役割の具体化」"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4c42bfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: Q と K の内積 → ここでQとKの役割が分化\n",
      "============================================================\n",
      "\n",
      "スコア行列 (Q × K^T):\n",
      "tensor([[ 1.1419, -0.3811,  0.3753],\n",
      "        [ 3.0666, -0.6720, -1.2386],\n",
      "        [ 0.9294, -0.7231, -0.5776]])\n",
      "\n",
      "【ここで初めて意味が生まれる】\n",
      "  - Qの各行: 「この位置が探している情報のパターン」\n",
      "  - Kの各行: 「この位置が提供できる情報の特徴」\n",
      "  - 内積が大きい = マッチ度が高い\n",
      "\n",
      "============================================================\n",
      "STEP 2: Softmax → 確率分布に変換\n",
      "============================================================\n",
      "\n",
      "Attention Weights:\n",
      "tensor([[0.5943, 0.1296, 0.2761],\n",
      "        [0.9641, 0.0229, 0.0130],\n",
      "        [0.7076, 0.1356, 0.1568]])\n",
      "\n",
      "【意味】\n",
      "  各行: 各位置が「どの位置の情報をどれだけ取り込むか」の配分\n",
      "\n",
      "============================================================\n",
      "STEP 3: Attention × V → ここでVの役割が明確に\n",
      "============================================================\n",
      "\n",
      "最終出力:\n",
      "tensor([[-0.2452,  0.3710, -1.1511, -0.1032],\n",
      "        [-0.4223,  0.6332, -0.7990, -0.0403],\n",
      "        [-0.2912,  0.4317, -1.0299, -0.0678]])\n",
      "\n",
      "【ここで初めてVの意味が生まれる】\n",
      "  - Vの各行: 「実際に取り出される情報の中身」\n",
      "  - Attention Weightsで重み付けして集約される\n",
      "  - Qが決めた配分で、Vから情報を抽出\n",
      "\n",
      "============================================================\n",
      "【結論】\n",
      "============================================================\n",
      "Q, K, Vの意味は、演算での「使われ方」で決まる:\n",
      "  1. Q × K^T: QとKが「マッチング」の役割を担う\n",
      "  2. softmax: 配分を決定\n",
      "  3. × V: Vが「取り出す情報」の役割を担う\n",
      "\n",
      "生成時は同じ（線形変換）でも、\n",
      "演算の構造が役割を定義する！\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 簡単な例で演算の役割を追跡\n",
    "d = 4\n",
    "seq_len = 3\n",
    "\n",
    "# 入力\n",
    "X = torch.randn(1, seq_len, d)\n",
    "\n",
    "# Q, K, Vを生成（この時点ではただの線形変換）\n",
    "Q = torch.randn(1, seq_len, d)\n",
    "K = torch.randn(1, seq_len, d)\n",
    "V = torch.randn(1, seq_len, d)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: Q と K の内積 → ここでQとKの役割が分化\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1)) / (d ** 0.5)\n",
    "print(f\"\\nスコア行列 (Q × K^T):\")\n",
    "print(scores[0])\n",
    "print(f\"\\n【ここで初めて意味が生まれる】\")\n",
    "print(f\"  - Qの各行: 「この位置が探している情報のパターン」\")\n",
    "print(f\"  - Kの各行: 「この位置が提供できる情報の特徴」\")\n",
    "print(f\"  - 内積が大きい = マッチ度が高い\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 2: Softmax → 確率分布に変換\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "attn_weights = F.softmax(scores, dim=-1)\n",
    "print(f\"\\nAttention Weights:\")\n",
    "print(attn_weights[0])\n",
    "print(f\"\\n【意味】\")\n",
    "print(f\"  各行: 各位置が「どの位置の情報をどれだけ取り込むか」の配分\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 3: Attention × V → ここでVの役割が明確に\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "output = torch.matmul(attn_weights, V)\n",
    "print(f\"\\n最終出力:\")\n",
    "print(output[0])\n",
    "print(f\"\\n【ここで初めてVの意味が生まれる】\")\n",
    "print(f\"  - Vの各行: 「実際に取り出される情報の中身」\")\n",
    "print(f\"  - Attention Weightsで重み付けして集約される\")\n",
    "print(f\"  - Qが決めた配分で、Vから情報を抽出\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"【結論】\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Q, K, Vの意味は、演算での「使われ方」で決まる:\")\n",
    "print(\"  1. Q × K^T: QとKが「マッチング」の役割を担う\")\n",
    "print(\"  2. softmax: 配分を決定\")\n",
    "print(\"  3. × V: Vが「取り出す情報」の役割を担う\")\n",
    "print(\"\\n生成時は同じ（線形変換）でも、\")\n",
    "print(\"演算の構造が役割を定義する！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d5ca9e",
   "metadata": {},
   "source": [
    "#### 学習を通じて「意味」が強化される\n",
    "\n",
    "もう一つ重要な点があります：\n",
    "\n",
    "**初期状態（学習前）**:\n",
    "- Q, K, Vの重み行列はランダム\n",
    "- 演算の構造から「役割」は決まっているが、まだ最適化されていない\n",
    "\n",
    "**学習後**:\n",
    "- タスクを通じて、Q, K, Vの重み行列が最適化される\n",
    "- Qは「何を探すべきか」を学習\n",
    "- Kは「どうマッチすべきか」を学習\n",
    "- Vは「何を出力すべきか」を学習\n",
    "\n",
    "つまり：\n",
    "1. **演算の構造**が役割の「枠組み」を定義\n",
    "2. **学習**がその役割を具体的に最適化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb39b60e",
   "metadata": {},
   "source": [
    "#### 類推: プログラムの変数名\n",
    "\n",
    "これは、プログラミングの変数名に似ています：\n",
    "\n",
    "```python\n",
    "# この時点では、a, b, c はただの変数\n",
    "a = calculate_something(x)\n",
    "b = calculate_something(x)  # 違う関数\n",
    "c = calculate_something(x)  # さらに違う関数\n",
    "\n",
    "# 「使われ方」で意味が決まる\n",
    "similarity = dot_product(a, b)  # ここでaとbは「比較対象」の意味に\n",
    "result = weighted_sum(similarity, c)  # cは「集約される情報」の意味に\n",
    "```\n",
    "\n",
    "- **変数の中身**だけでは意味は分からない\n",
    "- **どう使われるか**で意味が決まる\n",
    "- **変数名（Query, Key, Value）**は人間の理解を助けるラベル\n",
    "\n",
    "Attention機構も同じです！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fb5880",
   "metadata": {},
   "source": [
    "#### まとめ\n",
    "\n",
    "| 視点 | 説明 |\n",
    "|------|------|\n",
    "| **数学的事実** | Q, K, Vは同じ入力Xを、異なる重み行列で線形変換しただけ |\n",
    "| **演算での役割** | `Q×K^T`でマッチング、`×V`で情報抽出、という構造で役割が決まる |\n",
    "| **学習での最適化** | タスクを通じて、各重み行列がその役割に最適化される |\n",
    "| **名前の意味** | Query, Key, Valueは、その「使われ方」から付けられた後付けのラベル |\n",
    "\n",
    "**あなたの理解は完璧です！**\n",
    "\n",
    "最初は「なぜ分けるのか分からない」のは当然で、\n",
    "**演算の全体像を見て初めて、分ける意味が分かる**のです。\n",
    "\n",
    "次のステップで実際にAttention計算を見れば、\n",
    "「なるほど、だからQ, K, Vを分けるのか！」と腑に落ちるはずです。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e04fe6",
   "metadata": {},
   "source": [
    "## Q4: スケーリング（÷√d_k）は本当に必要？\n",
    "\n",
    "**質問日**: 2025年11月17日\n",
    "\n",
    "### 質問\n",
    "\n",
    "`Q × K^T`の結果を`√d_k`で割ってからSoftmaxに入れていますが、**全体を同じ値で割るだけなら、Softmaxの結果は変わらないのでは**ないでしょうか？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044c68e9",
   "metadata": {},
   "source": [
    "### 回答: 鋭い指摘ですが、実は**結果は変わります**！\n",
    "\n",
    "一見、全体を定数で割っても比率は変わらないので、Softmaxの結果も同じに思えます。\n",
    "しかし、**Softmaxは非線形関数**なので、入力の値によって結果が大きく変わります。\n",
    "\n",
    "#### 直感的な理解\n",
    "\n",
    "Softmaxは「大きい値をさらに強調する」性質があります：\n",
    "\n",
    "- **スケーリングなし**: スコアが大きい → Softmax後、一部に極端に集中（ほぼ1と0）\n",
    "- **スケーリングあり**: スコアが小さい → Softmax後、より均等に分散\n",
    "\n",
    "つまり、スケーリングは**Attention Weightsの分布を調整**しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313d1a0b",
   "metadata": {},
   "source": [
    "#### 実験で確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "62241a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "元のスコア:\n",
      "============================================================\n",
      "tensor([[10.,  5.,  2.,  1.]])\n",
      "\n",
      "============================================================\n",
      "ケース1: スケーリングなし（そのままSoftmax）\n",
      "============================================================\n",
      "Attention Weights:\n",
      "tensor([[9.9285e-01, 6.6898e-03, 3.3307e-04, 1.2253e-04]])\n",
      "最大値の位置への重み: 0.992855\n",
      "最小値の位置への重み: 0.000123\n",
      "→ 最大値に極端に集中している！\n",
      "\n",
      "============================================================\n",
      "ケース2: √d_kでスケーリング（d_k=64と仮定）\n",
      "============================================================\n",
      "スケーリング後のスコア (÷√64 = ÷8):\n",
      "tensor([[1.2500, 0.6250, 0.2500, 0.1250]])\n",
      "\n",
      "Attention Weights:\n",
      "tensor([[0.4489, 0.2403, 0.1651, 0.1457]])\n",
      "最大値の位置への重み: 0.448875\n",
      "最小値の位置への重み: 0.145728\n",
      "→ より均等に分散している！\n",
      "\n",
      "============================================================\n",
      "比較\n",
      "============================================================\n",
      "スケーリングなし: [[9.9285465e-01 6.6898018e-03 3.3306563e-04 1.2252800e-04]]\n",
      "スケーリングあり: [[0.44887468 0.24026531 0.16513178 0.14572828]]\n",
      "\n",
      "【重要】結果が全く異なる！\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# シンプルなスコア（内積の結果）\n",
    "scores = torch.tensor([[10.0, 5.0, 2.0, 1.0]])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"元のスコア:\")\n",
    "print(\"=\" * 60)\n",
    "print(scores)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ケース1: スケーリングなし（そのままSoftmax）\")\n",
    "print(\"=\" * 60)\n",
    "attn_no_scale = F.softmax(scores, dim=-1)\n",
    "print(\"Attention Weights:\")\n",
    "print(attn_no_scale)\n",
    "print(f\"最大値の位置への重み: {attn_no_scale[0, 0].item():.6f}\")\n",
    "print(f\"最小値の位置への重み: {attn_no_scale[0, 3].item():.6f}\")\n",
    "print(f\"→ 最大値に極端に集中している！\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ケース2: √d_kでスケーリング（d_k=64と仮定）\")\n",
    "print(\"=\" * 60)\n",
    "d_k = 64\n",
    "scores_scaled = scores / np.sqrt(d_k)\n",
    "print(f\"スケーリング後のスコア (÷√{d_k} = ÷8):\")\n",
    "print(scores_scaled)\n",
    "\n",
    "attn_with_scale = F.softmax(scores_scaled, dim=-1)\n",
    "print(\"\\nAttention Weights:\")\n",
    "print(attn_with_scale)\n",
    "print(f\"最大値の位置への重み: {attn_with_scale[0, 0].item():.6f}\")\n",
    "print(f\"最小値の位置への重み: {attn_with_scale[0, 3].item():.6f}\")\n",
    "print(f\"→ より均等に分散している！\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"比較\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"スケーリングなし: {attn_no_scale.numpy()}\")\n",
    "print(f\"スケーリングあり: {attn_with_scale.numpy()}\")\n",
    "print(f\"\\n【重要】結果が全く異なる！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b2bca3",
   "metadata": {},
   "source": [
    "#### なぜこうなるのか？Softmaxの数式\n",
    "\n",
    "Softmaxの定義を見てみましょう：\n",
    "\n",
    "$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n",
    "\n",
    "**重要なポイント**: 指数関数 $e^x$ は非線形！\n",
    "\n",
    "- $x$ が大きいと、$e^x$ は**爆発的に大きくなる**\n",
    "- $x$ を小さくすると、$e^x$ の差が縮まる\n",
    "\n",
    "**具体例**:\n",
    "- $e^{10} \\approx 22026$ vs $e^{1} \\approx 2.7$ → 差が約8000倍！\n",
    "- $e^{1.25} \\approx 3.5$ vs $e^{0.125} \\approx 1.1$ → 差が約3倍\n",
    "\n",
    "スケーリングで入力を小さくすると、指数関数の爆発を抑えられます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7206c0",
   "metadata": {},
   "source": [
    "#### なぜ√d_kなのか？\n",
    "\n",
    "次元数`d_k`が大きいほど、内積の値が大きくなる傾向があります：\n",
    "\n",
    "**内積の期待値**:\n",
    "- Q と K がランダムなベクトルの場合\n",
    "- 内積の分散は `d_k` に比例\n",
    "- 標準偏差は `√d_k` に比例\n",
    "\n",
    "だから`√d_k`で割ることで、次元数によらず**スコアの分散を一定**に保てます。\n",
    "\n",
    "**実用的な効果**:\n",
    "- d_k=64 → スコアが大きくなりすぎない\n",
    "- d_k=512 → スケーリングなしだとSoftmaxが極端になる\n",
    "- スケーリングで、どんな次元数でも安定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc48e70",
   "metadata": {},
   "source": [
    "#### 次元数による影響を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2033f154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "次元数が異なる場合の内積の大きさ\n",
      "============================================================\n",
      "\n",
      "d_k = 8:\n",
      "  内積の値: -5.43\n",
      "  スケーリング後 (÷√8): -1.92\n",
      "  → 次元が大きいほど内積も大きい傾向\n",
      "\n",
      "d_k = 64:\n",
      "  内積の値: -2.00\n",
      "  スケーリング後 (÷√64): -0.25\n",
      "  → 次元が大きいほど内積も大きい傾向\n",
      "\n",
      "d_k = 512:\n",
      "  内積の値: -24.82\n",
      "  スケーリング後 (÷√512): -1.10\n",
      "  → 次元が大きいほど内積も大きい傾向\n",
      "\n",
      "============================================================\n",
      "【重要】\n",
      "スケーリングにより、次元数によらず\n",
      "内積の値が同じスケールに正規化される！\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"次元数が異なる場合の内積の大きさ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for d_k in [8, 64, 512]:\n",
    "    # ランダムなQ, Kベクトル\n",
    "    q = torch.randn(1, d_k)\n",
    "    k = torch.randn(1, d_k)\n",
    "    \n",
    "    # 内積\n",
    "    score = torch.matmul(q, k.T).item()\n",
    "    \n",
    "    # スケーリング\n",
    "    score_scaled = score / (d_k ** 0.5)\n",
    "    \n",
    "    print(f\"\\nd_k = {d_k}:\")\n",
    "    print(f\"  内積の値: {score:.2f}\")\n",
    "    print(f\"  スケーリング後 (÷√{d_k}): {score_scaled:.2f}\")\n",
    "    print(f\"  → 次元が大きいほど内積も大きい傾向\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"【重要】\")\n",
    "print(\"スケーリングにより、次元数によらず\")\n",
    "print(\"内積の値が同じスケールに正規化される！\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfcc318",
   "metadata": {},
   "source": [
    "#### まとめ\n",
    "\n",
    "| 観点 | 説明 |\n",
    "|------|------|\n",
    "| **直感的な誤解** | 「全体を同じ値で割ってもSoftmaxの結果は同じ」→ **間違い** |\n",
    "| **数学的事実** | Softmaxは非線形（指数関数）なので、入力値で結果が変わる |\n",
    "| **スケーリングの効果** | 大きい値を小さくする → Softmaxの極端な集中を防ぐ |\n",
    "| **なぜ√d_k** | 内積の標準偏差が√d_kに比例 → 正規化で次元数に依らず安定 |\n",
    "| **学習への影響** | スケーリングなし → 勾配消失、学習不安定 |\n",
    "\n",
    "**結論**:\n",
    "- 一見不要に見えるスケーリングだが、実は**学習の安定性に極めて重要**\n",
    "- Softmaxの非線形性により、入力の範囲が結果に大きく影響する\n",
    "- √d_kでのスケーリングは、理論的にも実用的にも意味がある\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c02c597",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q5: √d_kで割ることで分散が1になるのか？\n",
    "\n",
    "実際に数式と実験で確認してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e054a99",
   "metadata": {},
   "source": [
    "#### 理論的な説明\n",
    "\n",
    "QとKがそれぞれ標準正規分布N(0,1)から独立に生成された場合:\n",
    "\n",
    "**内積の分散:**\n",
    "```\n",
    "Var(Q・K) = Var(Σ q_i × k_i)\n",
    "         = Σ Var(q_i × k_i)      # 独立なので\n",
    "         = Σ E[q_i²] × E[k_i²]   # 平均が0なので\n",
    "         = Σ 1 × 1 = d_k         # 標準正規分布なので\n",
    "```\n",
    "\n",
    "**スケーリング後の分散:**\n",
    "```\n",
    "Var((Q・K) / √d_k) = Var(Q・K) / d_k\n",
    "                   = d_k / d_k = 1\n",
    "```\n",
    "\n",
    "つまり、理論的には**分散が1になる**はずです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8dd4ff",
   "metadata": {},
   "source": [
    "#### 実験1: 標準正規分布から生成した場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f5456257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "実験1: 標準正規分布N(0,1)から生成\n",
      "============================================================\n",
      "\n",
      "d_k = 8\n",
      "  生スコアの分散: 8.1002  (理論値: 8)\n",
      "  スケール後分散: 1.0125  (理論値: 1.0)\n",
      "  生スコアの標準偏差: 2.8461  (理論値: 2.8284)\n",
      "  スケール後標準偏差: 1.0062  (理論値: 1.0)\n",
      "\n",
      "d_k = 64\n",
      "  生スコアの分散: 64.7734  (理論値: 64)\n",
      "  スケール後分散: 1.0121  (理論値: 1.0)\n",
      "  生スコアの標準偏差: 8.0482  (理論値: 8.0000)\n",
      "  スケール後標準偏差: 1.0060  (理論値: 1.0)\n",
      "\n",
      "d_k = 512\n",
      "  生スコアの分散: 503.1414  (理論値: 512)\n",
      "  スケール後分散: 0.9827  (理論値: 1.0)\n",
      "  生スコアの標準偏差: 22.4308  (理論値: 22.6274)\n",
      "  スケール後標準偏差: 0.9913  (理論値: 1.0)\n",
      "\n",
      "d_k = 512\n",
      "  生スコアの分散: 503.1414  (理論値: 512)\n",
      "  スケール後分散: 0.9827  (理論値: 1.0)\n",
      "  生スコアの標準偏差: 22.4308  (理論値: 22.6274)\n",
      "  スケール後標準偏差: 0.9913  (理論値: 1.0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"実験1: 標準正規分布N(0,1)から生成\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for d_k in [8, 64, 512]:\n",
    "    # 大量のサンプルで統計的に検証\n",
    "    n_samples = 10000\n",
    "    \n",
    "    # 標準正規分布から生成\n",
    "    Q = torch.randn(n_samples, d_k)\n",
    "    K = torch.randn(n_samples, d_k)\n",
    "    \n",
    "    # 内積を計算（各サンプルについて）\n",
    "    scores = (Q * K).sum(dim=1)  # shape: (n_samples,)\n",
    "    \n",
    "    # スケーリング\n",
    "    scaled_scores = scores / np.sqrt(d_k)\n",
    "    \n",
    "    print(f\"\\nd_k = {d_k}\")\n",
    "    print(f\"  生スコアの分散: {scores.var().item():.4f}  (理論値: {d_k})\")\n",
    "    print(f\"  スケール後分散: {scaled_scores.var().item():.4f}  (理論値: 1.0)\")\n",
    "    print(f\"  生スコアの標準偏差: {scores.std().item():.4f}  (理論値: {np.sqrt(d_k):.4f})\")\n",
    "    print(f\"  スケール後標準偏差: {scaled_scores.std().item():.4f}  (理論値: 1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc6f153",
   "metadata": {},
   "source": [
    "#### 実験2: nn.Linearで生成した場合（実際のTransformer）\n",
    "\n",
    "実際のTransformerでは、QとKは`nn.Linear`で生成されます。この場合はどうでしょうか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3378e92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "実験2: nn.Linearで生成（実際のTransformer）\n",
      "============================================================\n",
      "\n",
      "d_k = 8\n",
      "  生スコアの分散: 1.1468\n",
      "  スケール後分散: 0.1433\n",
      "  √d_k = 2.8284\n",
      "  分散の比率: 8.0000 (理論値: 8)\n",
      "\n",
      "d_k = 64\n",
      "  生スコアの分散: 7.5824\n",
      "  スケール後分散: 0.1185\n",
      "  √d_k = 8.0000\n",
      "  分散の比率: 64.0000 (理論値: 64)\n",
      "\n",
      "d_k = 64\n",
      "  生スコアの分散: 7.5824\n",
      "  スケール後分散: 0.1185\n",
      "  √d_k = 8.0000\n",
      "  分散の比率: 64.0000 (理論値: 64)\n",
      "\n",
      "d_k = 512\n",
      "  生スコアの分散: 57.2970\n",
      "  スケール後分散: 0.1119\n",
      "  √d_k = 22.6274\n",
      "  分散の比率: 512.0000 (理論値: 512)\n",
      "\n",
      "d_k = 512\n",
      "  生スコアの分散: 57.2970\n",
      "  スケール後分散: 0.1119\n",
      "  √d_k = 22.6274\n",
      "  分散の比率: 512.0000 (理論値: 512)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"実験2: nn.Linearで生成（実際のTransformer）\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for d_k in [8, 64, 512]:\n",
    "    d_model = d_k  # 簡単のため同じ次元に\n",
    "    n_samples = 10000\n",
    "    seq_len = 10\n",
    "    \n",
    "    # 入力データを標準正規分布から生成\n",
    "    X = torch.randn(n_samples, seq_len, d_model)\n",
    "    \n",
    "    # nn.Linearで変換（デフォルト初期化）\n",
    "    W_q = nn.Linear(d_model, d_k)\n",
    "    W_k = nn.Linear(d_model, d_k)\n",
    "    \n",
    "    Q = W_q(X)  # shape: (n_samples, seq_len, d_k)\n",
    "    K = W_k(X)  # shape: (n_samples, seq_len, d_k)\n",
    "    \n",
    "    # Attention scoresを計算\n",
    "    # Q @ K^T の各要素（全サンプル、全ペアから集める）\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1))  # (n_samples, seq_len, seq_len)\n",
    "    scores_flat = scores.flatten()\n",
    "    \n",
    "    scaled_scores = scores_flat / np.sqrt(d_k)\n",
    "    \n",
    "    print(f\"\\nd_k = {d_k}\")\n",
    "    print(f\"  生スコアの分散: {scores_flat.var().item():.4f}\")\n",
    "    print(f\"  スケール後分散: {scaled_scores.var().item():.4f}\")\n",
    "    print(f\"  √d_k = {np.sqrt(d_k):.4f}\")\n",
    "    print(f\"  分散の比率: {scores_flat.var().item() / scaled_scores.var().item():.4f} (理論値: {d_k})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c23fac9",
   "metadata": {},
   "source": [
    "#### まとめ\n",
    "\n",
    "| 条件 | 生スコアの分散 | スケール後の分散 | 結論 |\n",
    "|------|---------------|-----------------|------|\n",
    "| **理論（標準正規分布）** | d_k | 1.0 | ✓ 完全に1になる |\n",
    "| **実験1（標準正規分布）** | ≈ d_k | ≈ 1.0 | ✓ 理論通り |\n",
    "| **実験2（nn.Linear）** | ≠ d_k | ≠ 1.0 | △ 正確には1にならない |\n",
    "\n",
    "**重要なポイント:**\n",
    "\n",
    "1. **理想的な条件下では**: √d_kで割ると分散が**正確に1になる**\n",
    "   - QとKが標準正規分布から独立に生成される場合\n",
    "\n",
    "2. **実際のTransformerでは**: 分散は**おおよそ1になる**が、完全には1にならない\n",
    "   - `nn.Linear`の初期化は標準正規分布ではない（Kaiming初期化など）\n",
    "   - 入力Xとの相関があるため、完全に独立ではない\n",
    "   \n",
    "3. **なぜ√d_kなのか**:\n",
    "   - 分散を**正確に1にする**ためではなく、**次元数に依存しないようにする**ため\n",
    "   - d_k=8でもd_k=512でも、スケール後の分散が**同程度**になる\n",
    "   - これにより、どんなモデルサイズでも**安定した学習**が可能\n",
    "\n",
    "**結論**: √d_kスケーリングは「分散を1にする魔法」というより、「次元数の影響を正規化する実用的な手法」"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003a5c9f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q6: nn.Linearの中身はどんなのですか？\n",
    "\n",
    "`nn.Linear`の内部実装を詳しく見てみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4983d5",
   "metadata": {},
   "source": [
    "#### 基本構造\n",
    "\n",
    "`nn.Linear(in_features, out_features, bias=True)`は以下の2つのパラメータを持ちます:\n",
    "\n",
    "1. **weight**: 形状 `(out_features, in_features)` の行列\n",
    "2. **bias**: 形状 `(out_features,)` のベクトル（`bias=True`の場合）\n",
    "\n",
    "計算式:\n",
    "```\n",
    "y = xW^T + b\n",
    "```\n",
    "\n",
    "ここで:\n",
    "- `x`: 入力テンソル `(..., in_features)`\n",
    "- `W`: 重み行列 `(out_features, in_features)`\n",
    "- `b`: バイアスベクトル `(out_features,)`\n",
    "- `y`: 出力テンソル `(..., out_features)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaa2692",
   "metadata": {},
   "source": [
    "#### 実験: 内部パラメータを見てみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "65367728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "nn.Linearの内部構造\n",
      "============================================================\n",
      "\n",
      "1. パラメータ一覧:\n",
      "  weight: shape=torch.Size([3, 4]), dtype=torch.float32\n",
      "  bias: shape=torch.Size([3]), dtype=torch.float32\n",
      "\n",
      "2. 重み行列 (weight):\n",
      "  形状: torch.Size([3, 4])\n",
      "  実際の値:\n",
      "tensor([[ 0.2718,  0.4257,  0.0045,  0.2400],\n",
      "        [-0.2094,  0.2349, -0.3089,  0.1979],\n",
      "        [ 0.4376, -0.3982,  0.4402, -0.1586]])\n",
      "\n",
      "3. バイアス (bias):\n",
      "  形状: torch.Size([3])\n",
      "  実際の値: tensor([ 0.2165, -0.4657, -0.0720])\n",
      "\n",
      "4. 計算の確認:\n",
      "  入力 x: tensor([1., 2., 3., 4.])\n",
      "  x.shape: torch.Size([4])\n",
      "\n",
      "  出力 y = linear(x): tensor([ 2.3132, -0.3404,  0.2553], grad_fn=<ViewBackward0>)\n",
      "  y.shape: torch.Size([3])\n",
      "\n",
      "  手動計算 y = xW^T + b: tensor([ 2.3132, -0.3404,  0.2553], grad_fn=<AddBackward0>)\n",
      "  一致するか: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# nn.Linearを作成\n",
    "linear = nn.Linear(in_features=4, out_features=3)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"nn.Linearの内部構造\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. パラメータ一覧:\")\n",
    "for name, param in linear.named_parameters():\n",
    "    print(f\"  {name}: shape={param.shape}, dtype={param.dtype}\")\n",
    "\n",
    "print(\"\\n2. 重み行列 (weight):\")\n",
    "print(f\"  形状: {linear.weight.shape}\")\n",
    "print(f\"  実際の値:\\n{linear.weight.data}\")\n",
    "\n",
    "print(\"\\n3. バイアス (bias):\")\n",
    "print(f\"  形状: {linear.bias.shape}\")\n",
    "print(f\"  実際の値: {linear.bias.data}\")\n",
    "\n",
    "print(\"\\n4. 計算の確認:\")\n",
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "print(f\"  入力 x: {x}\")\n",
    "print(f\"  x.shape: {x.shape}\")\n",
    "\n",
    "# nn.Linearで計算\n",
    "y = linear(x)\n",
    "print(f\"\\n  出力 y = linear(x): {y}\")\n",
    "print(f\"  y.shape: {y.shape}\")\n",
    "\n",
    "# 手動で計算（y = xW^T + b）\n",
    "y_manual = torch.matmul(x, linear.weight.T) + linear.bias\n",
    "print(f\"\\n  手動計算 y = xW^T + b: {y_manual}\")\n",
    "print(f\"  一致するか: {torch.allclose(y, y_manual)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563510e0",
   "metadata": {},
   "source": [
    "#### 初期化方法\n",
    "\n",
    "PyTorchの`nn.Linear`は**Kaiming Uniform初期化**をデフォルトで使用します:\n",
    "\n",
    "```python\n",
    "# PyTorchのソースコードより\n",
    "def reset_parameters(self):\n",
    "    nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "    if self.bias is not None:\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "        nn.init.uniform_(self.bias, -bound, bound)\n",
    "```\n",
    "\n",
    "**Kaiming初期化の詳細:**\n",
    "- 重みは一様分布 $U(-\\text{bound}, \\text{bound})$ から初期化\n",
    "- $\\text{bound} = \\sqrt{\\frac{6}{(1+a^2) \\times \\text{fan\\_in}}}$\n",
    "- $a = \\sqrt{5}$ （デフォルト）\n",
    "- $\\text{fan\\_in}$ = 入力特徴数 = `in_features`\n",
    "\n",
    "**なぜこの初期化？**\n",
    "- 勾配消失・爆発を防ぐため\n",
    "- 各層の出力の分散を一定に保つため"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559d625e",
   "metadata": {},
   "source": [
    "#### 実験: 初期化の範囲を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9bc9d613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Kaiming初期化の範囲\n",
      "============================================================\n",
      "\n",
      "in_features = 8\n",
      "  理論的な範囲: [-0.3536, 0.3536]\n",
      "  実際の範囲:   [-0.3530, 0.3531]\n",
      "  標準偏差:     0.2010\n",
      "  理論的std:    0.2041\n",
      "\n",
      "in_features = 64\n",
      "  理論的な範囲: [-0.1250, 0.1250]\n",
      "  実際の範囲:   [-0.1249, 0.1250]\n",
      "  標準偏差:     0.0720\n",
      "  理論的std:    0.0722\n",
      "\n",
      "in_features = 512\n",
      "  理論的な範囲: [-0.0442, 0.0442]\n",
      "  実際の範囲:   [-0.0442, 0.0442]\n",
      "  標準偏差:     0.0255\n",
      "  理論的std:    0.0255\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Kaiming初期化の範囲\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for in_features in [8, 64, 512]:\n",
    "    linear = nn.Linear(in_features, 64)\n",
    "    \n",
    "    # 理論的な境界値を計算\n",
    "    a = np.sqrt(5)\n",
    "    fan_in = in_features\n",
    "    bound = np.sqrt(6 / ((1 + a**2) * fan_in))\n",
    "    \n",
    "    # 実際の重みの範囲\n",
    "    weight_min = linear.weight.data.min().item()\n",
    "    weight_max = linear.weight.data.max().item()\n",
    "    weight_std = linear.weight.data.std().item()\n",
    "    \n",
    "    print(f\"\\nin_features = {in_features}\")\n",
    "    print(f\"  理論的な範囲: [{-bound:.4f}, {bound:.4f}]\")\n",
    "    print(f\"  実際の範囲:   [{weight_min:.4f}, {weight_max:.4f}]\")\n",
    "    print(f\"  標準偏差:     {weight_std:.4f}\")\n",
    "    print(f\"  理論的std:    {bound/np.sqrt(3):.4f}\")  # 一様分布の標準偏差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d21eaf",
   "metadata": {},
   "source": [
    "#### バッチ処理とブロードキャスト\n",
    "\n",
    "`nn.Linear`は任意の形状のテンソルに対応できます:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "74e9c68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "様々な形状の入力に対する処理\n",
      "============================================================\n",
      "\n",
      "1次元: xtorch.Size([4]) -> ytorch.Size([3])\n",
      "2次元: xtorch.Size([5, 4]) -> ytorch.Size([5, 3])\n",
      "3次元: xtorch.Size([5, 10, 4]) -> ytorch.Size([5, 10, 3])\n",
      "4次元: xtorch.Size([5, 8, 10, 4]) -> ytorch.Size([5, 8, 10, 3])\n",
      "\n",
      "重要なポイント:\n",
      "  最後の次元だけが変換され、他の次元はそのまま保持される\n",
      "  (..., in_features) -> (..., out_features)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "linear = nn.Linear(4, 3)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"様々な形状の入力に対する処理\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1次元入力\n",
    "x1 = torch.randn(4)\n",
    "y1 = linear(x1)\n",
    "print(f\"\\n1次元: x{x1.shape} -> y{y1.shape}\")\n",
    "\n",
    "# 2次元入力（バッチ）\n",
    "x2 = torch.randn(5, 4)  # (batch_size, in_features)\n",
    "y2 = linear(x2)\n",
    "print(f\"2次元: x{x2.shape} -> y{y2.shape}\")\n",
    "\n",
    "# 3次元入力（バッチ + シーケンス）\n",
    "x3 = torch.randn(5, 10, 4)  # (batch_size, seq_len, in_features)\n",
    "y3 = linear(x3)\n",
    "print(f\"3次元: x{x3.shape} -> y{y3.shape}\")\n",
    "\n",
    "# 4次元入力（バッチ + マルチヘッド + シーケンス）\n",
    "x4 = torch.randn(5, 8, 10, 4)  # (batch, heads, seq_len, in_features)\n",
    "y4 = linear(x4)\n",
    "print(f\"4次元: x{x4.shape} -> y{y4.shape}\")\n",
    "\n",
    "print(\"\\n重要なポイント:\")\n",
    "print(\"  最後の次元だけが変換され、他の次元はそのまま保持される\")\n",
    "print(\"  (..., in_features) -> (..., out_features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368575bc",
   "metadata": {},
   "source": [
    "#### まとめ: nn.Linearの内部構造\n",
    "\n",
    "| 項目 | 内容 |\n",
    "|------|------|\n",
    "| **パラメータ** | `weight`: (out_features, in_features)<br>`bias`: (out_features,) |\n",
    "| **計算式** | $y = xW^T + b$ |\n",
    "| **初期化** | Kaiming Uniform (He初期化の一種) |\n",
    "| **初期化範囲** | $U(-\\text{bound}, \\text{bound})$ where $\\text{bound} = \\sqrt{\\frac{6}{(1+a^2) \\times \\text{fan\\_in}}}$ |\n",
    "| **入力形状** | `(..., in_features)` |\n",
    "| **出力形状** | `(..., out_features)` |\n",
    "| **特徴** | 最後の次元だけを変換、他は保持 |\n",
    "\n",
    "**Transformerでの使用例:**\n",
    "```python\n",
    "# Self-AttentionでQ, K, Vを生成\n",
    "W_q = nn.Linear(d_model, d_k)  # Query変換\n",
    "W_k = nn.Linear(d_model, d_k)  # Key変換\n",
    "W_v = nn.Linear(d_model, d_v)  # Value変換\n",
    "\n",
    "# 入力: (batch_size, seq_len, d_model)\n",
    "# 出力: (batch_size, seq_len, d_k) など\n",
    "```\n",
    "\n",
    "**重要なポイント:**\n",
    "1. 内部は単純な行列積 + バイアス\n",
    "2. Kaiming初期化で学習の安定性を確保\n",
    "3. 任意の次元のテンソルに対応（最後の次元を変換）\n",
    "4. パラメータ数 = `in_features × out_features + out_features`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553d470b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q7: Q, K, Vの場合は通常バイアスはないのか？\n",
    "\n",
    "はい、**Transformerの実装ではQ/K/V変換にバイアスを使わないことが多い**です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2beafb2",
   "metadata": {},
   "source": [
    "#### 元のTransformer論文（\"Attention is All You Need\"）\n",
    "\n",
    "元論文では**バイアスについて明記されていません**が、公式実装では:\n",
    "- Q/K/V変換: **bias=True**（バイアスあり）\n",
    "- 出力変換（Multi-Head後）: **bias=True**\n",
    "\n",
    "しかし、その後の多くの実装では**bias=False**が標準になっています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbc77d3",
   "metadata": {},
   "source": [
    "#### 主要な実装でのバイアスの扱い\n",
    "\n",
    "| 実装 | Q/K/V変換のbias | 理由・備考 |\n",
    "|------|----------------|-----------|\n",
    "| **PyTorch公式** (`nn.MultiheadAttention`) | `bias=True`（デフォルト） | 互換性重視 |\n",
    "| **BERT** | `bias=True` | 元論文に従う |\n",
    "| **GPT-2/GPT-3** | `bias=True` | - |\n",
    "| **T5** | **`bias=False`** | 性能向上・パラメータ削減 |\n",
    "| **LLaMA** | **`bias=False`** | 最新のベストプラクティス |\n",
    "| **GPT-NeoX** | **`bias=False`** | - |\n",
    "\n",
    "**最近のトレンド**: `bias=False`が主流になってきている"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1018915",
   "metadata": {},
   "source": [
    "#### なぜバイアスなしが好まれるのか？\n",
    "\n",
    "**1. 理論的な理由:**\n",
    "- Attentionは**相対的な関係性**を捉えるメカニズム\n",
    "- バイアスは**絶対的なオフセット**を加える\n",
    "- 相対性を重視するAttentionには不要\n",
    "\n",
    "**2. 数学的な観点:**\n",
    "```\n",
    "Q = XW_q + b_q\n",
    "K = XW_k + b_k\n",
    "\n",
    "QK^T = (XW_q + b_q)(XW_k + b_k)^T\n",
    "     = XW_qW_k^TX^T + XW_qb_k^T + b_qW_k^TX^T + b_qb_k^T\n",
    "```\n",
    "バイアス項が複雑な相互作用を生み、解釈が難しくなる\n",
    "\n",
    "**3. 実用的な理由:**\n",
    "- パラメータ数の削減（メモリ効率）\n",
    "- LayerNormと組み合わせると、バイアスの効果が打ち消される\n",
    "- 実験的に性能差がほとんどない\n",
    "\n",
    "**4. LayerNormとの関係:**\n",
    "```python\n",
    "# Attentionの後にLayerNormを適用\n",
    "x = Attention(x)  # bias=Falseでも\n",
    "x = LayerNorm(x)  # ここでバイアスが学習される\n",
    "```\n",
    "LayerNormがバイアス相当の機能を持つため、Q/K/Vにバイアスは不要"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4badb113",
   "metadata": {},
   "source": [
    "#### 実験: バイアスあり vs なし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3487e605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "バイアスあり vs なしの比較\n",
      "============================================================\n",
      "\n",
      "【バイアスあり】\n",
      "  パラメータ数:\n",
      "    W_q: 72 = 8×8 + 8\n",
      "    W_k: 72 = 8×8 + 8\n",
      "    合計: 144\n",
      "  Scores shape: torch.Size([2, 4, 4])\n",
      "  Scores mean: -0.0364\n",
      "  Scores std: 1.0342\n",
      "\n",
      "【バイアスなし】\n",
      "  パラメータ数:\n",
      "    W_q: 64 = 8×8\n",
      "    W_k: 64 = 8×8\n",
      "    合計: 128\n",
      "  Scores shape: torch.Size([2, 4, 4])\n",
      "  Scores mean: -0.1319\n",
      "  Scores std: 0.7986\n",
      "\n",
      "【削減されたパラメータ数】\n",
      "  16 パラメータ (11.1%削減)\n",
      "\n",
      "  大規模モデル（d_model=4096, 96層）の場合:\n",
      "  削減: 1,179,648 パラメータ (1.2M)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "d_model = 8\n",
    "d_k = 8\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "\n",
    "X = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"バイアスあり vs なしの比較\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# バイアスあり\n",
    "W_q_with_bias = nn.Linear(d_model, d_k, bias=True)\n",
    "W_k_with_bias = nn.Linear(d_model, d_k, bias=True)\n",
    "\n",
    "# バイアスなし\n",
    "W_q_no_bias = nn.Linear(d_model, d_k, bias=False)\n",
    "W_k_no_bias = nn.Linear(d_model, d_k, bias=False)\n",
    "\n",
    "print(f\"\\n【バイアスあり】\")\n",
    "print(f\"  パラメータ数:\")\n",
    "params_with = sum(p.numel() for p in W_q_with_bias.parameters())\n",
    "params_with += sum(p.numel() for p in W_k_with_bias.parameters())\n",
    "print(f\"    W_q: {sum(p.numel() for p in W_q_with_bias.parameters())} = {d_model}×{d_k} + {d_k}\")\n",
    "print(f\"    W_k: {sum(p.numel() for p in W_k_with_bias.parameters())} = {d_model}×{d_k} + {d_k}\")\n",
    "print(f\"    合計: {params_with}\")\n",
    "\n",
    "Q_with = W_q_with_bias(X)\n",
    "K_with = W_k_with_bias(X)\n",
    "scores_with = torch.matmul(Q_with, K_with.transpose(-2, -1))\n",
    "print(f\"  Scores shape: {scores_with.shape}\")\n",
    "print(f\"  Scores mean: {scores_with.mean().item():.4f}\")\n",
    "print(f\"  Scores std: {scores_with.std().item():.4f}\")\n",
    "\n",
    "print(f\"\\n【バイアスなし】\")\n",
    "print(f\"  パラメータ数:\")\n",
    "params_without = sum(p.numel() for p in W_q_no_bias.parameters())\n",
    "params_without += sum(p.numel() for p in W_k_no_bias.parameters())\n",
    "print(f\"    W_q: {sum(p.numel() for p in W_q_no_bias.parameters())} = {d_model}×{d_k}\")\n",
    "print(f\"    W_k: {sum(p.numel() for p in W_k_no_bias.parameters())} = {d_model}×{d_k}\")\n",
    "print(f\"    合計: {params_without}\")\n",
    "\n",
    "Q_without = W_q_no_bias(X)\n",
    "K_without = W_k_no_bias(X)\n",
    "scores_without = torch.matmul(Q_without, K_without.transpose(-2, -1))\n",
    "print(f\"  Scores shape: {scores_without.shape}\")\n",
    "print(f\"  Scores mean: {scores_without.mean().item():.4f}\")\n",
    "print(f\"  Scores std: {scores_without.std().item():.4f}\")\n",
    "\n",
    "print(f\"\\n【削減されたパラメータ数】\")\n",
    "saved = params_with - params_without\n",
    "print(f\"  {saved} パラメータ ({saved/params_with*100:.1f}%削減)\")\n",
    "print(f\"\\n  大規模モデル（d_model=4096, 96層）の場合:\")\n",
    "d_large = 4096\n",
    "n_layers = 96\n",
    "# Q, K, V それぞれにバイアス\n",
    "saved_large = n_layers * 3 * d_large  # 3 = Q, K, V\n",
    "print(f\"  削減: {saved_large:,} パラメータ ({saved_large/1e6:.1f}M)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca2739f",
   "metadata": {},
   "source": [
    "#### 実装の推奨事項\n",
    "\n",
    "**学習目的の実装:**\n",
    "```python\n",
    "# シンプルさ重視ならbias=True（デフォルト）\n",
    "W_q = nn.Linear(d_model, d_k)  # bias=True\n",
    "```\n",
    "\n",
    "**本格的な実装:**\n",
    "```python\n",
    "# 最新のベストプラクティスに従う\n",
    "W_q = nn.Linear(d_model, d_k, bias=False)\n",
    "W_k = nn.Linear(d_model, d_k, bias=False)\n",
    "W_v = nn.Linear(d_model, d_v, bias=False)\n",
    "```\n",
    "\n",
    "**PyTorchのnn.MultiheadAttentionを使う場合:**\n",
    "```python\n",
    "# add_bias_kvパラメータで制御可能\n",
    "attn = nn.MultiheadAttention(\n",
    "    embed_dim=d_model,\n",
    "    num_heads=8,\n",
    "    bias=False,  # Q/K/V変換のバイアス\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc6c651",
   "metadata": {},
   "source": [
    "#### まとめ\n",
    "\n",
    "| 観点 | バイアスあり | バイアスなし |\n",
    "|------|------------|------------|\n",
    "| **元論文** | ✓（暗黙的） | - |\n",
    "| **最近のトレンド** | - | ✓ 主流 |\n",
    "| **パラメータ数** | 多い | 少ない |\n",
    "| **理論的根拠** | 弱い | 強い（相対性重視） |\n",
    "| **LayerNormとの相性** | 冗長 | 良い |\n",
    "| **性能** | ≈同等 | ≈同等 |\n",
    "| **代表例** | BERT, GPT-2 | T5, LLaMA, GPT-NeoX |\n",
    "\n",
    "**結論:**\n",
    "- **歴史的経緯**: 初期の実装は`bias=True`\n",
    "- **現在のベストプラクティス**: `bias=False`が推奨\n",
    "- **実用的な差**: ほとんどないが、大規模モデルではパラメータ削減が重要\n",
    "- **学習目的**: どちらでもOK。`bias=False`の方がモダン\n",
    "\n",
    "**我々の実装方針:**\n",
    "```python\n",
    "# src/attention.pyでbias=Falseを採用\n",
    "self.W_q = nn.Linear(d_model, d_k, bias=False)\n",
    "self.W_k = nn.Linear(d_model, d_k, bias=False)\n",
    "self.W_v = nn.Linear(d_model, d_v, bias=False)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}