{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Network（FFN）デモ\n",
    "\n",
    "このノートブックでは、Transformerにおける**Feed Forward Network**の仕組みを学びます。\n",
    "\n",
    "## FFNの役割\n",
    "\n",
    "Transformerの各層は2つの主要コンポーネントで構成されます：\n",
    "\n",
    "1. **Multi-Head Attention**: トークン間の関係を捉える\n",
    "2. **Feed Forward Network**: 各トークンの特徴を非線形変換する\n",
    "\n",
    "```\n",
    "入力 → Attention → FFN → 出力\n",
    "        (線形)     (非線形)\n",
    "```\n",
    "\n",
    "FFNがなぜ必要か：\n",
    "- Attentionは本質的に**線形変換**（重み付き和）\n",
    "- FFNで**非線形性**を導入し、複雑なパターンを学習可能に"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.feed_forward import FeedForward, GatedFeedForward\n",
    "\n",
    "# 日本語フォント設定（macOS）\n",
    "plt.rcParams['font.family'] = 'Hiragino Sans'\n",
    "\n",
    "# 再現性のためのシード設定\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. FFNの構造\n",
    "\n",
    "論文 \"Attention is All You Need\" での定義：\n",
    "\n",
    "$$\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "つまり：\n",
    "1. **拡大**: `d_model → d_ff`（通常4倍）\n",
    "2. **活性化関数**: ReLU（または GELU）\n",
    "3. **縮小**: `d_ff → d_model`\n",
    "\n",
    "```\n",
    "入力         1層目           活性化       2層目         出力\n",
    "[d_model] → [d_ff=4×d_model] → ReLU → [d_model]\n",
    "   64    →      256         →      →    64\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFNの構造を確認\n",
    "\n",
    "d_model = 64\n",
    "d_ff = 256  # 4倍\n",
    "\n",
    "ffn = FeedForward(d_model, d_ff, dropout=0.0, activation='relu')\n",
    "\n",
    "print(\"Feed Forward Network の構造\")\n",
    "print(\"=\" * 50)\n",
    "print(ffn)\n",
    "print()\n",
    "\n",
    "# 各層の形状\n",
    "print(\"各層のパラメータ形状:\")\n",
    "print(f\"  linear1.weight: {ffn.linear1.weight.shape} (d_model→d_ff)\")\n",
    "print(f\"  linear1.bias:   {ffn.linear1.bias.shape}\")\n",
    "print(f\"  linear2.weight: {ffn.linear2.weight.shape} (d_ff→d_model)\")\n",
    "print(f\"  linear2.bias:   {ffn.linear2.bias.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 順伝播の流れを追跡\n",
    "\n",
    "batch_size = 1\n",
    "seq_len = 5\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"順伝播の流れ\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"入力:       {x.shape}\")\n",
    "\n",
    "# 手動で各ステップを実行\n",
    "h1 = ffn.linear1(x)\n",
    "print(f\"1層目後:    {h1.shape}  (d_model={d_model} → d_ff={d_ff})\")\n",
    "\n",
    "h2 = ffn.activation(h1)\n",
    "print(f\"ReLU後:     {h2.shape}  (形状は同じ、負の値が0に)\")\n",
    "\n",
    "h3 = ffn.linear2(h2)\n",
    "print(f\"2層目後:    {h3.shape}  (d_ff={d_ff} → d_model={d_model})\")\n",
    "\n",
    "# 検証\n",
    "output = ffn(x)\n",
    "print(f\"\\n最終出力:   {output.shape}\")\n",
    "print(f\"手動計算との差: {(output - h3).abs().max().item():.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. なぜ中間層を拡大するのか？\n",
    "\n",
    "FFNでは `d_model → d_ff (4倍) → d_model` と、一度次元を拡大してから縮小します。\n",
    "\n",
    "**理由**: 表現力の向上\n",
    "\n",
    "- 高次元空間でより複雑なパターンを捉える\n",
    "- ReLUで多くのニューロンが活性化し、多様な特徴を表現\n",
    "- 縮小時に重要な情報を圧縮・選択"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拡大率による違いを確認\n",
    "\n",
    "d_model = 64\n",
    "expansion_ratios = [1, 2, 4, 8]\n",
    "\n",
    "print(\"拡大率によるパラメータ数の変化\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for ratio in expansion_ratios:\n",
    "    d_ff = d_model * ratio\n",
    "    ffn = FeedForward(d_model, d_ff, dropout=0.0)\n",
    "    params = sum(p.numel() for p in ffn.parameters())\n",
    "    print(f\"拡大率 {ratio}x: d_ff={d_ff:4d}, パラメータ数={params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU活性化の効果を可視化\n",
    "\n",
    "d_model = 64\n",
    "d_ff = 256\n",
    "ffn = FeedForward(d_model, d_ff, dropout=0.0, activation='relu')\n",
    "\n",
    "x = torch.randn(1, 1, d_model)  # 1つのトークン\n",
    "\n",
    "# 1層目の出力（活性化前後）\n",
    "h_before = ffn.linear1(x).squeeze()\n",
    "h_after = ffn.activation(h_before)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# 活性化前\n",
    "axes[0].bar(range(len(h_before)), h_before.detach().numpy(), alpha=0.7)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--', linewidth=1)\n",
    "axes[0].set_title('ReLU適用前（1層目出力）')\n",
    "axes[0].set_xlabel('ニューロン')\n",
    "axes[0].set_ylabel('活性値')\n",
    "\n",
    "# 活性化後\n",
    "axes[1].bar(range(len(h_after)), h_after.detach().numpy(), alpha=0.7, color='green')\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', linewidth=1)\n",
    "axes[1].set_title('ReLU適用後（負の値が0に）')\n",
    "axes[1].set_xlabel('ニューロン')\n",
    "axes[1].set_ylabel('活性値')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 活性化されたニューロンの割合\n",
    "active_ratio = (h_after > 0).float().mean().item()\n",
    "print(f\"活性化されたニューロンの割合: {active_ratio*100:.1f}%\")\n",
    "print(f\"（約半分が活性化されるのが典型的）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 活性化関数の比較: ReLU vs GELU\n",
    "\n",
    "| 活性化関数 | 数式 | 使用モデル |\n",
    "|-----------|------|------------|\n",
    "| ReLU | max(0, x) | 元のTransformer |\n",
    "| GELU | x × Φ(x) | BERT, GPT-2以降 |\n",
    "\n",
    "GELUは滑らかな活性化関数で、勾配消失問題を軽減します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU vs GELU の比較\n",
    "\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "\n",
    "relu = nn.ReLU()\n",
    "gelu = nn.GELU()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 活性化関数の形状\n",
    "axes[0].plot(x.numpy(), relu(x).numpy(), label='ReLU', linewidth=2)\n",
    "axes[0].plot(x.numpy(), gelu(x).numpy(), label='GELU', linewidth=2)\n",
    "axes[0].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "axes[0].axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "axes[0].set_xlabel('入力 x')\n",
    "axes[0].set_ylabel('出力')\n",
    "axes[0].set_title('活性化関数の比較')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 勾配の比較\n",
    "x_grad = x.clone().requires_grad_(True)\n",
    "relu_out = relu(x_grad)\n",
    "relu_grad = torch.autograd.grad(relu_out.sum(), x_grad, create_graph=True)[0]\n",
    "\n",
    "x_grad2 = x.clone().requires_grad_(True)\n",
    "gelu_out = gelu(x_grad2)\n",
    "gelu_grad = torch.autograd.grad(gelu_out.sum(), x_grad2, create_graph=True)[0]\n",
    "\n",
    "axes[1].plot(x.numpy(), relu_grad.detach().numpy(), label='ReLU勾配', linewidth=2)\n",
    "axes[1].plot(x.numpy(), gelu_grad.detach().numpy(), label='GELU勾配', linewidth=2)\n",
    "axes[1].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "axes[1].axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "axes[1].set_xlabel('入力 x')\n",
    "axes[1].set_ylabel('勾配')\n",
    "axes[1].set_title('勾配の比較')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"観察:\")\n",
    "print(\"- ReLU: x=0で不連続な勾配（0 or 1）\")\n",
    "print(\"- GELU: 滑らかな勾配、負の入力にも小さな勾配\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFNでの出力比較\n",
    "\n",
    "d_model = 64\n",
    "d_ff = 256\n",
    "\n",
    "ffn_relu = FeedForward(d_model, d_ff, dropout=0.0, activation='relu')\n",
    "ffn_gelu = FeedForward(d_model, d_ff, dropout=0.0, activation='gelu')\n",
    "\n",
    "# 同じ重みを使用して比較\n",
    "ffn_gelu.linear1.weight.data = ffn_relu.linear1.weight.data.clone()\n",
    "ffn_gelu.linear1.bias.data = ffn_relu.linear1.bias.data.clone()\n",
    "ffn_gelu.linear2.weight.data = ffn_relu.linear2.weight.data.clone()\n",
    "ffn_gelu.linear2.bias.data = ffn_relu.linear2.bias.data.clone()\n",
    "\n",
    "x = torch.randn(1, 10, d_model)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_relu = ffn_relu(x)\n",
    "    out_gelu = ffn_gelu(x)\n",
    "\n",
    "print(\"同じ重みでの出力比較（最初のトークン、最初の10次元）:\")\n",
    "print(f\"ReLU: {out_relu[0, 0, :10].numpy()}\")\n",
    "print(f\"GELU: {out_gelu[0, 0, :10].numpy()}\")\n",
    "print(f\"\\n差の絶対値の平均: {(out_relu - out_gelu).abs().mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Position-wise（位置ごと）とは？\n",
    "\n",
    "FFNの重要な特徴は、**各位置（トークン）に独立に同じ変換を適用する**ことです。\n",
    "\n",
    "```\n",
    "トークン1 → FFN → 出力1\n",
    "トークン2 → FFN → 出力2   (同じFFN)\n",
    "トークン3 → FFN → 出力3   (同じFFN)\n",
    "```\n",
    "\n",
    "- **Attention**: トークン間の情報を交換\n",
    "- **FFN**: 各トークンを独立に変換（位置間の交換なし）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position-wiseの確認\n",
    "\n",
    "d_model = 64\n",
    "ffn = FeedForward(d_model, dropout=0.0)\n",
    "\n",
    "# 3つのトークンを持つシーケンス\n",
    "x = torch.randn(1, 3, d_model)\n",
    "\n",
    "# 全体を一度に処理\n",
    "with torch.no_grad():\n",
    "    output_batch = ffn(x)\n",
    "\n",
    "# 各トークンを個別に処理\n",
    "with torch.no_grad():\n",
    "    output_0 = ffn(x[:, 0:1, :])\n",
    "    output_1 = ffn(x[:, 1:2, :])\n",
    "    output_2 = ffn(x[:, 2:3, :])\n",
    "    output_individual = torch.cat([output_0, output_1, output_2], dim=1)\n",
    "\n",
    "print(\"Position-wise（位置ごと）の確認\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"バッチ処理の出力形状: {output_batch.shape}\")\n",
    "print(f\"個別処理の出力形状:   {output_individual.shape}\")\n",
    "print(f\"\\n両者の差: {(output_batch - output_individual).abs().max().item():.10f}\")\n",
    "print(\"→ 完全に一致！各位置が独立に処理されている\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attentionとの比較\n",
    "\n",
    "from src.attention import MultiHeadAttention\n",
    "\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads, dropout=0.0)\n",
    "ffn = FeedForward(d_model, dropout=0.0)\n",
    "\n",
    "x = torch.randn(1, 5, d_model)\n",
    "\n",
    "print(\"AttentionとFFNの違い\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 位置1のトークンだけ変更\n",
    "x_modified = x.clone()\n",
    "x_modified[0, 1, :] = torch.randn(d_model)  # 位置1を変更\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Attention\n",
    "    attn_out_orig, _ = mha(x, x, x)\n",
    "    attn_out_mod, _ = mha(x_modified, x_modified, x_modified)\n",
    "    \n",
    "    # FFN\n",
    "    ffn_out_orig = ffn(x)\n",
    "    ffn_out_mod = ffn(x_modified)\n",
    "\n",
    "print(\"位置1のトークンを変更した場合:\")\n",
    "print()\n",
    "print(\"【Attention】\")\n",
    "for pos in range(5):\n",
    "    diff = (attn_out_orig[0, pos] - attn_out_mod[0, pos]).abs().mean().item()\n",
    "    print(f\"  位置{pos}の出力変化: {diff:.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"【FFN】\")\n",
    "for pos in range(5):\n",
    "    diff = (ffn_out_orig[0, pos] - ffn_out_mod[0, pos]).abs().mean().item()\n",
    "    print(f\"  位置{pos}の出力変化: {diff:.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"観察:\")\n",
    "print(\"- Attention: 位置1の変更が全位置に影響\")\n",
    "print(\"- FFN: 位置1の変更は位置1の出力にのみ影響\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gated FFN（SwiGLU）\n",
    "\n",
    "最新のLLM（LLaMA, PaLMなど）では、**ゲート機構**付きのFFNが使われています：\n",
    "\n",
    "$$\\text{FFN}_{\\text{SwiGLU}}(x) = (xW_1 \\odot \\text{SiLU}(xW_{\\text{gate}})) W_2$$\n",
    "\n",
    "- $\\odot$: element-wise乗算\n",
    "- SiLU: Swish活性化関数（$x \\cdot \\sigma(x)$）\n",
    "\n",
    "ゲートにより、情報の流れを動的に制御します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gated FFNの構造\n",
    "\n",
    "d_model = 64\n",
    "d_ff = 256\n",
    "\n",
    "gated_ffn = GatedFeedForward(d_model, d_ff, dropout=0.0)\n",
    "\n",
    "print(\"Gated Feed Forward Network の構造\")\n",
    "print(\"=\" * 50)\n",
    "print(gated_ffn)\n",
    "print()\n",
    "\n",
    "print(\"各層のパラメータ形状:\")\n",
    "print(f\"  w1.weight:     {gated_ffn.w1.weight.shape} (d_model→d_ff)\")\n",
    "print(f\"  w_gate.weight: {gated_ffn.w_gate.weight.shape} (d_model→d_ff, ゲート用)\")\n",
    "print(f\"  w2.weight:     {gated_ffn.w2.weight.shape} (d_ff→d_model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SiLU活性化関数の可視化\n",
    "\n",
    "x = torch.linspace(-5, 5, 100)\n",
    "\n",
    "silu = nn.SiLU()\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# SiLU = x * sigmoid(x)\n",
    "axes[0].plot(x.numpy(), silu(x).numpy(), label='SiLU (Swish)', linewidth=2)\n",
    "axes[0].plot(x.numpy(), x.numpy() * sigmoid(x).numpy(), '--', label='x × σ(x)', linewidth=2, alpha=0.7)\n",
    "axes[0].plot(x.numpy(), x.numpy(), ':', label='y = x', linewidth=1, alpha=0.5)\n",
    "axes[0].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "axes[0].axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "axes[0].set_xlabel('入力 x')\n",
    "axes[0].set_ylabel('出力')\n",
    "axes[0].set_title('SiLU (Swish) 活性化関数')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# ゲート機構のイメージ\n",
    "axes[1].plot(x.numpy(), sigmoid(x).numpy(), label='σ(x) (ゲート値)', linewidth=2)\n",
    "axes[1].axhline(y=0.5, color='r', linestyle='--', linewidth=1, alpha=0.5)\n",
    "axes[1].set_xlabel('入力 x')\n",
    "axes[1].set_ylabel('ゲート値 (0~1)')\n",
    "axes[1].set_title('ゲート: 情報をどれだけ通すか')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"SiLUの特徴:\")\n",
    "print(\"- 負の入力に対しても小さな負の値を出力（ReLUと異なり完全に0にしない）\")\n",
    "print(\"- 滑らかな勾配で学習が安定\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 標準FFN vs Gated FFNのパラメータ比較\n",
    "\n",
    "d_model = 512  # 実際のTransformerでよく使われる値\n",
    "d_ff = 2048\n",
    "\n",
    "standard_ffn = FeedForward(d_model, d_ff, dropout=0.0)\n",
    "gated_ffn = GatedFeedForward(d_model, d_ff, dropout=0.0)\n",
    "\n",
    "standard_params = sum(p.numel() for p in standard_ffn.parameters())\n",
    "gated_params = sum(p.numel() for p in gated_ffn.parameters())\n",
    "\n",
    "print(\"パラメータ数の比較 (d_model=512, d_ff=2048)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"標準FFN:  {standard_params:,} パラメータ\")\n",
    "print(f\"  - linear1: {d_model}×{d_ff} + {d_ff} = {d_model*d_ff + d_ff:,}\")\n",
    "print(f\"  - linear2: {d_ff}×{d_model} + {d_model} = {d_ff*d_model + d_model:,}\")\n",
    "print()\n",
    "print(f\"Gated FFN: {gated_params:,} パラメータ\")\n",
    "print(f\"  - w1:     {d_model}×{d_ff} = {d_model*d_ff:,}\")\n",
    "print(f\"  - w_gate: {d_model}×{d_ff} = {d_model*d_ff:,}\")\n",
    "print(f\"  - w2:     {d_ff}×{d_model} = {d_ff*d_model:,}\")\n",
    "print()\n",
    "print(f\"比率: Gated / Standard = {gated_params / standard_params:.2f}x\")\n",
    "print(\"\\n→ Gated FFNは約1.5倍のパラメータだが、性能向上が報告されている\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. FFNのパラメータ数\n",
    "\n",
    "Transformerにおいて、FFNはパラメータの大部分を占めます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformerにおけるパラメータ配分\n",
    "\n",
    "d_model = 512\n",
    "d_ff = 2048  # 4倍\n",
    "num_heads = 8\n",
    "\n",
    "# Multi-Head Attention\n",
    "mha = MultiHeadAttention(d_model, num_heads, dropout=0.0)\n",
    "mha_params = sum(p.numel() for p in mha.parameters())\n",
    "\n",
    "# Feed Forward\n",
    "ffn = FeedForward(d_model, d_ff, dropout=0.0)\n",
    "ffn_params = sum(p.numel() for p in ffn.parameters())\n",
    "\n",
    "total = mha_params + ffn_params\n",
    "\n",
    "print(\"1層あたりのパラメータ配分 (d_model=512, d_ff=2048)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Multi-Head Attention: {mha_params:,} ({mha_params/total*100:.1f}%)\")\n",
    "print(f\"Feed Forward Network: {ffn_params:,} ({ffn_params/total*100:.1f}%)\")\n",
    "print(f\"合計: {total:,}\")\n",
    "\n",
    "# 可視化\n",
    "plt.figure(figsize=(8, 6))\n",
    "labels = ['Multi-Head Attention', 'Feed Forward Network']\n",
    "sizes = [mha_params, ffn_params]\n",
    "colors = ['#3498db', '#e74c3c']\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Transformer 1層のパラメータ配分')\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n→ FFNがパラメータの約2/3を占める！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. まとめ\n",
    "\n",
    "### Feed Forward Networkの役割\n",
    "- Attentionで集約した情報を**非線形変換**\n",
    "- 各位置（トークン）に**独立に**適用（Position-wise）\n",
    "\n",
    "### 構造\n",
    "```\n",
    "入力[d_model] → 拡大[d_ff=4×d_model] → 活性化 → 縮小[d_model] → 出力\n",
    "```\n",
    "\n",
    "### 活性化関数\n",
    "| 活性化関数 | 特徴 | 使用例 |\n",
    "|-----------|------|--------|\n",
    "| ReLU | シンプル | 元のTransformer |\n",
    "| GELU | 滑らか | BERT, GPT |\n",
    "| SwiGLU | ゲート機構 | LLaMA, PaLM |\n",
    "\n",
    "### 次のステップ\n",
    "- Layer Normalization\n",
    "- Residual Connection\n",
    "- Encoderブロック全体の構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習パラメータの確認\n",
    "\n",
    "print(\"Feed Forward Network パラメータ一覧\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "d_model = 64\n",
    "d_ff = 256\n",
    "\n",
    "print(\"\\n【標準 FFN (ReLU)】\")\n",
    "ffn = FeedForward(d_model, d_ff)\n",
    "for name, param in ffn.named_parameters():\n",
    "    print(f\"  {name}: {param.shape} = {param.numel():,} params\")\n",
    "print(f\"  合計: {sum(p.numel() for p in ffn.parameters()):,} params\")\n",
    "\n",
    "print(\"\\n【Gated FFN (SwiGLU)】\")\n",
    "gated = GatedFeedForward(d_model, d_ff)\n",
    "for name, param in gated.named_parameters():\n",
    "    print(f\"  {name}: {param.shape} = {param.numel():,} params\")\n",
    "print(f\"  合計: {sum(p.numel() for p in gated.parameters()):,} params\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
