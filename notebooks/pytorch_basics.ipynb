{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch入門 - 自作Transformerを理解するための基礎\n",
    "\n",
    "このノートブックでは、自作Transformerのソースコードを読み解くために必要なPyTorchの基礎を学びます。\n",
    "\n",
    "## 目次\n",
    "\n",
    "1. [テンソル（Tensor）の基礎](#1-テンソルの基礎)\n",
    "2. [自動微分（Autograd）](#2-自動微分)\n",
    "3. [nn.Moduleでモデルを作る](#3-nnmoduleでモデルを作る)\n",
    "4. [よく使う層（Linear, Embedding, LayerNorm）](#4-よく使う層)\n",
    "5. [活性化関数の選び方](#5-活性化関数の選び方)\n",
    "6. [損失関数](#6-損失関数)\n",
    "7. [最適化アルゴリズム](#7-最適化アルゴリズム)\n",
    "8. [学習ループの書き方](#8-学習ループの書き方)\n",
    "9. [推論（評価）モード](#9-推論モード)\n",
    "10. [モデルの確認とデバッグ](#10-モデルの確認とデバッグ)\n",
    "11. [GPUの使い方](#11-gpuの使い方)\n",
    "12. [Transformerコードとの対応](#12-transformerコードとの対応)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 日本語フォント設定\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.family'] = 'Hiragino Sans'\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 再現性のためのシード固定\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. テンソルの基礎\n",
    "\n",
    "**テンソル（Tensor）** はPyTorchの基本データ構造です。NumPyの配列に似ていますが、GPUで計算でき、自動微分をサポートします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 テンソルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接作成\n",
    "a = torch.tensor([1, 2, 3])\n",
    "print(f\"1次元テンソル: {a}\")\n",
    "print(f\"形状: {a.shape}\")\n",
    "print(f\"データ型: {a.dtype}\")\n",
    "print()\n",
    "\n",
    "# 2次元テンソル（行列）\n",
    "b = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "print(f\"2次元テンソル:\\n{b}\")\n",
    "print(f\"形状: {b.shape}  # (行数, 列数)\")\n",
    "print()\n",
    "\n",
    "# 3次元テンソル（Transformerで頻出）\n",
    "c = torch.randn(2, 3, 4)  # (バッチ, シーケンス長, 特徴次元)\n",
    "print(f\"3次元テンソルの形状: {c.shape}\")\n",
    "print(f\"  - バッチサイズ: {c.shape[0]}\")\n",
    "print(f\"  - シーケンス長: {c.shape[1]}\")\n",
    "print(f\"  - 特徴次元: {c.shape[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 特殊なテンソルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ゼロで初期化\n",
    "zeros = torch.zeros(3, 4)\n",
    "print(f\"ゼロテンソル:\\n{zeros}\")\n",
    "\n",
    "# 1で初期化\n",
    "ones = torch.ones(2, 3)\n",
    "print(f\"\\n1テンソル:\\n{ones}\")\n",
    "\n",
    "# 乱数（標準正規分布）\n",
    "randn = torch.randn(2, 3)  # 平均0、標準偏差1\n",
    "print(f\"\\n正規分布乱数:\\n{randn}\")\n",
    "\n",
    "# 整数乱数（トークンIDなどに使用）\n",
    "randint = torch.randint(0, 10, (3, 4))  # 0-9の整数\n",
    "print(f\"\\n整数乱数:\\n{randint}\")\n",
    "\n",
    "# 単位行列\n",
    "eye = torch.eye(3)\n",
    "print(f\"\\n単位行列:\\n{eye}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 テンソルの演算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1., 2.], [3., 4.]])\n",
    "y = torch.tensor([[5., 6.], [7., 8.]])\n",
    "\n",
    "# 要素ごとの演算\n",
    "print(f\"x + y (要素ごとの加算):\\n{x + y}\")\n",
    "print(f\"\\nx * y (要素ごとの乗算):\\n{x * y}\")\n",
    "\n",
    "# 行列積（Transformerで最重要！）\n",
    "print(f\"\\nx @ y (行列積):\\n{x @ y}\")\n",
    "print(f\"torch.matmul(x, y) も同じ:\\n{torch.matmul(x, y)}\")\n",
    "\n",
    "# スカラー演算\n",
    "print(f\"\\nx * 2:\\n{x * 2}\")\n",
    "print(f\"\\nx / 2:\\n{x / 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 形状変換（reshape, view, transpose）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(12)  # [0, 1, 2, ..., 11]\n",
    "print(f\"元のテンソル: {x}\")\n",
    "print(f\"形状: {x.shape}\")\n",
    "\n",
    "# reshape: 形状を変更\n",
    "x_reshaped = x.reshape(3, 4)\n",
    "print(f\"\\nreshape(3, 4):\\n{x_reshaped}\")\n",
    "\n",
    "# view: reshapeとほぼ同じ（メモリ連続性が必要）\n",
    "x_viewed = x.view(4, 3)\n",
    "print(f\"\\nview(4, 3):\\n{x_viewed}\")\n",
    "\n",
    "# -1 は自動計算\n",
    "x_auto = x.reshape(2, -1)  # 2行、列数は自動\n",
    "print(f\"\\nreshape(2, -1): 形状={x_auto.shape}\")\n",
    "\n",
    "# transpose: 次元を入れ替え\n",
    "y = torch.randn(2, 3, 4)\n",
    "print(f\"\\n元の形状: {y.shape}\")\n",
    "print(f\"transpose(1, 2): {y.transpose(1, 2).shape}  # (2, 4, 3)\")\n",
    "\n",
    "# permute: 任意の次元順に並び替え\n",
    "print(f\"permute(2, 0, 1): {y.permute(2, 0, 1).shape}  # (4, 2, 3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 次元の追加・削除（unsqueeze, squeeze）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, 4)\n",
    "print(f\"元の形状: {x.shape}\")\n",
    "\n",
    "# unsqueeze: 次元を追加\n",
    "x1 = x.unsqueeze(0)  # 先頭に次元追加\n",
    "print(f\"unsqueeze(0): {x1.shape}  # バッチ次元を追加\")\n",
    "\n",
    "x2 = x.unsqueeze(1)  # 1番目に次元追加\n",
    "print(f\"unsqueeze(1): {x2.shape}\")\n",
    "\n",
    "x3 = x.unsqueeze(-1)  # 末尾に次元追加\n",
    "print(f\"unsqueeze(-1): {x3.shape}\")\n",
    "\n",
    "# squeeze: サイズ1の次元を削除\n",
    "y = torch.randn(1, 3, 1, 4)\n",
    "print(f\"\\n元の形状: {y.shape}\")\n",
    "print(f\"squeeze(): {y.squeeze().shape}  # サイズ1の次元をすべて削除\")\n",
    "print(f\"squeeze(0): {y.squeeze(0).shape}  # 0番目の次元のみ削除\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 自動微分（Autograd）\n",
    "\n",
    "PyTorchは**自動微分**機能を持っています。これにより、勾配（微分）を自動で計算できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires_grad=True で勾配を追跡\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "print(f\"x = {x}\")\n",
    "\n",
    "# 計算グラフを構築\n",
    "y = x ** 2  # y = [4, 9]\n",
    "z = y.sum()  # z = 13\n",
    "print(f\"y = x^2 = {y}\")\n",
    "print(f\"z = sum(y) = {z}\")\n",
    "\n",
    "# 逆伝播（勾配計算）\n",
    "z.backward()\n",
    "\n",
    "# dz/dx = 2x\n",
    "print(f\"\\n勾配 dz/dx = {x.grad}\")\n",
    "print(f\"期待値: 2 * x = {2 * x.detach()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 勾配の流れを止める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "# 方法1: with torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    y = x * 2\n",
    "    print(f\"no_grad内: requires_grad = {y.requires_grad}\")\n",
    "\n",
    "# 方法2: detach()\n",
    "z = x.detach()\n",
    "print(f\"detach後: requires_grad = {z.requires_grad}\")\n",
    "\n",
    "# 推論時は勾配計算を止める（メモリ節約、高速化）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. nn.Moduleでモデルを作る\n",
    "\n",
    "PyTorchでニューラルネットワークを作るには `nn.Module` を継承します。\n",
    "\n",
    "### 基本構造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()  # 親クラスの初期化（必須！）\n",
    "        \n",
    "        # 層の定義\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"順伝播を定義（必須！）\"\"\"\n",
    "        x = self.fc1(x)      # 線形変換\n",
    "        x = self.relu(x)     # 活性化関数\n",
    "        x = self.fc2(x)      # 線形変換\n",
    "        return x\n",
    "\n",
    "# モデルのインスタンス化\n",
    "model = SimpleNetwork(input_size=10, hidden_size=20, output_size=5)\n",
    "print(model)\n",
    "\n",
    "# 順伝播\n",
    "x = torch.randn(3, 10)  # バッチサイズ3、入力次元10\n",
    "output = model(x)       # model.forward(x) と同じ\n",
    "print(f\"\\n入力形状: {x.shape}\")\n",
    "print(f\"出力形状: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 super().__init__() の意味\n",
    "\n",
    "`super().__init__()` は親クラス（`nn.Module`）の初期化を呼び出します。これにより：\n",
    "- パラメータの自動登録\n",
    "- `.to(device)` でのGPU移動\n",
    "- `.train()` / `.eval()` モード切り替え\n",
    "\n",
    "などの機能が使えるようになります。**必ず書く必要があります。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 nn.Sequential を使った簡潔な書き方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Sequential: 層を順番に適用\n",
    "simple_model = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 5)\n",
    ")\n",
    "\n",
    "print(simple_model)\n",
    "\n",
    "x = torch.randn(3, 10)\n",
    "output = simple_model(x)\n",
    "print(f\"\\n出力形状: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. よく使う層\n",
    "\n",
    "Transformerで使われる主要な層を理解しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 nn.Linear（線形層・全結合層）\n",
    "\n",
    "数式: $y = xW^T + b$\n",
    "\n",
    "入力の各要素に重みを掛けて足し合わせ、バイアスを加えます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Linear(入力次元, 出力次元)\n",
    "linear = nn.Linear(4, 3)\n",
    "\n",
    "print(f\"重み(weight)の形状: {linear.weight.shape}\")\n",
    "print(f\"バイアス(bias)の形状: {linear.bias.shape}\")\n",
    "\n",
    "x = torch.randn(2, 4)  # バッチ2、入力4次元\n",
    "y = linear(x)\n",
    "print(f\"\\n入力形状: {x.shape}\")\n",
    "print(f\"出力形状: {y.shape}\")\n",
    "\n",
    "# バイアスなしの場合\n",
    "linear_no_bias = nn.Linear(4, 3, bias=False)\n",
    "print(f\"\\nbias=False: バイアスは{linear_no_bias.bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 nn.Embedding（埋め込み層）\n",
    "\n",
    "整数のインデックス（トークンID）を密なベクトルに変換します。\n",
    "\n",
    "**本質はルックアップテーブル**：ID→ベクトルの対応表です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Embedding(語彙サイズ, 埋め込み次元)\n",
    "vocab_size = 100  # 100種類の単語\n",
    "embed_dim = 16    # 各単語を16次元ベクトルで表現\n",
    "\n",
    "embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "print(f\"埋め込み行列の形状: {embedding.weight.shape}\")\n",
    "print(f\"  = (語彙サイズ, 埋め込み次元)\")\n",
    "\n",
    "# トークンID列を入力\n",
    "token_ids = torch.tensor([[1, 5, 3, 2],   # 文1\n",
    "                          [7, 2, 8, 0]])  # 文2\n",
    "\n",
    "embedded = embedding(token_ids)\n",
    "print(f\"\\n入力（トークンID）形状: {token_ids.shape}\")\n",
    "print(f\"出力（埋め込みベクトル）形状: {embedded.shape}\")\n",
    "print(f\"  = (バッチサイズ, シーケンス長, 埋め込み次元)\")\n",
    "\n",
    "# padding_idx: 特定のIDをゼロベクトルに固定\n",
    "embedding_with_pad = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "print(f\"\\npadding_idx=0: ID=0は常にゼロベクトル\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 nn.LayerNorm（層正規化）\n",
    "\n",
    "各サンプルの特徴量を正規化（平均0、分散1に近づける）します。\n",
    "\n",
    "学習を安定させ、収束を早める効果があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.LayerNorm(正規化する次元のサイズ)\n",
    "d_model = 8\n",
    "layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "x = torch.randn(2, 3, d_model)  # (バッチ, シーケンス長, 特徴次元)\n",
    "normalized = layer_norm(x)\n",
    "\n",
    "print(f\"入力形状: {x.shape}\")\n",
    "print(f\"出力形状: {normalized.shape}\")\n",
    "\n",
    "# 正規化の効果を確認\n",
    "print(f\"\\n正規化前: 平均={x[0, 0].mean():.4f}, 標準偏差={x[0, 0].std():.4f}\")\n",
    "print(f\"正規化後: 平均={normalized[0, 0].mean():.4f}, 標準偏差={normalized[0, 0].std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 nn.Dropout（ドロップアウト）\n",
    "\n",
    "ランダムに一部のニューロンを無効化します。過学習を防ぐ正則化手法です。\n",
    "\n",
    "**重要**: 訓練時のみ適用され、推論時は無効になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = nn.Dropout(p=0.5)  # 50%の確率で無効化\n",
    "\n",
    "x = torch.ones(2, 10)\n",
    "\n",
    "# 訓練モード\n",
    "dropout.train()\n",
    "out_train = dropout(x)\n",
    "print(f\"訓練モード: {out_train}\")\n",
    "print(f\"  → 一部が0になり、残りは1/(1-p)=2倍にスケール\")\n",
    "\n",
    "# 評価モード\n",
    "dropout.eval()\n",
    "out_eval = dropout(x)\n",
    "print(f\"\\n評価モード: {out_eval}\")\n",
    "print(f\"  → 何も変化しない\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. 活性化関数の選び方\n",
    "\n",
    "活性化関数はニューラルネットワークに**非線形性**を導入します。\n",
    "\n",
    "線形変換だけでは、どれだけ層を重ねても1つの線形変換と等価になってしまいます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-5, 5, 100)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 6))\n",
    "\n",
    "# ReLU: 最も基本的、勾配消失しにくい\n",
    "axes[0, 0].plot(x, F.relu(x))\n",
    "axes[0, 0].set_title('ReLU: max(0, x)')\n",
    "axes[0, 0].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[0, 0].axvline(x=0, color='k', linewidth=0.5)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# GELU: Transformerで使用、滑らか\n",
    "axes[0, 1].plot(x, F.gelu(x))\n",
    "axes[0, 1].set_title('GELU: Transformerで人気')\n",
    "axes[0, 1].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[0, 1].axvline(x=0, color='k', linewidth=0.5)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Sigmoid: 0-1に圧縮、二値分類の出力に\n",
    "axes[0, 2].plot(x, torch.sigmoid(x))\n",
    "axes[0, 2].set_title('Sigmoid: 0-1に圧縮')\n",
    "axes[0, 2].axhline(y=0.5, color='r', linewidth=0.5, linestyle='--')\n",
    "axes[0, 2].axvline(x=0, color='k', linewidth=0.5)\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Tanh: -1から1に圧縮\n",
    "axes[1, 0].plot(x, torch.tanh(x))\n",
    "axes[1, 0].set_title('Tanh: -1から1に圧縮')\n",
    "axes[1, 0].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[1, 0].axvline(x=0, color='k', linewidth=0.5)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Softmax: 確率分布に変換\n",
    "logits = torch.tensor([-1.0, 0.0, 1.0, 2.0])\n",
    "probs = F.softmax(logits, dim=0)\n",
    "axes[1, 1].bar(range(4), probs.numpy())\n",
    "axes[1, 1].set_title('Softmax: 確率分布に変換')\n",
    "axes[1, 1].set_xlabel('クラス')\n",
    "axes[1, 1].set_ylabel('確率')\n",
    "\n",
    "# SiLU/Swish: GELU類似、LLaMaなどで使用\n",
    "axes[1, 2].plot(x, F.silu(x))\n",
    "axes[1, 2].set_title('SiLU/Swish: x * sigmoid(x)')\n",
    "axes[1, 2].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[1, 2].axvline(x=0, color='k', linewidth=0.5)\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 活性化関数の選び方ガイド\n",
    "\n",
    "| 活性化関数 | 用途 | 特徴 |\n",
    "|-----------|------|------|\n",
    "| **ReLU** | 隠れ層（デフォルト） | シンプル、高速、勾配消失しにくい |\n",
    "| **GELU** | Transformer隠れ層 | 滑らか、BERT/GPTで使用 |\n",
    "| **SiLU/Swish** | 最新のLLM | LLaMA, PaLMで使用 |\n",
    "| **Sigmoid** | 二値分類の出力層 | 0-1に変換 |\n",
    "| **Softmax** | 多クラス分類の出力層 | 確率分布に変換 |\n",
    "| **Tanh** | 特殊な用途 | -1から1に変換 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 nn.関数 vs F.関数\n",
    "\n",
    "PyTorchには2種類の使い方があります："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, 4)\n",
    "\n",
    "# 方法1: nn.Module として（状態を持てる）\n",
    "relu_module = nn.ReLU()\n",
    "out1 = relu_module(x)\n",
    "\n",
    "# 方法2: 関数として（F = torch.nn.functional）\n",
    "out2 = F.relu(x)\n",
    "\n",
    "print(f\"nn.ReLU(): {out1.shape}\")\n",
    "print(f\"F.relu(): {out2.shape}\")\n",
    "print(f\"結果は同じ: {torch.allclose(out1, out2)}\")\n",
    "\n",
    "# どちらを使うべきか？\n",
    "# - パラメータがある層（Linear, LayerNorm）→ nn.Module\n",
    "# - パラメータがない関数（relu, softmax）→ どちらでもOK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. 損失関数\n",
    "\n",
    "損失関数は「モデルの予測がどれだけ間違っているか」を数値化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クロスエントロピー損失（分類タスク）\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 予測（logits、softmax前の値）\n",
    "predictions = torch.tensor([[2.0, 1.0, 0.1],   # クラス0が最大\n",
    "                            [0.5, 2.5, 0.3]])  # クラス1が最大\n",
    "\n",
    "# 正解ラベル\n",
    "targets = torch.tensor([0, 1])  # サンプル1はクラス0、サンプル2はクラス1\n",
    "\n",
    "loss = criterion(predictions, targets)\n",
    "print(f\"損失: {loss.item():.4f}\")\n",
    "print(f\"  → 予測が正解に近いほど損失は小さい\")\n",
    "\n",
    "# 間違った予測の場合\n",
    "wrong_targets = torch.tensor([2, 0])  # 間違ったラベル\n",
    "wrong_loss = criterion(predictions, wrong_targets)\n",
    "print(f\"\\n間違った予測の損失: {wrong_loss.item():.4f}\")\n",
    "print(f\"  → 損失が大きくなる\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 ignore_index（パディングを無視）\n",
    "\n",
    "Transformerでは、パディングトークンの損失を計算しないようにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パディングを無視する損失関数\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # ID=0をパディングとして無視\n",
    "\n",
    "# 予測（バッチ2、シーケンス長3、語彙サイズ5）\n",
    "predictions = torch.randn(2, 3, 5)\n",
    "\n",
    "# ターゲット（0はパディング）\n",
    "targets = torch.tensor([[1, 2, 0],   # 3番目はパディング\n",
    "                        [3, 0, 0]])  # 2,3番目はパディング\n",
    "\n",
    "# 形状を変換して損失計算\n",
    "loss = criterion(\n",
    "    predictions.reshape(-1, 5),  # (バッチ*シーケンス, 語彙サイズ)\n",
    "    targets.reshape(-1)          # (バッチ*シーケンス,)\n",
    ")\n",
    "\n",
    "print(f\"損失（パディング無視）: {loss.item():.4f}\")\n",
    "print(f\"  → ID=0の位置は損失計算に含まれない\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 主な損失関数\n",
    "\n",
    "| 損失関数 | 用途 | 入力 |\n",
    "|---------|------|------|\n",
    "| `nn.CrossEntropyLoss` | 多クラス分類 | logits (softmax前) |\n",
    "| `nn.BCEWithLogitsLoss` | 二値分類 | logits (sigmoid前) |\n",
    "| `nn.MSELoss` | 回帰 | 連続値 |\n",
    "| `nn.L1Loss` | 回帰（外れ値に強い） | 連続値 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. 最適化アルゴリズム\n",
    "\n",
    "最適化アルゴリズムは、損失を最小化するようにパラメータを更新します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 簡単なモデル\n",
    "model = nn.Linear(10, 5)\n",
    "\n",
    "# Adam最適化（最も一般的）\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# SGD（基本的な勾配降下法）\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# AdamW（重み減衰付きAdam、Transformerで推奨）\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "print(f\"最適化対象のパラメータ数: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 学習率（Learning Rate）\n",
    "\n",
    "学習率は「1回の更新でどれだけパラメータを変えるか」を決めます。\n",
    "\n",
    "- **大きすぎる**: 発散する、収束しない\n",
    "- **小さすぎる**: 学習が遅い、局所解に陥りやすい\n",
    "\n",
    "一般的な値: `1e-3`（0.001）〜 `1e-4`（0.0001）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. 学習ループの書き方\n",
    "\n",
    "PyTorchでの学習ループの基本パターンを学びます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 簡単な分類モデル\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(4, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 3)\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# ダミーデータ\n",
    "X = torch.randn(100, 4)  # 100サンプル、4特徴\n",
    "y = torch.randint(0, 3, (100,))  # 3クラス分類\n",
    "\n",
    "# 学習ループ\n",
    "num_epochs = 100\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 1. 訓練モードに設定\n",
    "    model.train()\n",
    "    \n",
    "    # 2. 順伝播\n",
    "    predictions = model(X)\n",
    "    \n",
    "    # 3. 損失計算\n",
    "    loss = criterion(predictions, y)\n",
    "    \n",
    "    # 4. 勾配をリセット（重要！）\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 5. 逆伝播（勾配計算）\n",
    "    loss.backward()\n",
    "    \n",
    "    # 6. パラメータ更新\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 損失の推移をプロット\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('エポック')\n",
    "plt.ylabel('損失')\n",
    "plt.title('学習曲線')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 学習ループの各ステップ解説\n",
    "\n",
    "```python\n",
    "# 1. model.train() - 訓練モード\n",
    "#    Dropout, BatchNormなどが訓練用の動作になる\n",
    "\n",
    "# 2. predictions = model(X) - 順伝播\n",
    "#    入力から出力を計算\n",
    "\n",
    "# 3. loss = criterion(predictions, y) - 損失計算\n",
    "#    予測と正解の差を計算\n",
    "\n",
    "# 4. optimizer.zero_grad() - 勾配リセット\n",
    "#    前回の勾配をクリア（しないと勾配が蓄積される）\n",
    "\n",
    "# 5. loss.backward() - 逆伝播\n",
    "#    損失から各パラメータの勾配を計算\n",
    "\n",
    "# 6. optimizer.step() - パラメータ更新\n",
    "#    勾配を使ってパラメータを更新\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 よくある間違い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ❌ 間違い1: zero_grad()を忘れる\n",
    "# → 勾配が蓄積されて学習がおかしくなる\n",
    "\n",
    "# ❌ 間違い2: backward()の後にzero_grad()を呼ぶ\n",
    "# → 計算した勾配が消えてしまう\n",
    "\n",
    "# ❌ 間違い3: step()の前にbackward()を忘れる\n",
    "# → 勾配がゼロなので更新されない\n",
    "\n",
    "# ✅ 正しい順序\n",
    "# optimizer.zero_grad() → 順伝播 → loss.backward() → optimizer.step()\n",
    "\n",
    "print(\"正しい順序:\")\n",
    "print(\"1. optimizer.zero_grad()  # 勾配クリア\")\n",
    "print(\"2. output = model(input)  # 順伝播\")\n",
    "print(\"3. loss = criterion(...)  # 損失計算\")\n",
    "print(\"4. loss.backward()        # 逆伝播\")\n",
    "print(\"5. optimizer.step()       # 更新\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. 推論（評価）モード\n",
    "\n",
    "学習後のモデルで予測を行う方法です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価モードに設定\n",
    "model.eval()\n",
    "\n",
    "# 勾配計算を無効化（メモリ節約、高速化）\n",
    "with torch.no_grad():\n",
    "    # テストデータ\n",
    "    X_test = torch.randn(10, 4)\n",
    "    \n",
    "    # 予測\n",
    "    predictions = model(X_test)\n",
    "    \n",
    "    # 予測クラスを取得\n",
    "    predicted_classes = predictions.argmax(dim=1)\n",
    "    \n",
    "    print(f\"予測形状: {predictions.shape}\")\n",
    "    print(f\"予測クラス: {predicted_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 train() vs eval() の違い\n",
    "\n",
    "| モード | Dropout | BatchNorm | 用途 |\n",
    "|--------|---------|-----------|------|\n",
    "| `model.train()` | 有効 | 統計更新 | 学習時 |\n",
    "| `model.eval()` | 無効 | 固定統計 | 推論時 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. モデルの確認とデバッグ\n",
    "\n",
    "モデルの構造やパラメータを確認する方法を学びます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプルモデル\n",
    "class SampleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(100, 32)\n",
    "        self.linear1 = nn.Linear(32, 64)\n",
    "        self.linear2 = nn.Linear(64, 10)\n",
    "        self.layer_norm = nn.LayerNorm(64)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "model = SampleModel()\n",
    "\n",
    "# モデル構造を表示\n",
    "print(\"=\" * 50)\n",
    "print(\"モデル構造\")\n",
    "print(\"=\" * 50)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータ数を確認\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"総パラメータ数: {count_parameters(model):,}\")\n",
    "\n",
    "# 各層のパラメータ数\n",
    "print(\"\\n各層のパラメータ:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name}: {param.shape} = {param.numel():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入出力形状の確認\n",
    "x = torch.randint(0, 100, (2, 5))  # バッチ2、シーケンス長5\n",
    "print(f\"入力形状: {x.shape}\")\n",
    "\n",
    "# 中間出力を確認したい場合\n",
    "with torch.no_grad():\n",
    "    embedded = model.embedding(x)\n",
    "    print(f\"Embedding後: {embedded.shape}\")\n",
    "    \n",
    "    linear1_out = model.linear1(embedded)\n",
    "    print(f\"Linear1後: {linear1_out.shape}\")\n",
    "    \n",
    "    output = model(x)\n",
    "    print(f\"最終出力: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 モデルの保存と読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存（パラメータのみ）\n",
    "# torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "# 読み込み\n",
    "# model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "# state_dictの中身を確認\n",
    "print(\"state_dict のキー:\")\n",
    "for key in model.state_dict().keys():\n",
    "    print(f\"  {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. GPUの使い方\n",
    "\n",
    "PyTorchではGPU（CUDA/MPS）を使って計算を高速化できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# デバイスの選択\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"CUDA GPU使用可能: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Apple Silicon GPU (MPS) 使用可能\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"CPU使用\")\n",
    "\n",
    "print(f\"\\n使用デバイス: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルをGPUに移動\n",
    "model = SampleModel().to(device)\n",
    "print(f\"モデルのデバイス: 次のパラメータで確認\")\n",
    "print(f\"  {next(model.parameters()).device}\")\n",
    "\n",
    "# データもGPUに移動\n",
    "x = torch.randint(0, 100, (2, 5)).to(device)\n",
    "print(f\"\\nデータのデバイス: {x.device}\")\n",
    "\n",
    "# 計算実行\n",
    "output = model(x)\n",
    "print(f\"出力のデバイス: {output.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1 デバイス間でのデータ移動"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUからCPUに移動（結果の取得時など）\n",
    "output_cpu = output.cpu()\n",
    "print(f\"CPUに移動: {output_cpu.device}\")\n",
    "\n",
    "# NumPy配列に変換（CPUのみ可能）\n",
    "output_numpy = output_cpu.detach().numpy()\n",
    "print(f\"NumPy配列に変換: {type(output_numpy)}\")\n",
    "\n",
    "# 注意: GPUテンソルを直接NumPyに変換するとエラー\n",
    "# output.numpy()  # ❌ エラー\n",
    "# output.cpu().numpy()  # ✅ OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Transformerコードとの対応\n",
    "\n",
    "ここまで学んだ内容が、自作Transformerのコードでどのように使われているか確認しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1 Self-Attention（src/attention.py）\n",
    "\n",
    "```python\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()  # ← 親クラス初期化（必須）\n",
    "        \n",
    "        # Linear層でQ, K, Vを生成\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # 線形変換\n",
    "        Q = self.W_q(x)  # ← nn.Linearを適用\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Attention計算\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1))  # ← 行列積とtranspose\n",
    "        scores = scores / (self.d_model ** 0.5)  # ← スケーリング\n",
    "        \n",
    "        weights = F.softmax(scores, dim=-1)  # ← softmaxで確率化\n",
    "        output = torch.matmul(weights, V)  # ← 行列積\n",
    "        \n",
    "        return output, weights\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2 Transformer（src/transformer.py）\n",
    "\n",
    "```python\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, ...):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 埋め込み層（トークンID → ベクトル）\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model, padding_idx=0)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model, padding_idx=0)\n",
    "        \n",
    "        # 位置エンコーディング\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
    "        \n",
    "        # Encoder/Decoder（nn.Moduleを継承したクラス）\n",
    "        self.encoder = Encoder(...)\n",
    "        self.decoder = Decoder(...)\n",
    "        \n",
    "        # 出力層（ベクトル → 語彙サイズの確率）\n",
    "        self.output_projection = nn.Linear(d_model, tgt_vocab_size)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3 学習ループ（notebooks内のデモ）\n",
    "\n",
    "```python\n",
    "model = Transformer(...).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # ← 訓練モード\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        src, tgt_input, tgt_output = batch\n",
    "        src, tgt_input, tgt_output = src.to(device), ...  # ← GPU移動\n",
    "        \n",
    "        optimizer.zero_grad()  # ← 勾配リセット\n",
    "        \n",
    "        output = model(src, tgt_input)  # ← 順伝播\n",
    "        \n",
    "        loss = criterion(\n",
    "            output.reshape(-1, vocab_size),\n",
    "            tgt_output.reshape(-1)\n",
    "        )\n",
    "        \n",
    "        loss.backward()  # ← 逆伝播\n",
    "        optimizer.step()  # ← 更新\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## まとめ\n",
    "\n",
    "### 最低限覚えるべきこと\n",
    "\n",
    "1. **テンソル**: PyTorchの基本データ型、形状（shape）が重要\n",
    "2. **nn.Module**: モデルの基底クラス、`__init__`と`forward`を定義\n",
    "3. **nn.Linear**: 線形変換 $y = xW^T + b$\n",
    "4. **nn.Embedding**: ID → ベクトルのルックアップテーブル\n",
    "5. **活性化関数**: ReLU（デフォルト）、GELU（Transformer）\n",
    "6. **損失関数**: CrossEntropyLoss（分類）\n",
    "7. **最適化**: Adam, AdamW\n",
    "8. **学習ループ**: zero_grad → forward → loss → backward → step\n",
    "9. **評価モード**: model.eval() + torch.no_grad()\n",
    "10. **GPU**: .to(device) でモデルとデータを移動\n",
    "\n",
    "### 次のステップ\n",
    "\n",
    "この基礎を理解したら、以下のノートブックでTransformerの実装を学んでいきましょう：\n",
    "\n",
    "1. `01_self_attention_demo.ipynb` - Attention機構の理解\n",
    "2. `02_multi_head_attention_demo.ipynb` - Multi-Head Attention\n",
    "3. `tutorial_transformer.ipynb` - 完全なTransformerの使い方"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
