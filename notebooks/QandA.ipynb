{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13c53921",
   "metadata": {},
   "source": [
    "# Transformer学習 Q&A\n",
    "\n",
    "このノートブックは、Transformerの実装学習中に出た質問と回答をまとめたものです。\n",
    "\n",
    "各Q&Aには、質問内容、回答、実際に動かせるコード例が含まれています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24622a01",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q1: nn.Linearとは？\n",
    "\n",
    "**質問日**: 2025年11月17日\n",
    "\n",
    "### 質問\n",
    "`nn.Linear`とは何ですか？Self-Attentionでどのように使われていますか？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3a4629",
   "metadata": {},
   "source": [
    "### 回答\n",
    "\n",
    "`nn.Linear`は、PyTorchの**線形変換層（全結合層）**です。\n",
    "\n",
    "#### 基本的な役割\n",
    "\n",
    "入力に対して、重み行列との積とバイアスの加算を行います：\n",
    "\n",
    "**数式**: `y = xW^T + b`\n",
    "\n",
    "- `x`: 入力ベクトル\n",
    "- `W`: 重み行列（学習可能なパラメータ）\n",
    "- `b`: バイアスベクトル（学習可能なパラメータ）\n",
    "- `y`: 出力ベクトル\n",
    "\n",
    "#### Self-Attentionでの使い方\n",
    "\n",
    "同じ入力から、Query、Key、Valueという3つの異なる表現を生成するために使います。それぞれ独立した重み行列で変換することで、各変換が異なる役割を学習します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb105478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kouhei/tmp/learn_transformer/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebab6707",
   "metadata": {},
   "source": [
    "#### コード例: 基本的な使い方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50a3bf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力の形状: torch.Size([2, 8])\n",
      "入力:\n",
      "tensor([[-0.7372,  0.7487, -2.3663,  0.1733,  0.4600, -0.0624,  0.2781,  0.4151],\n",
      "        [ 0.3485,  1.5541,  0.0344,  0.3960, -0.9718,  0.6995, -0.3303,  0.2447]])\n",
      "\n",
      "出力の形状: torch.Size([2, 8])\n",
      "出力:\n",
      "tensor([[ 0.7823,  0.4769,  0.4181,  0.5381, -0.4960,  0.4418,  0.4725, -0.8636],\n",
      "        [-0.0645,  0.3408, -0.1933, -0.0480, -0.5187,  0.5934,  0.3662, -0.3147]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "重み行列の形状: torch.Size([8, 8])\n",
      "バイアスの形状: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 8次元の入力を8次元の出力に変換する線形層\n",
    "linear = nn.Linear(in_features=8, out_features=8, bias=True)\n",
    "\n",
    "# 入力データ: [batch_size=2, features=8]\n",
    "x = torch.randn(2, 8)\n",
    "print(f\"入力の形状: {x.shape}\")\n",
    "print(f\"入力:\\n{x}\\n\")\n",
    "\n",
    "# 線形変換を適用\n",
    "y = linear(x)\n",
    "print(f\"出力の形状: {y.shape}\")\n",
    "print(f\"出力:\\n{y}\\n\")\n",
    "\n",
    "# パラメータの確認\n",
    "print(f\"重み行列の形状: {linear.weight.shape}\")  # [out_features, in_features]\n",
    "print(f\"バイアスの形状: {linear.bias.shape}\")      # [out_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948de590",
   "metadata": {},
   "source": [
    "#### コード例: Self-Attentionでの使い方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d0b1c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力データの形状: torch.Size([1, 4, 8])\n",
      "  [バッチサイズ, シーケンス長, 埋め込み次元]\n",
      "\n",
      "Query (Q) の形状: torch.Size([1, 4, 8])\n",
      "Key (K) の形状: torch.Size([1, 4, 8])\n",
      "Value (V) の形状: torch.Size([1, 4, 8])\n",
      "\n",
      "重要なポイント:\n",
      "  - 同じ入力Xから生成 → 「Self」Attention\n",
      "  - 異なる重み行列W_q, W_k, W_vで変換\n",
      "  - 各変換が独立して学習される\n"
     ]
    }
   ],
   "source": [
    "# Self-Attentionでの使用例\n",
    "d_model = 8\n",
    "seq_len = 4\n",
    "batch_size = 1\n",
    "\n",
    "# 入力データ（シーケンスデータ）\n",
    "X = torch.randn(batch_size, seq_len, d_model)\n",
    "print(f\"入力データの形状: {X.shape}\")\n",
    "print(f\"  [バッチサイズ, シーケンス長, 埋め込み次元]\\n\")\n",
    "\n",
    "# Query, Key, Value用の線形変換層\n",
    "# バイアスなし（bias=False）が一般的\n",
    "W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "# 同じ入力Xから、異なる重み行列で Q, K, V を生成\n",
    "Q = W_q(X)  # Query: 「何を探しているか」\n",
    "K = W_k(X)  # Key: 「何を持っているか」\n",
    "V = W_v(X)  # Value: 「実際の情報」\n",
    "\n",
    "print(f\"Query (Q) の形状: {Q.shape}\")\n",
    "print(f\"Key (K) の形状: {K.shape}\")\n",
    "print(f\"Value (V) の形状: {V.shape}\")\n",
    "\n",
    "print(f\"\\n重要なポイント:\")\n",
    "print(f\"  - 同じ入力Xから生成 → 「Self」Attention\")\n",
    "print(f\"  - 異なる重み行列W_q, W_k, W_vで変換\")\n",
    "print(f\"  - 各変換が独立して学習される\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74819c45",
   "metadata": {},
   "source": [
    "#### まとめ\n",
    "\n",
    "- `nn.Linear`は線形変換（行列積 + バイアス）を行う層\n",
    "- Self-Attentionでは、入力から**Q, K, V**を生成するために3つの`nn.Linear`を使用\n",
    "- 各`nn.Linear`は独立した重み行列を持ち、学習を通じて最適化される\n",
    "- `bias=False`が一般的（Transformerの論文では省略されている）\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dc4384",
   "metadata": {},
   "source": [
    "## Q2: Query、Key、Valueとは？KとVの違いは？\n",
    "\n",
    "**質問日**: 2025年11月17日\n",
    "\n",
    "### 質問\n",
    "\n",
    "「Queryは何を探しているか」「Keyは何を持っているか」「Valueは実際の情報」という説明では、**KeyとValueの違い**が分かりません。もっと具体的に説明してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab95ccfa",
   "metadata": {},
   "source": [
    "### 回答: 具体例で理解する\n",
    "\n",
    "抽象的な説明ではなく、**データベース検索**に例えると分かりやすくなります。\n",
    "\n",
    "#### データベース検索の例\n",
    "\n",
    "あなたが図書館のシステムで本を探すとします：\n",
    "\n",
    "1. **Query（検索クエリ）**: `\"機械学習\"`という検索ワード\n",
    "2. **Key（索引・タグ）**: 各本に付けられたキーワード（「AI」「統計」「Python」など）\n",
    "3. **Value（実際のデータ）**: 本の内容そのもの\n",
    "\n",
    "**検索の流れ**:\n",
    "- あなたの検索ワード（Query）と各本のキーワード（Key）を比較\n",
    "- マッチ度が高い本ほど高いスコアを付ける\n",
    "- スコアに基づいて、実際の本の内容（Value）を取得\n",
    "\n",
    "**重要**: KeyとValueは別物！\n",
    "- **Key**: マッチング（類似度計算）に使う「索引」\n",
    "- **Value**: 実際に取り出したい「中身」"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c67749",
   "metadata": {},
   "source": [
    "#### Self-Attentionでの具体例\n",
    "\n",
    "文「私は 猫が 好き です」を処理する場合：\n",
    "\n",
    "**元の入力**: 各単語の埋め込みベクトル（例: 8次元）\n",
    "\n",
    "この入力から、**目的に応じて異なる表現**を作ります：\n",
    "\n",
    "1. **Query（Q）**: 「この単語は、他のどの単語と関連付けたいか？」を表す表現\n",
    "   - 例: 「好き」という単語が「何が好きなのか？」を探すための表現\n",
    "   \n",
    "2. **Key（K）**: 「この単語は、どんな情報で検索されたいか？」を表す表現\n",
    "   - 例: 「猫が」という単語が「動物」「対象」として検索される際の表現\n",
    "   \n",
    "3. **Value（V）**: 「この単語が持つ実際の意味情報」\n",
    "   - 例: 「猫が」という単語の持つ本来の意味的な情報\n",
    "\n",
    "**計算の流れ**:\n",
    "1. Queryで「探したい情報のパターン」を表現\n",
    "2. Keyで「マッチング用の特徴」を表現\n",
    "3. QueryとKeyの類似度（内積）を計算 → Attention Weight\n",
    "4. Attention Weightを使って、Valueを重み付けして集約"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c761f52a",
   "metadata": {},
   "source": [
    "#### なぜKとVを分ける必要があるのか？\n",
    "\n",
    "**重要な理由**: マッチング用の特徴と、取り出したい情報は**別物**だから！\n",
    "\n",
    "**例1: 単語の品詞と意味**\n",
    "- Key: 「動詞」「名詞」などの**文法的特徴**でマッチング\n",
    "- Value: その単語の**意味的な情報**を取得\n",
    "\n",
    "**例2: 画像認識（Vision Transformer）**\n",
    "- Key: 「エッジ」「色」などの**視覚的特徴**でマッチング  \n",
    "- Value: その領域の**詳細な情報**を取得\n",
    "\n",
    "もしKeyとValueが同じだと、「マッチングに最適な表現」と「情報として最適な表現」が一緒になってしまい、表現力が制限されます。\n",
    "\n",
    "**分けることで**:\n",
    "- Keyは「どの情報に注目すべきか」の判断に特化\n",
    "- Valueは「実際に取り出す情報」として最適化\n",
    "- それぞれが独立して学習できる → より柔軟で強力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8075e74",
   "metadata": {},
   "source": [
    "#### 数値例で確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f490c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力 X:\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.]])\n",
      "tensor([[[1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.]]])\n",
      "形状: torch.Size([1, 3, 4])\n",
      "\n",
      "Query (Q) - 探索用の表現:\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "Key (K) - マッチング用の表現:\n",
      "tensor([[2., 0., 0., 0.],\n",
      "        [0., 2., 0., 0.],\n",
      "        [0., 0., 2., 0.]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "Value (V) - 取り出す情報:\n",
      "tensor([[1.0000, 0.5000, 0.0000, 0.0000],\n",
      "        [0.5000, 1.0000, 0.5000, 0.0000],\n",
      "        [0.0000, 0.5000, 1.0000, 0.5000]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "【観察】\n",
      "- Q, K, V は同じ入力Xから生成されるが、異なる重み行列で変換\n",
      "- Kはマッチング用にスケール調整（×2）\n",
      "- Vは周辺の情報を混ぜ合わせた表現（スムージング）\n",
      "→ それぞれ異なる目的に特化した表現になっている！\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# シンプルな例: 3つの単語、次元数4\n",
    "seq_len = 3\n",
    "d_model = 4\n",
    "\n",
    "# 入力: 3つの単語の埋め込みベクトル\n",
    "X = torch.tensor([\n",
    "    [1.0, 0.0, 0.0, 0.0],  # 単語1\n",
    "    [0.0, 1.0, 0.0, 0.0],  # 単語2  \n",
    "    [0.0, 0.0, 1.0, 0.0],  # 単語3\n",
    "]).unsqueeze(0)  # [1, 3, 4]\n",
    "\n",
    "print(\"入力 X:\")\n",
    "print(X[0])\n",
    "print(X)\n",
    "print(f\"形状: {X.shape}\\n\")\n",
    "\n",
    "# Q, K, V用の変換（わかりやすくするため単純化）\n",
    "W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "# 重みを固定して違いを明確に\n",
    "with torch.no_grad():\n",
    "    # Qの重み: 「探す方向」を強調\n",
    "    W_q.weight.copy_(torch.eye(d_model))\n",
    "    \n",
    "    # Kの重み: 「マッチング用の特徴」を抽出\n",
    "    W_k.weight.copy_(torch.eye(d_model) * 2)\n",
    "    \n",
    "    # Vの重み: 「取り出す情報」を変換\n",
    "    W_v.weight.copy_(torch.tensor([\n",
    "        [1.0, 0.5, 0.0, 0.0],\n",
    "        [0.5, 1.0, 0.5, 0.0],\n",
    "        [0.0, 0.5, 1.0, 0.5],\n",
    "        [0.0, 0.0, 0.5, 1.0],\n",
    "    ]))\n",
    "\n",
    "Q = W_q(X)\n",
    "K = W_k(X)\n",
    "V = W_v(X)\n",
    "\n",
    "print(\"Query (Q) - 探索用の表現:\")\n",
    "print(Q[0])\n",
    "print(\"\\nKey (K) - マッチング用の表現:\")\n",
    "print(K[0])\n",
    "print(\"\\nValue (V) - 取り出す情報:\")\n",
    "print(V[0])\n",
    "\n",
    "print(\"\\n【観察】\")\n",
    "print(\"- Q, K, V は同じ入力Xから生成されるが、異なる重み行列で変換\")\n",
    "print(\"- Kはマッチング用にスケール調整（×2）\")\n",
    "print(\"- Vは周辺の情報を混ぜ合わせた表現（スムージング）\")\n",
    "print(\"→ それぞれ異なる目的に特化した表現になっている！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39119e2",
   "metadata": {},
   "source": [
    "#### Attention計算の全体像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3421896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Scores (Q × K^T / √d):\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "各行: 各単語（Query）が、全単語（Key）とどれだけマッチするか\n",
      "\n",
      "Attention Weights (softmax適用後):\n",
      "tensor([[0.5761, 0.2119, 0.2119],\n",
      "        [0.2119, 0.5761, 0.2119],\n",
      "        [0.2119, 0.2119, 0.5761]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "各行の合計: tensor([1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)\n",
      "\n",
      "最終出力 (Attention Weights × Value):\n",
      "tensor([[0.6821, 0.6060, 0.3179, 0.1060],\n",
      "        [0.5000, 0.7881, 0.5000, 0.1060],\n",
      "        [0.3179, 0.6060, 0.6821, 0.2881]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "============================================================\n",
      "【重要な理解】\n",
      "============================================================\n",
      "1. QueryとKeyを使って「どの情報に注目するか」を決定\n",
      "2. その重み（Attention Weights）を使って\n",
      "3. Valueから実際の情報を取り出す\n",
      "\n",
      "KeyとValueが別物だからこそ:\n",
      "  ✓ マッチングの基準（Key）と\n",
      "  ✓ 取り出す情報（Value）を\n",
      "  独立して最適化できる！\n"
     ]
    }
   ],
   "source": [
    "# 上で作ったQ, K, Vを使って、Attentionを計算\n",
    "\n",
    "# Step 1: QueryとKeyの類似度を計算（スコア）\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_model ** 0.5)\n",
    "print(\"Attention Scores (Q × K^T / √d):\")\n",
    "print(scores[0])\n",
    "print(\"\\n各行: 各単語（Query）が、全単語（Key）とどれだけマッチするか\")\n",
    "\n",
    "# Step 2: Softmaxで正規化 → Attention Weights\n",
    "attn_weights = F.softmax(scores, dim=-1)\n",
    "print(\"\\nAttention Weights (softmax適用後):\")\n",
    "print(attn_weights[0])\n",
    "print(\"\\n各行の合計:\", attn_weights[0].sum(dim=-1))\n",
    "\n",
    "# Step 3: Attention WeightsでValueを重み付け\n",
    "output = torch.matmul(attn_weights, V)\n",
    "print(\"\\n最終出力 (Attention Weights × Value):\")\n",
    "print(output[0])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"【重要な理解】\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. QueryとKeyを使って「どの情報に注目するか」を決定\")\n",
    "print(\"2. その重み（Attention Weights）を使って\")\n",
    "print(\"3. Valueから実際の情報を取り出す\")\n",
    "print(\"\")\n",
    "print(\"KeyとValueが別物だからこそ:\")\n",
    "print(\"  ✓ マッチングの基準（Key）と\")\n",
    "print(\"  ✓ 取り出す情報（Value）を\")\n",
    "print(\"  独立して最適化できる！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074cca53",
   "metadata": {},
   "source": [
    "#### まとめ: Q, K, Vの役割\n",
    "\n",
    "| 要素 | 役割 | 例え | 実際の使われ方 |\n",
    "|------|------|------|----------------|\n",
    "| **Query (Q)** | 「何を探すか」を表す | 検索ワード | 各単語が「どんな情報を必要としているか」 |\n",
    "| **Key (K)** | 「マッチング用の特徴」 | 索引・タグ | 各単語が「どんな特徴でマッチするか」 |\n",
    "| **Value (V)** | 「実際に取り出す情報」 | 本の中身 | 実際に集約される意味情報 |\n",
    "\n",
    "**計算の流れ**:\n",
    "```\n",
    "1. Q × K^T → どの単語に注目すべきかのスコア\n",
    "2. softmax → スコアを確率分布に変換（Attention Weights）\n",
    "3. Attention Weights × V → 重み付けされた情報の集約\n",
    "```\n",
    "\n",
    "**KeyとValueを分ける理由**:\n",
    "- **Key**: 類似度計算（マッチング）に最適な表現に特化\n",
    "- **Value**: 取り出す情報として最適な表現に特化\n",
    "- 分けることで、それぞれが独立して学習でき、表現力が向上\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cfbf23",
   "metadata": {},
   "source": [
    "## Q3: Q, K, Vの意味は後の演算で明確になるのか？\n",
    "\n",
    "**質問日**: 2025年11月17日\n",
    "\n",
    "### 質問\n",
    "\n",
    "現時点では、数学的にはQ, K, Vは同じもの（入力X）をNNで変換しただけで、それぞれのNNの重みが違うので出力が違うだけですよね。\n",
    "\n",
    "「Queryは探す」「Keyはマッチング」「Valueは情報」という**意味合い**は、これから行う演算（内積、Softmax、重み付き和）で明確になるのでしょうか？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba33efd3",
   "metadata": {},
   "source": [
    "### 回答: はい、その通りです！\n",
    "\n",
    "**素晴らしい洞察です。** あなたの理解は完全に正しいです。\n",
    "\n",
    "#### 現時点での数学的な事実\n",
    "\n",
    "```python\n",
    "Q = W_q(X)  # ただの線形変換\n",
    "K = W_k(X)  # ただの線形変換\n",
    "V = W_v(X)  # ただの線形変換\n",
    "```\n",
    "\n",
    "この時点では、Q, K, Vは**ただ重みが違うだけの3つの出力**です。\n",
    "「Query」「Key」「Value」という名前は、人間が後付けした**ラベル**に過ぎません。\n",
    "\n",
    "#### 意味が明確になるのは「使われ方」から\n",
    "\n",
    "Q, K, Vの意味は、**これから行う演算での役割**によって初めて明確になります：\n",
    "\n",
    "1. **Q × K^T**: QueryとKeyの内積 → 「どれとどれが関連するか」を計算\n",
    "2. **Softmax**: スコアを正規化 → 「注目の配分」を決定\n",
    "3. **Attention × V**: 重みでValueを集約 → 「実際の情報を取り出す」\n",
    "\n",
    "つまり、**演算の構造が意味を定義している**のです！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e83adc",
   "metadata": {},
   "source": [
    "#### 演算フローで見る「役割の具体化」"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c42bfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: Q と K の内積 → ここでQとKの役割が分化\n",
      "============================================================\n",
      "\n",
      "スコア行列 (Q × K^T):\n",
      "tensor([[-0.5625, -0.3980, -0.1153],\n",
      "        [-1.9257, -0.3124, -0.0290],\n",
      "        [ 0.9459,  0.1995, -0.0170]])\n",
      "\n",
      "【ここで初めて意味が生まれる】\n",
      "  - Qの各行: 「この位置が探している情報のパターン」\n",
      "  - Kの各行: 「この位置が提供できる情報の特徴」\n",
      "  - 内積が大きい = マッチ度が高い\n",
      "\n",
      "============================================================\n",
      "STEP 2: Softmax → 確率分布に変換\n",
      "============================================================\n",
      "\n",
      "Attention Weights:\n",
      "tensor([[0.2672, 0.3150, 0.4179],\n",
      "        [0.0788, 0.3957, 0.5254],\n",
      "        [0.5388, 0.2555, 0.2057]])\n",
      "\n",
      "【意味】\n",
      "  各行: 各位置が「どの位置の情報をどれだけ取り込むか」の配分\n",
      "\n",
      "============================================================\n",
      "STEP 3: Attention × V → ここでVの役割が明確に\n",
      "============================================================\n",
      "\n",
      "最終出力:\n",
      "tensor([[-0.5283, -0.0061, -0.7205,  0.3914],\n",
      "        [-1.1627, -0.2045, -0.9031,  0.6110],\n",
      "        [ 0.3818,  0.3055, -0.5251,  0.1105]])\n",
      "\n",
      "【ここで初めてVの意味が生まれる】\n",
      "  - Vの各行: 「実際に取り出される情報の中身」\n",
      "  - Attention Weightsで重み付けして集約される\n",
      "  - Qが決めた配分で、Vから情報を抽出\n",
      "\n",
      "============================================================\n",
      "【結論】\n",
      "============================================================\n",
      "Q, K, Vの意味は、演算での「使われ方」で決まる:\n",
      "  1. Q × K^T: QとKが「マッチング」の役割を担う\n",
      "  2. softmax: 配分を決定\n",
      "  3. × V: Vが「取り出す情報」の役割を担う\n",
      "\n",
      "生成時は同じ（線形変換）でも、\n",
      "演算の構造が役割を定義する！\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 簡単な例で演算の役割を追跡\n",
    "d = 4\n",
    "seq_len = 3\n",
    "\n",
    "# 入力\n",
    "X = torch.randn(1, seq_len, d)\n",
    "\n",
    "# Q, K, Vを生成（この時点ではただの線形変換）\n",
    "Q = torch.randn(1, seq_len, d)\n",
    "K = torch.randn(1, seq_len, d)\n",
    "V = torch.randn(1, seq_len, d)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: Q と K の内積 → ここでQとKの役割が分化\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1)) / (d ** 0.5)\n",
    "print(f\"\\nスコア行列 (Q × K^T):\")\n",
    "print(scores[0])\n",
    "print(f\"\\n【ここで初めて意味が生まれる】\")\n",
    "print(f\"  - Qの各行: 「この位置が探している情報のパターン」\")\n",
    "print(f\"  - Kの各行: 「この位置が提供できる情報の特徴」\")\n",
    "print(f\"  - 内積が大きい = マッチ度が高い\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 2: Softmax → 確率分布に変換\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "attn_weights = F.softmax(scores, dim=-1)\n",
    "print(f\"\\nAttention Weights:\")\n",
    "print(attn_weights[0])\n",
    "print(f\"\\n【意味】\")\n",
    "print(f\"  各行: 各位置が「どの位置の情報をどれだけ取り込むか」の配分\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 3: Attention × V → ここでVの役割が明確に\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "output = torch.matmul(attn_weights, V)\n",
    "print(f\"\\n最終出力:\")\n",
    "print(output[0])\n",
    "print(f\"\\n【ここで初めてVの意味が生まれる】\")\n",
    "print(f\"  - Vの各行: 「実際に取り出される情報の中身」\")\n",
    "print(f\"  - Attention Weightsで重み付けして集約される\")\n",
    "print(f\"  - Qが決めた配分で、Vから情報を抽出\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"【結論】\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Q, K, Vの意味は、演算での「使われ方」で決まる:\")\n",
    "print(\"  1. Q × K^T: QとKが「マッチング」の役割を担う\")\n",
    "print(\"  2. softmax: 配分を決定\")\n",
    "print(\"  3. × V: Vが「取り出す情報」の役割を担う\")\n",
    "print(\"\\n生成時は同じ（線形変換）でも、\")\n",
    "print(\"演算の構造が役割を定義する！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d5ca9e",
   "metadata": {},
   "source": [
    "#### 学習を通じて「意味」が強化される\n",
    "\n",
    "もう一つ重要な点があります：\n",
    "\n",
    "**初期状態（学習前）**:\n",
    "- Q, K, Vの重み行列はランダム\n",
    "- 演算の構造から「役割」は決まっているが、まだ最適化されていない\n",
    "\n",
    "**学習後**:\n",
    "- タスクを通じて、Q, K, Vの重み行列が最適化される\n",
    "- Qは「何を探すべきか」を学習\n",
    "- Kは「どうマッチすべきか」を学習\n",
    "- Vは「何を出力すべきか」を学習\n",
    "\n",
    "つまり：\n",
    "1. **演算の構造**が役割の「枠組み」を定義\n",
    "2. **学習**がその役割を具体的に最適化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb39b60e",
   "metadata": {},
   "source": [
    "#### 類推: プログラムの変数名\n",
    "\n",
    "これは、プログラミングの変数名に似ています：\n",
    "\n",
    "```python\n",
    "# この時点では、a, b, c はただの変数\n",
    "a = calculate_something(x)\n",
    "b = calculate_something(x)  # 違う関数\n",
    "c = calculate_something(x)  # さらに違う関数\n",
    "\n",
    "# 「使われ方」で意味が決まる\n",
    "similarity = dot_product(a, b)  # ここでaとbは「比較対象」の意味に\n",
    "result = weighted_sum(similarity, c)  # cは「集約される情報」の意味に\n",
    "```\n",
    "\n",
    "- **変数の中身**だけでは意味は分からない\n",
    "- **どう使われるか**で意味が決まる\n",
    "- **変数名（Query, Key, Value）**は人間の理解を助けるラベル\n",
    "\n",
    "Attention機構も同じです！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fb5880",
   "metadata": {},
   "source": [
    "#### まとめ\n",
    "\n",
    "| 視点 | 説明 |\n",
    "|------|------|\n",
    "| **数学的事実** | Q, K, Vは同じ入力Xを、異なる重み行列で線形変換しただけ |\n",
    "| **演算での役割** | `Q×K^T`でマッチング、`×V`で情報抽出、という構造で役割が決まる |\n",
    "| **学習での最適化** | タスクを通じて、各重み行列がその役割に最適化される |\n",
    "| **名前の意味** | Query, Key, Valueは、その「使われ方」から付けられた後付けのラベル |\n",
    "\n",
    "**あなたの理解は完璧です！**\n",
    "\n",
    "最初は「なぜ分けるのか分からない」のは当然で、\n",
    "**演算の全体像を見て初めて、分ける意味が分かる**のです。\n",
    "\n",
    "次のステップで実際にAttention計算を見れば、\n",
    "「なるほど、だからQ, K, Vを分けるのか！」と腑に落ちるはずです。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e04fe6",
   "metadata": {},
   "source": [
    "## Q4: スケーリング（÷√d_k）は本当に必要？\n",
    "\n",
    "**質問日**: 2025年11月17日\n",
    "\n",
    "### 質問\n",
    "\n",
    "`Q × K^T`の結果を`√d_k`で割ってからSoftmaxに入れていますが、**全体を同じ値で割るだけなら、Softmaxの結果は変わらないのでは**ないでしょうか？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044c68e9",
   "metadata": {},
   "source": [
    "### 回答: 鋭い指摘ですが、実は**結果は変わります**！\n",
    "\n",
    "一見、全体を定数で割っても比率は変わらないので、Softmaxの結果も同じに思えます。\n",
    "しかし、**Softmaxは非線形関数**なので、入力の値によって結果が大きく変わります。\n",
    "\n",
    "#### 直感的な理解\n",
    "\n",
    "Softmaxは「大きい値をさらに強調する」性質があります：\n",
    "\n",
    "- **スケーリングなし**: スコアが大きい → Softmax後、一部に極端に集中（ほぼ1と0）\n",
    "- **スケーリングあり**: スコアが小さい → Softmax後、より均等に分散\n",
    "\n",
    "つまり、スケーリングは**Attention Weightsの分布を調整**しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313d1a0b",
   "metadata": {},
   "source": [
    "#### 実験で確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62241a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "元のスコア:\n",
      "============================================================\n",
      "tensor([[10.,  5.,  2.,  1.]])\n",
      "\n",
      "============================================================\n",
      "ケース1: スケーリングなし（そのままSoftmax）\n",
      "============================================================\n",
      "Attention Weights:\n",
      "tensor([[9.9285e-01, 6.6898e-03, 3.3307e-04, 1.2253e-04]])\n",
      "最大値の位置への重み: 0.992855\n",
      "最小値の位置への重み: 0.000123\n",
      "→ 最大値に極端に集中している！\n",
      "\n",
      "============================================================\n",
      "ケース2: √d_kでスケーリング（d_k=64と仮定）\n",
      "============================================================\n",
      "スケーリング後のスコア (÷√64 = ÷8):\n",
      "tensor([[1.2500, 0.6250, 0.2500, 0.1250]])\n",
      "\n",
      "Attention Weights:\n",
      "tensor([[0.4489, 0.2403, 0.1651, 0.1457]])\n",
      "最大値の位置への重み: 0.448875\n",
      "最小値の位置への重み: 0.145728\n",
      "→ より均等に分散している！\n",
      "\n",
      "============================================================\n",
      "比較\n",
      "============================================================\n",
      "スケーリングなし: [[9.9285465e-01 6.6898018e-03 3.3306563e-04 1.2252800e-04]]\n",
      "スケーリングあり: [[0.44887468 0.24026531 0.16513178 0.14572828]]\n",
      "\n",
      "【重要】結果が全く異なる！\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# シンプルなスコア（内積の結果）\n",
    "scores = torch.tensor([[10.0, 5.0, 2.0, 1.0]])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"元のスコア:\")\n",
    "print(\"=\" * 60)\n",
    "print(scores)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ケース1: スケーリングなし（そのままSoftmax）\")\n",
    "print(\"=\" * 60)\n",
    "attn_no_scale = F.softmax(scores, dim=-1)\n",
    "print(\"Attention Weights:\")\n",
    "print(attn_no_scale)\n",
    "print(f\"最大値の位置への重み: {attn_no_scale[0, 0].item():.6f}\")\n",
    "print(f\"最小値の位置への重み: {attn_no_scale[0, 3].item():.6f}\")\n",
    "print(f\"→ 最大値に極端に集中している！\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ケース2: √d_kでスケーリング（d_k=64と仮定）\")\n",
    "print(\"=\" * 60)\n",
    "d_k = 64\n",
    "scores_scaled = scores / np.sqrt(d_k)\n",
    "print(f\"スケーリング後のスコア (÷√{d_k} = ÷8):\")\n",
    "print(scores_scaled)\n",
    "\n",
    "attn_with_scale = F.softmax(scores_scaled, dim=-1)\n",
    "print(\"\\nAttention Weights:\")\n",
    "print(attn_with_scale)\n",
    "print(f\"最大値の位置への重み: {attn_with_scale[0, 0].item():.6f}\")\n",
    "print(f\"最小値の位置への重み: {attn_with_scale[0, 3].item():.6f}\")\n",
    "print(f\"→ より均等に分散している！\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"比較\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"スケーリングなし: {attn_no_scale.numpy()}\")\n",
    "print(f\"スケーリングあり: {attn_with_scale.numpy()}\")\n",
    "print(f\"\\n【重要】結果が全く異なる！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b2bca3",
   "metadata": {},
   "source": [
    "#### なぜこうなるのか？Softmaxの数式\n",
    "\n",
    "Softmaxの定義を見てみましょう：\n",
    "\n",
    "$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n",
    "\n",
    "**重要なポイント**: 指数関数 $e^x$ は非線形！\n",
    "\n",
    "- $x$ が大きいと、$e^x$ は**爆発的に大きくなる**\n",
    "- $x$ を小さくすると、$e^x$ の差が縮まる\n",
    "\n",
    "**具体例**:\n",
    "- $e^{10} \\approx 22026$ vs $e^{1} \\approx 2.7$ → 差が約8000倍！\n",
    "- $e^{1.25} \\approx 3.5$ vs $e^{0.125} \\approx 1.1$ → 差が約3倍\n",
    "\n",
    "スケーリングで入力を小さくすると、指数関数の爆発を抑えられます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7206c0",
   "metadata": {},
   "source": [
    "#### なぜ√d_kなのか？\n",
    "\n",
    "次元数`d_k`が大きいほど、内積の値が大きくなる傾向があります：\n",
    "\n",
    "**内積の期待値**:\n",
    "- Q と K がランダムなベクトルの場合\n",
    "- 内積の分散は `d_k` に比例\n",
    "- 標準偏差は `√d_k` に比例\n",
    "\n",
    "だから`√d_k`で割ることで、次元数によらず**スコアの分散を一定**に保てます。\n",
    "\n",
    "**実用的な効果**:\n",
    "- d_k=64 → スコアが大きくなりすぎない\n",
    "- d_k=512 → スケーリングなしだとSoftmaxが極端になる\n",
    "- スケーリングで、どんな次元数でも安定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc48e70",
   "metadata": {},
   "source": [
    "#### 次元数による影響を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2033f154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "次元数が異なる場合の内積の大きさ\n",
      "============================================================\n",
      "\n",
      "d_k = 8:\n",
      "  内積の値: -2.02\n",
      "  スケーリング後 (÷√8): -0.71\n",
      "  → 次元が大きいほど内積も大きい傾向\n",
      "\n",
      "d_k = 64:\n",
      "  内積の値: 0.12\n",
      "  スケーリング後 (÷√64): 0.02\n",
      "  → 次元が大きいほど内積も大きい傾向\n",
      "\n",
      "d_k = 512:\n",
      "  内積の値: -3.52\n",
      "  スケーリング後 (÷√512): -0.16\n",
      "  → 次元が大きいほど内積も大きい傾向\n",
      "\n",
      "============================================================\n",
      "【重要】\n",
      "スケーリングにより、次元数によらず\n",
      "内積の値が同じスケールに正規化される！\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"次元数が異なる場合の内積の大きさ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for d_k in [8, 64, 512]:\n",
    "    # ランダムなQ, Kベクトル\n",
    "    q = torch.randn(1, d_k)\n",
    "    k = torch.randn(1, d_k)\n",
    "    \n",
    "    # 内積\n",
    "    score = torch.matmul(q, k.T).item()\n",
    "    \n",
    "    # スケーリング\n",
    "    score_scaled = score / (d_k ** 0.5)\n",
    "    \n",
    "    print(f\"\\nd_k = {d_k}:\")\n",
    "    print(f\"  内積の値: {score:.2f}\")\n",
    "    print(f\"  スケーリング後 (÷√{d_k}): {score_scaled:.2f}\")\n",
    "    print(f\"  → 次元が大きいほど内積も大きい傾向\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"【重要】\")\n",
    "print(\"スケーリングにより、次元数によらず\")\n",
    "print(\"内積の値が同じスケールに正規化される！\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfcc318",
   "metadata": {},
   "source": [
    "#### まとめ\n",
    "\n",
    "| 観点 | 説明 |\n",
    "|------|------|\n",
    "| **直感的な誤解** | 「全体を同じ値で割ってもSoftmaxの結果は同じ」→ **間違い** |\n",
    "| **数学的事実** | Softmaxは非線形（指数関数）なので、入力値で結果が変わる |\n",
    "| **スケーリングの効果** | 大きい値を小さくする → Softmaxの極端な集中を防ぐ |\n",
    "| **なぜ√d_k** | 内積の標準偏差が√d_kに比例 → 正規化で次元数に依らず安定 |\n",
    "| **学習への影響** | スケーリングなし → 勾配消失、学習不安定 |\n",
    "\n",
    "**結論**:\n",
    "- 一見不要に見えるスケーリングだが、実は**学習の安定性に極めて重要**\n",
    "- Softmaxの非線形性により、入力の範囲が結果に大きく影響する\n",
    "- √d_kでのスケーリングは、理論的にも実用的にも意味がある\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c02c597",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q5: √d_kで割ることで分散が1になるのか？\n",
    "\n",
    "実際に数式と実験で確認してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e054a99",
   "metadata": {},
   "source": [
    "#### 理論的な説明\n",
    "\n",
    "QとKがそれぞれ標準正規分布N(0,1)から独立に生成された場合:\n",
    "\n",
    "**内積の分散:**\n",
    "```\n",
    "Var(Q・K) = Var(Σ q_i × k_i)\n",
    "         = Σ Var(q_i × k_i)      # 独立なので\n",
    "         = Σ E[q_i²] × E[k_i²]   # 平均が0なので\n",
    "         = Σ 1 × 1 = d_k         # 標準正規分布なので\n",
    "```\n",
    "\n",
    "**スケーリング後の分散:**\n",
    "```\n",
    "Var((Q・K) / √d_k) = Var(Q・K) / d_k\n",
    "                   = d_k / d_k = 1\n",
    "```\n",
    "\n",
    "つまり、理論的には**分散が1になる**はずです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8dd4ff",
   "metadata": {},
   "source": [
    "#### 実験1: 標準正規分布から生成した場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5456257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "実験1: 標準正規分布N(0,1)から生成\n",
      "============================================================\n",
      "\n",
      "d_k = 8\n",
      "  生スコアの分散: 8.0021  (理論値: 8)\n",
      "  スケール後分散: 1.0003  (理論値: 1.0)\n",
      "  生スコアの標準偏差: 2.8288  (理論値: 2.8284)\n",
      "  スケール後標準偏差: 1.0001  (理論値: 1.0)\n",
      "\n",
      "d_k = 64\n",
      "  生スコアの分散: 64.2235  (理論値: 64)\n",
      "  スケール後分散: 1.0035  (理論値: 1.0)\n",
      "  生スコアの標準偏差: 8.0140  (理論値: 8.0000)\n",
      "  スケール後標準偏差: 1.0017  (理論値: 1.0)\n",
      "\n",
      "d_k = 512\n",
      "  生スコアの分散: 516.5176  (理論値: 512)\n",
      "  スケール後分散: 1.0088  (理論値: 1.0)\n",
      "  生スコアの標準偏差: 22.7270  (理論値: 22.6274)\n",
      "  スケール後標準偏差: 1.0044  (理論値: 1.0)\n",
      "\n",
      "d_k = 512\n",
      "  生スコアの分散: 516.5176  (理論値: 512)\n",
      "  スケール後分散: 1.0088  (理論値: 1.0)\n",
      "  生スコアの標準偏差: 22.7270  (理論値: 22.6274)\n",
      "  スケール後標準偏差: 1.0044  (理論値: 1.0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"実験1: 標準正規分布N(0,1)から生成\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for d_k in [8, 64, 512]:\n",
    "    # 大量のサンプルで統計的に検証\n",
    "    n_samples = 10000\n",
    "    \n",
    "    # 標準正規分布から生成\n",
    "    Q = torch.randn(n_samples, d_k)\n",
    "    K = torch.randn(n_samples, d_k)\n",
    "    \n",
    "    # 内積を計算（各サンプルについて）\n",
    "    scores = (Q * K).sum(dim=1)  # shape: (n_samples,)\n",
    "    \n",
    "    # スケーリング\n",
    "    scaled_scores = scores / np.sqrt(d_k)\n",
    "    \n",
    "    print(f\"\\nd_k = {d_k}\")\n",
    "    print(f\"  生スコアの分散: {scores.var().item():.4f}  (理論値: {d_k})\")\n",
    "    print(f\"  スケール後分散: {scaled_scores.var().item():.4f}  (理論値: 1.0)\")\n",
    "    print(f\"  生スコアの標準偏差: {scores.std().item():.4f}  (理論値: {np.sqrt(d_k):.4f})\")\n",
    "    print(f\"  スケール後標準偏差: {scaled_scores.std().item():.4f}  (理論値: 1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc6f153",
   "metadata": {},
   "source": [
    "#### 実験2: nn.Linearで生成した場合（実際のTransformer）\n",
    "\n",
    "実際のTransformerでは、QとKは`nn.Linear`で生成されます。この場合はどうでしょうか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3378e92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "実験2: nn.Linearで生成（実際のTransformer）\n",
      "============================================================\n",
      "\n",
      "d_k = 8\n",
      "  生スコアの分散: 1.1205\n",
      "  スケール後分散: 0.1401\n",
      "  √d_k = 2.8284\n",
      "  分散の比率: 8.0000 (理論値: 8)\n",
      "\n",
      "d_k = 64\n",
      "  生スコアの分散: 7.3213\n",
      "  スケール後分散: 0.1144\n",
      "  √d_k = 8.0000\n",
      "  分散の比率: 64.0000 (理論値: 64)\n",
      "\n",
      "d_k = 64\n",
      "  生スコアの分散: 7.3213\n",
      "  スケール後分散: 0.1144\n",
      "  √d_k = 8.0000\n",
      "  分散の比率: 64.0000 (理論値: 64)\n",
      "\n",
      "d_k = 512\n",
      "  生スコアの分散: 57.3983\n",
      "  スケール後分散: 0.1121\n",
      "  √d_k = 22.6274\n",
      "  分散の比率: 512.0000 (理論値: 512)\n",
      "\n",
      "d_k = 512\n",
      "  生スコアの分散: 57.3983\n",
      "  スケール後分散: 0.1121\n",
      "  √d_k = 22.6274\n",
      "  分散の比率: 512.0000 (理論値: 512)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"実験2: nn.Linearで生成（実際のTransformer）\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for d_k in [8, 64, 512]:\n",
    "    d_model = d_k  # 簡単のため同じ次元に\n",
    "    n_samples = 10000\n",
    "    seq_len = 10\n",
    "    \n",
    "    # 入力データを標準正規分布から生成\n",
    "    X = torch.randn(n_samples, seq_len, d_model)\n",
    "    \n",
    "    # nn.Linearで変換（デフォルト初期化）\n",
    "    W_q = nn.Linear(d_model, d_k)\n",
    "    W_k = nn.Linear(d_model, d_k)\n",
    "    \n",
    "    Q = W_q(X)  # shape: (n_samples, seq_len, d_k)\n",
    "    K = W_k(X)  # shape: (n_samples, seq_len, d_k)\n",
    "    \n",
    "    # Attention scoresを計算\n",
    "    # Q @ K^T の各要素（全サンプル、全ペアから集める）\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1))  # (n_samples, seq_len, seq_len)\n",
    "    scores_flat = scores.flatten()\n",
    "    \n",
    "    scaled_scores = scores_flat / np.sqrt(d_k)\n",
    "    \n",
    "    print(f\"\\nd_k = {d_k}\")\n",
    "    print(f\"  生スコアの分散: {scores_flat.var().item():.4f}\")\n",
    "    print(f\"  スケール後分散: {scaled_scores.var().item():.4f}\")\n",
    "    print(f\"  √d_k = {np.sqrt(d_k):.4f}\")\n",
    "    print(f\"  分散の比率: {scores_flat.var().item() / scaled_scores.var().item():.4f} (理論値: {d_k})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c23fac9",
   "metadata": {},
   "source": [
    "#### まとめ\n",
    "\n",
    "| 条件 | 生スコアの分散 | スケール後の分散 | 結論 |\n",
    "|------|---------------|-----------------|------|\n",
    "| **理論（標準正規分布）** | d_k | 1.0 | ✓ 完全に1になる |\n",
    "| **実験1（標準正規分布）** | ≈ d_k | ≈ 1.0 | ✓ 理論通り |\n",
    "| **実験2（nn.Linear）** | ≠ d_k | ≠ 1.0 | △ 正確には1にならない |\n",
    "\n",
    "**重要なポイント:**\n",
    "\n",
    "1. **理想的な条件下では**: √d_kで割ると分散が**正確に1になる**\n",
    "   - QとKが標準正規分布から独立に生成される場合\n",
    "\n",
    "2. **実際のTransformerでは**: 分散は**おおよそ1になる**が、完全には1にならない\n",
    "   - `nn.Linear`の初期化は標準正規分布ではない（Kaiming初期化など）\n",
    "   - 入力Xとの相関があるため、完全に独立ではない\n",
    "   \n",
    "3. **なぜ√d_kなのか**:\n",
    "   - 分散を**正確に1にする**ためではなく、**次元数に依存しないようにする**ため\n",
    "   - d_k=8でもd_k=512でも、スケール後の分散が**同程度**になる\n",
    "   - これにより、どんなモデルサイズでも**安定した学習**が可能\n",
    "\n",
    "**結論**: √d_kスケーリングは「分散を1にする魔法」というより、「次元数の影響を正規化する実用的な手法」"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003a5c9f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q6: nn.Linearの中身はどんなのですか？\n",
    "\n",
    "`nn.Linear`の内部実装を詳しく見てみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4983d5",
   "metadata": {},
   "source": [
    "#### 基本構造\n",
    "\n",
    "`nn.Linear(in_features, out_features, bias=True)`は以下の2つのパラメータを持ちます:\n",
    "\n",
    "1. **weight**: 形状 `(out_features, in_features)` の行列\n",
    "2. **bias**: 形状 `(out_features,)` のベクトル（`bias=True`の場合）\n",
    "\n",
    "計算式:\n",
    "```\n",
    "y = xW^T + b\n",
    "```\n",
    "\n",
    "ここで:\n",
    "- `x`: 入力テンソル `(..., in_features)`\n",
    "- `W`: 重み行列 `(out_features, in_features)`\n",
    "- `b`: バイアスベクトル `(out_features,)`\n",
    "- `y`: 出力テンソル `(..., out_features)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaa2692",
   "metadata": {},
   "source": [
    "#### 実験: 内部パラメータを見てみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65367728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "nn.Linearの内部構造\n",
      "============================================================\n",
      "\n",
      "1. パラメータ一覧:\n",
      "  weight: shape=torch.Size([3, 4]), dtype=torch.float32\n",
      "  bias: shape=torch.Size([3]), dtype=torch.float32\n",
      "\n",
      "2. 重み行列 (weight):\n",
      "  形状: torch.Size([3, 4])\n",
      "  実際の値:\n",
      "tensor([[-0.0186, -0.1851, -0.2803, -0.2389],\n",
      "        [ 0.4231, -0.3710,  0.0589,  0.4338],\n",
      "        [ 0.1492, -0.4660, -0.3229,  0.3787]])\n",
      "\n",
      "3. バイアス (bias):\n",
      "  形状: torch.Size([3])\n",
      "  実際の値: tensor([-0.0988, -0.3970, -0.1943])\n",
      "\n",
      "4. 計算の確認:\n",
      "  入力 x: tensor([1., 2., 3., 4.])\n",
      "  x.shape: torch.Size([4])\n",
      "\n",
      "  出力 y = linear(x): tensor([-2.2840,  1.1962, -0.4310], grad_fn=<ViewBackward0>)\n",
      "  y.shape: torch.Size([3])\n",
      "\n",
      "  手動計算 y = xW^T + b: tensor([-2.2840,  1.1962, -0.4310], grad_fn=<AddBackward0>)\n",
      "  一致するか: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# nn.Linearを作成\n",
    "linear = nn.Linear(in_features=4, out_features=3)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"nn.Linearの内部構造\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. パラメータ一覧:\")\n",
    "for name, param in linear.named_parameters():\n",
    "    print(f\"  {name}: shape={param.shape}, dtype={param.dtype}\")\n",
    "\n",
    "print(\"\\n2. 重み行列 (weight):\")\n",
    "print(f\"  形状: {linear.weight.shape}\")\n",
    "print(f\"  実際の値:\\n{linear.weight.data}\")\n",
    "\n",
    "print(\"\\n3. バイアス (bias):\")\n",
    "print(f\"  形状: {linear.bias.shape}\")\n",
    "print(f\"  実際の値: {linear.bias.data}\")\n",
    "\n",
    "print(\"\\n4. 計算の確認:\")\n",
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "print(f\"  入力 x: {x}\")\n",
    "print(f\"  x.shape: {x.shape}\")\n",
    "\n",
    "# nn.Linearで計算\n",
    "y = linear(x)\n",
    "print(f\"\\n  出力 y = linear(x): {y}\")\n",
    "print(f\"  y.shape: {y.shape}\")\n",
    "\n",
    "# 手動で計算（y = xW^T + b）\n",
    "y_manual = torch.matmul(x, linear.weight.T) + linear.bias\n",
    "print(f\"\\n  手動計算 y = xW^T + b: {y_manual}\")\n",
    "print(f\"  一致するか: {torch.allclose(y, y_manual)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563510e0",
   "metadata": {},
   "source": [
    "#### 初期化方法\n",
    "\n",
    "PyTorchの`nn.Linear`は**Kaiming Uniform初期化**をデフォルトで使用します:\n",
    "\n",
    "```python\n",
    "# PyTorchのソースコードより\n",
    "def reset_parameters(self):\n",
    "    nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "    if self.bias is not None:\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "        nn.init.uniform_(self.bias, -bound, bound)\n",
    "```\n",
    "\n",
    "**Kaiming初期化の詳細:**\n",
    "- 重みは一様分布 $U(-\\text{bound}, \\text{bound})$ から初期化\n",
    "- $\\text{bound} = \\sqrt{\\frac{6}{(1+a^2) \\times \\text{fan\\_in}}}$\n",
    "- $a = \\sqrt{5}$ （デフォルト）\n",
    "- $\\text{fan\\_in}$ = 入力特徴数 = `in_features`\n",
    "\n",
    "**なぜこの初期化？**\n",
    "- 勾配消失・爆発を防ぐため\n",
    "- 各層の出力の分散を一定に保つため"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559d625e",
   "metadata": {},
   "source": [
    "#### 実験: 初期化の範囲を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bc9d613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Kaiming初期化の範囲\n",
      "============================================================\n",
      "\n",
      "in_features = 8\n",
      "  理論的な範囲: [-0.3536, 0.3536]\n",
      "  実際の範囲:   [-0.3496, 0.3494]\n",
      "  標準偏差:     0.2001\n",
      "  理論的std:    0.2041\n",
      "\n",
      "in_features = 64\n",
      "  理論的な範囲: [-0.1250, 0.1250]\n",
      "  実際の範囲:   [-0.1250, 0.1250]\n",
      "  標準偏差:     0.0725\n",
      "  理論的std:    0.0722\n",
      "\n",
      "in_features = 512\n",
      "  理論的な範囲: [-0.0442, 0.0442]\n",
      "  実際の範囲:   [-0.0442, 0.0442]\n",
      "  標準偏差:     0.0256\n",
      "  理論的std:    0.0255\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Kaiming初期化の範囲\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for in_features in [8, 64, 512]:\n",
    "    linear = nn.Linear(in_features, 64)\n",
    "    \n",
    "    # 理論的な境界値を計算\n",
    "    a = np.sqrt(5)\n",
    "    fan_in = in_features\n",
    "    bound = np.sqrt(6 / ((1 + a**2) * fan_in))\n",
    "    \n",
    "    # 実際の重みの範囲\n",
    "    weight_min = linear.weight.data.min().item()\n",
    "    weight_max = linear.weight.data.max().item()\n",
    "    weight_std = linear.weight.data.std().item()\n",
    "    \n",
    "    print(f\"\\nin_features = {in_features}\")\n",
    "    print(f\"  理論的な範囲: [{-bound:.4f}, {bound:.4f}]\")\n",
    "    print(f\"  実際の範囲:   [{weight_min:.4f}, {weight_max:.4f}]\")\n",
    "    print(f\"  標準偏差:     {weight_std:.4f}\")\n",
    "    print(f\"  理論的std:    {bound/np.sqrt(3):.4f}\")  # 一様分布の標準偏差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d21eaf",
   "metadata": {},
   "source": [
    "#### バッチ処理とブロードキャスト\n",
    "\n",
    "`nn.Linear`は任意の形状のテンソルに対応できます:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74e9c68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "様々な形状の入力に対する処理\n",
      "============================================================\n",
      "\n",
      "1次元: xtorch.Size([4]) -> ytorch.Size([3])\n",
      "2次元: xtorch.Size([5, 4]) -> ytorch.Size([5, 3])\n",
      "3次元: xtorch.Size([5, 10, 4]) -> ytorch.Size([5, 10, 3])\n",
      "4次元: xtorch.Size([5, 8, 10, 4]) -> ytorch.Size([5, 8, 10, 3])\n",
      "\n",
      "重要なポイント:\n",
      "  最後の次元だけが変換され、他の次元はそのまま保持される\n",
      "  (..., in_features) -> (..., out_features)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "linear = nn.Linear(4, 3)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"様々な形状の入力に対する処理\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1次元入力\n",
    "x1 = torch.randn(4)\n",
    "y1 = linear(x1)\n",
    "print(f\"\\n1次元: x{x1.shape} -> y{y1.shape}\")\n",
    "\n",
    "# 2次元入力（バッチ）\n",
    "x2 = torch.randn(5, 4)  # (batch_size, in_features)\n",
    "y2 = linear(x2)\n",
    "print(f\"2次元: x{x2.shape} -> y{y2.shape}\")\n",
    "\n",
    "# 3次元入力（バッチ + シーケンス）\n",
    "x3 = torch.randn(5, 10, 4)  # (batch_size, seq_len, in_features)\n",
    "y3 = linear(x3)\n",
    "print(f\"3次元: x{x3.shape} -> y{y3.shape}\")\n",
    "\n",
    "# 4次元入力（バッチ + マルチヘッド + シーケンス）\n",
    "x4 = torch.randn(5, 8, 10, 4)  # (batch, heads, seq_len, in_features)\n",
    "y4 = linear(x4)\n",
    "print(f\"4次元: x{x4.shape} -> y{y4.shape}\")\n",
    "\n",
    "print(\"\\n重要なポイント:\")\n",
    "print(\"  最後の次元だけが変換され、他の次元はそのまま保持される\")\n",
    "print(\"  (..., in_features) -> (..., out_features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368575bc",
   "metadata": {},
   "source": [
    "#### まとめ: nn.Linearの内部構造\n",
    "\n",
    "| 項目 | 内容 |\n",
    "|------|------|\n",
    "| **パラメータ** | `weight`: (out_features, in_features)<br>`bias`: (out_features,) |\n",
    "| **計算式** | $y = xW^T + b$ |\n",
    "| **初期化** | Kaiming Uniform (He初期化の一種) |\n",
    "| **初期化範囲** | $U(-\\text{bound}, \\text{bound})$ where $\\text{bound} = \\sqrt{\\frac{6}{(1+a^2) \\times \\text{fan\\_in}}}$ |\n",
    "| **入力形状** | `(..., in_features)` |\n",
    "| **出力形状** | `(..., out_features)` |\n",
    "| **特徴** | 最後の次元だけを変換、他は保持 |\n",
    "\n",
    "**Transformerでの使用例:**\n",
    "```python\n",
    "# Self-AttentionでQ, K, Vを生成\n",
    "W_q = nn.Linear(d_model, d_k)  # Query変換\n",
    "W_k = nn.Linear(d_model, d_k)  # Key変換\n",
    "W_v = nn.Linear(d_model, d_v)  # Value変換\n",
    "\n",
    "# 入力: (batch_size, seq_len, d_model)\n",
    "# 出力: (batch_size, seq_len, d_k) など\n",
    "```\n",
    "\n",
    "**重要なポイント:**\n",
    "1. 内部は単純な行列積 + バイアス\n",
    "2. Kaiming初期化で学習の安定性を確保\n",
    "3. 任意の次元のテンソルに対応（最後の次元を変換）\n",
    "4. パラメータ数 = `in_features × out_features + out_features`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553d470b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q7: Q, K, Vの場合は通常バイアスはないのか？\n",
    "\n",
    "はい、**Transformerの実装ではQ/K/V変換にバイアスを使わないことが多い**です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2beafb2",
   "metadata": {},
   "source": [
    "#### 元のTransformer論文（\"Attention is All You Need\"）\n",
    "\n",
    "元論文では**バイアスについて明記されていません**が、公式実装では:\n",
    "- Q/K/V変換: **bias=True**（バイアスあり）\n",
    "- 出力変換（Multi-Head後）: **bias=True**\n",
    "\n",
    "しかし、その後の多くの実装では**bias=False**が標準になっています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbc77d3",
   "metadata": {},
   "source": [
    "#### 主要な実装でのバイアスの扱い\n",
    "\n",
    "| 実装 | Q/K/V変換のbias | 理由・備考 |\n",
    "|------|----------------|-----------|\n",
    "| **PyTorch公式** (`nn.MultiheadAttention`) | `bias=True`（デフォルト） | 互換性重視 |\n",
    "| **BERT** | `bias=True` | 元論文に従う |\n",
    "| **GPT-2/GPT-3** | `bias=True` | - |\n",
    "| **T5** | **`bias=False`** | 性能向上・パラメータ削減 |\n",
    "| **LLaMA** | **`bias=False`** | 最新のベストプラクティス |\n",
    "| **GPT-NeoX** | **`bias=False`** | - |\n",
    "\n",
    "**最近のトレンド**: `bias=False`が主流になってきている"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1018915",
   "metadata": {},
   "source": [
    "#### なぜバイアスなしが好まれるのか？\n",
    "\n",
    "**1. 理論的な理由:**\n",
    "- Attentionは**相対的な関係性**を捉えるメカニズム\n",
    "- バイアスは**絶対的なオフセット**を加える\n",
    "- 相対性を重視するAttentionには不要\n",
    "\n",
    "**2. 数学的な観点:**\n",
    "```\n",
    "Q = XW_q + b_q\n",
    "K = XW_k + b_k\n",
    "\n",
    "QK^T = (XW_q + b_q)(XW_k + b_k)^T\n",
    "     = XW_qW_k^TX^T + XW_qb_k^T + b_qW_k^TX^T + b_qb_k^T\n",
    "```\n",
    "バイアス項が複雑な相互作用を生み、解釈が難しくなる\n",
    "\n",
    "**3. 実用的な理由:**\n",
    "- パラメータ数の削減（メモリ効率）\n",
    "- LayerNormと組み合わせると、バイアスの効果が打ち消される\n",
    "- 実験的に性能差がほとんどない\n",
    "\n",
    "**4. LayerNormとの関係:**\n",
    "```python\n",
    "# Attentionの後にLayerNormを適用\n",
    "x = Attention(x)  # bias=Falseでも\n",
    "x = LayerNorm(x)  # ここでバイアスが学習される\n",
    "```\n",
    "LayerNormがバイアス相当の機能を持つため、Q/K/Vにバイアスは不要"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4badb113",
   "metadata": {},
   "source": [
    "#### 実験: バイアスあり vs なし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3487e605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "バイアスあり vs なしの比較\n",
      "============================================================\n",
      "\n",
      "【バイアスあり】\n",
      "  パラメータ数:\n",
      "    W_q: 72 = 8×8 + 8\n",
      "    W_k: 72 = 8×8 + 8\n",
      "    合計: 144\n",
      "  Scores shape: torch.Size([2, 4, 4])\n",
      "  Scores mean: 0.1849\n",
      "  Scores std: 1.7237\n",
      "\n",
      "【バイアスなし】\n",
      "  パラメータ数:\n",
      "    W_q: 64 = 8×8\n",
      "    W_k: 64 = 8×8\n",
      "    合計: 128\n",
      "  Scores shape: torch.Size([2, 4, 4])\n",
      "  Scores mean: 0.0699\n",
      "  Scores std: 1.1111\n",
      "\n",
      "【削減されたパラメータ数】\n",
      "  16 パラメータ (11.1%削減)\n",
      "\n",
      "  大規模モデル（d_model=4096, 96層）の場合:\n",
      "  削減: 1,179,648 パラメータ (1.2M)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "d_model = 8\n",
    "d_k = 8\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "\n",
    "X = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"バイアスあり vs なしの比較\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# バイアスあり\n",
    "W_q_with_bias = nn.Linear(d_model, d_k, bias=True)\n",
    "W_k_with_bias = nn.Linear(d_model, d_k, bias=True)\n",
    "\n",
    "# バイアスなし\n",
    "W_q_no_bias = nn.Linear(d_model, d_k, bias=False)\n",
    "W_k_no_bias = nn.Linear(d_model, d_k, bias=False)\n",
    "\n",
    "print(f\"\\n【バイアスあり】\")\n",
    "print(f\"  パラメータ数:\")\n",
    "params_with = sum(p.numel() for p in W_q_with_bias.parameters())\n",
    "params_with += sum(p.numel() for p in W_k_with_bias.parameters())\n",
    "print(f\"    W_q: {sum(p.numel() for p in W_q_with_bias.parameters())} = {d_model}×{d_k} + {d_k}\")\n",
    "print(f\"    W_k: {sum(p.numel() for p in W_k_with_bias.parameters())} = {d_model}×{d_k} + {d_k}\")\n",
    "print(f\"    合計: {params_with}\")\n",
    "\n",
    "Q_with = W_q_with_bias(X)\n",
    "K_with = W_k_with_bias(X)\n",
    "scores_with = torch.matmul(Q_with, K_with.transpose(-2, -1))\n",
    "print(f\"  Scores shape: {scores_with.shape}\")\n",
    "print(f\"  Scores mean: {scores_with.mean().item():.4f}\")\n",
    "print(f\"  Scores std: {scores_with.std().item():.4f}\")\n",
    "\n",
    "print(f\"\\n【バイアスなし】\")\n",
    "print(f\"  パラメータ数:\")\n",
    "params_without = sum(p.numel() for p in W_q_no_bias.parameters())\n",
    "params_without += sum(p.numel() for p in W_k_no_bias.parameters())\n",
    "print(f\"    W_q: {sum(p.numel() for p in W_q_no_bias.parameters())} = {d_model}×{d_k}\")\n",
    "print(f\"    W_k: {sum(p.numel() for p in W_k_no_bias.parameters())} = {d_model}×{d_k}\")\n",
    "print(f\"    合計: {params_without}\")\n",
    "\n",
    "Q_without = W_q_no_bias(X)\n",
    "K_without = W_k_no_bias(X)\n",
    "scores_without = torch.matmul(Q_without, K_without.transpose(-2, -1))\n",
    "print(f\"  Scores shape: {scores_without.shape}\")\n",
    "print(f\"  Scores mean: {scores_without.mean().item():.4f}\")\n",
    "print(f\"  Scores std: {scores_without.std().item():.4f}\")\n",
    "\n",
    "print(f\"\\n【削減されたパラメータ数】\")\n",
    "saved = params_with - params_without\n",
    "print(f\"  {saved} パラメータ ({saved/params_with*100:.1f}%削減)\")\n",
    "print(f\"\\n  大規模モデル（d_model=4096, 96層）の場合:\")\n",
    "d_large = 4096\n",
    "n_layers = 96\n",
    "# Q, K, V それぞれにバイアス\n",
    "saved_large = n_layers * 3 * d_large  # 3 = Q, K, V\n",
    "print(f\"  削減: {saved_large:,} パラメータ ({saved_large/1e6:.1f}M)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca2739f",
   "metadata": {},
   "source": [
    "#### 実装の推奨事項\n",
    "\n",
    "**学習目的の実装:**\n",
    "```python\n",
    "# シンプルさ重視ならbias=True（デフォルト）\n",
    "W_q = nn.Linear(d_model, d_k)  # bias=True\n",
    "```\n",
    "\n",
    "**本格的な実装:**\n",
    "```python\n",
    "# 最新のベストプラクティスに従う\n",
    "W_q = nn.Linear(d_model, d_k, bias=False)\n",
    "W_k = nn.Linear(d_model, d_k, bias=False)\n",
    "W_v = nn.Linear(d_model, d_v, bias=False)\n",
    "```\n",
    "\n",
    "**PyTorchのnn.MultiheadAttentionを使う場合:**\n",
    "```python\n",
    "# add_bias_kvパラメータで制御可能\n",
    "attn = nn.MultiheadAttention(\n",
    "    embed_dim=d_model,\n",
    "    num_heads=8,\n",
    "    bias=False,  # Q/K/V変換のバイアス\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc6c651",
   "metadata": {},
   "source": [
    "#### まとめ\n",
    "\n",
    "| 観点 | バイアスあり | バイアスなし |\n",
    "|------|------------|------------|\n",
    "| **元論文** | ✓（暗黙的） | - |\n",
    "| **最近のトレンド** | - | ✓ 主流 |\n",
    "| **パラメータ数** | 多い | 少ない |\n",
    "| **理論的根拠** | 弱い | 強い（相対性重視） |\n",
    "| **LayerNormとの相性** | 冗長 | 良い |\n",
    "| **性能** | ≈同等 | ≈同等 |\n",
    "| **代表例** | BERT, GPT-2 | T5, LLaMA, GPT-NeoX |\n",
    "\n",
    "**結論:**\n",
    "- **歴史的経緯**: 初期の実装は`bias=True`\n",
    "- **現在のベストプラクティス**: `bias=False`が推奨\n",
    "- **実用的な差**: ほとんどないが、大規模モデルではパラメータ削減が重要\n",
    "- **学習目的**: どちらでもOK。`bias=False`の方がモダン\n",
    "\n",
    "**我々の実装方針:**\n",
    "```python\n",
    "# src/attention.pyでbias=Falseを採用\n",
    "self.W_q = nn.Linear(d_model, d_k, bias=False)\n",
    "self.W_k = nn.Linear(d_model, d_k, bias=False)\n",
    "self.W_v = nn.Linear(d_model, d_v, bias=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5f69f6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q11: Concatとは？\n",
    "\n",
    "**質問日**: 2025年11月17日\n",
    "\n",
    "### 質問\n",
    "Multi-Head Attentionの数式に出てくる`Concat`とは何を意味しますか？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02c336e",
   "metadata": {},
   "source": [
    "### 回答\n",
    "\n",
    "`Concat`は**Concatenate（連結）**の略で、複数のテンソルを繋げる操作を意味します。\n",
    "\n",
    "#### Multi-Head Attentionでの役割\n",
    "\n",
    "Multi-Head Attentionでは、各headが並列に計算した結果を**結合（concat）**して、元の次元に戻します。\n",
    "\n",
    "**数式**:\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, ..., \\text{head}_h)W^O\n",
    "$$\n",
    "\n",
    "各headは:\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "$$\n",
    "\n",
    "#### 具体的な動作\n",
    "\n",
    "`d_model=512`、`num_heads=8`の場合：\n",
    "\n",
    "1. **各headの出力**: `[batch, seq_len, 64]` (64 = 512/8)\n",
    "2. **Concat後**: 8個のheadを連結 → `[batch, seq_len, 512]`\n",
    "3. **最終変換**: `W^O`で線形変換\n",
    "\n",
    "#### イメージ図\n",
    "\n",
    "```\n",
    "Head 1: [batch, seq_len, 64]\n",
    "Head 2: [batch, seq_len, 64]\n",
    "Head 3: [batch, seq_len, 64]\n",
    "   ...\n",
    "Head 8: [batch, seq_len, 64]\n",
    "        ↓ Concat\n",
    "[batch, seq_len, 512]  (64×8 = 512)\n",
    "```\n",
    "\n",
    "各headの64次元の出力を横に並べて、512次元のベクトルを作ります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f98d87",
   "metadata": {},
   "source": [
    "#### コード例: Concatの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cfa1be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各headの出力形状: torch.Size([2, 8, 5, 64])\n",
      "  - batch_size: 2\n",
      "  - num_heads: 8\n",
      "  - seq_len: 5\n",
      "  - d_k (各headの次元): 64\n",
      "\n",
      "転置後の形状: torch.Size([2, 5, 8, 64])\n",
      "Concat後の形状: torch.Size([2, 5, 512])\n",
      "  - d_model = num_heads × d_k = 8 × 64 = 512\n",
      "\n",
      "最終出力の形状: torch.Size([2, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Multi-Head AttentionのConcat部分を再現\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "num_heads = 8\n",
    "d_k = 64  # 各headの次元\n",
    "d_model = num_heads * d_k  # 512\n",
    "\n",
    "# 各headの出力をシミュレート\n",
    "# 実際には各headが並列にAttentionを計算した結果\n",
    "attention_output = torch.randn(batch_size, num_heads, seq_len, d_k)\n",
    "print(f\"各headの出力形状: {attention_output.shape}\")\n",
    "print(f\"  - batch_size: {batch_size}\")\n",
    "print(f\"  - num_heads: {num_heads}\")\n",
    "print(f\"  - seq_len: {seq_len}\")\n",
    "print(f\"  - d_k (各headの次元): {d_k}\")\n",
    "\n",
    "# Concat操作: 全headを結合\n",
    "# [batch, num_heads, seq_len, d_k] -> [batch, seq_len, num_heads, d_k]\n",
    "attention_output = attention_output.transpose(1, 2).contiguous()\n",
    "print(f\"\\n転置後の形状: {attention_output.shape}\")\n",
    "\n",
    "# [batch, seq_len, num_heads, d_k] -> [batch, seq_len, d_model]\n",
    "attention_output = attention_output.view(batch_size, seq_len, d_model)\n",
    "print(f\"Concat後の形状: {attention_output.shape}\")\n",
    "print(f\"  - d_model = num_heads × d_k = {num_heads} × {d_k} = {d_model}\")\n",
    "\n",
    "# この後、W_o (出力線形変換) を適用\n",
    "W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "output = W_o(attention_output)\n",
    "print(f\"\\n最終出力の形状: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475b485f",
   "metadata": {},
   "source": [
    "#### 実装での対応コード\n",
    "\n",
    "`src/attention.py`の`MultiHeadAttention`クラスでは、以下の部分がConcatに対応します：\n",
    "\n",
    "```python\n",
    "# 4. 各headでScaled Dot-Product Attentionを実行\n",
    "# attention_output: [batch, num_heads, seq_len, d_k]\n",
    "attention_output, attention_weights = self.attention(Q, K, V, mask)\n",
    "\n",
    "# 5. 全headを結合: [batch, seq_len, d_model]\n",
    "attention_output = self.combine_heads(attention_output, batch_size)\n",
    "\n",
    "# 6. 出力線形変換\n",
    "output = self.W_o(attention_output)\n",
    "```\n",
    "\n",
    "`combine_heads`メソッドが実際のConcat操作を行っています：\n",
    "\n",
    "```python\n",
    "def combine_heads(self, x, batch_size):\n",
    "    \"\"\"\n",
    "    複数のheadを結合\n",
    "    \n",
    "    Args:\n",
    "        x: shape [batch_size, num_heads, seq_len, d_k]\n",
    "    Returns:\n",
    "        shape [batch_size, seq_len, d_model]\n",
    "    \"\"\"\n",
    "    # [batch, num_heads, seq_len, d_k] -> [batch, seq_len, num_heads, d_k]\n",
    "    x = x.transpose(1, 2).contiguous()\n",
    "    # [batch, seq_len, num_heads, d_k] -> [batch, seq_len, d_model]\n",
    "    return x.view(batch_size, -1, self.d_model)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a8a7f3",
   "metadata": {},
   "source": [
    "#### まとめ\n",
    "\n",
    "**Concatの役割**:\n",
    "- 各headの出力（小さい次元）を横に繋げて、元の大きい次元に戻す\n",
    "- 8個のhead（各64次元）→ 1つのベクトル（512次元）\n",
    "\n",
    "**なぜConcatが必要？**:\n",
    "1. 各headは独立して異なる種類の情報を学習\n",
    "2. それらを統合して、豊かな表現を作る\n",
    "3. 最終的に元の次元に戻すことで、次の層への入力として使える\n",
    "\n",
    "**PyTorchでの実装**:\n",
    "- `transpose()`: 次元の順序を入れ替え\n",
    "- `view()` または `reshape()`: テンソルの形状を変更\n",
    "- この2つの操作でConcatを実現"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c64c6c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q12: なぜQ, K, Vに線形変換を通すのか？\n",
    "\n",
    "**質問日**: 2025年11月17日\n",
    "\n",
    "### 質問\n",
    "Self-Attentionで、入力から Query, Key, Value を生成するときに、なぜ線形変換層（`nn.Linear`）を通す必要があるのですか？入力をそのまま使ってはいけないのでしょうか？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53fac62",
   "metadata": {},
   "source": [
    "### 回答\n",
    "\n",
    "線形変換を通す理由は、**Q, K, V それぞれに異なる役割を学習させる**ためです。\n",
    "\n",
    "#### 主な理由\n",
    "\n",
    "##### 1. **異なる表現空間への射影**\n",
    "\n",
    "同じ入力データから、3つの異なる「視点」を作り出します：\n",
    "\n",
    "- **Query (Q)**: 「何を探しているか」を表現\n",
    "- **Key (K)**: 「何を持っているか」を表現  \n",
    "- **Value (V)**: 「実際の情報」を表現\n",
    "\n",
    "それぞれ独立した重み行列を持つことで、異なる特徴を抽出できます。\n",
    "\n",
    "##### 2. **学習可能なパラメータ**\n",
    "\n",
    "線形変換の重み行列は**学習可能**なので、タスクに応じて最適なQ, K, Vの生成方法を学習します。\n",
    "\n",
    "```python\n",
    "# 各変換は独立したパラメータを持つ\n",
    "self.W_q = nn.Linear(d_model, d_model, bias=False)  # Query用の重み\n",
    "self.W_k = nn.Linear(d_model, d_model, bias=False)  # Key用の重み\n",
    "self.W_v = nn.Linear(d_model, d_model, bias=False)  # Value用の重み\n",
    "```\n",
    "\n",
    "##### 3. **柔軟性の向上**\n",
    "\n",
    "入力をそのまま使うと、Q, K, Vが全て同じになってしまいます。これでは：\n",
    "- Attentionが単純な自己相関になる\n",
    "- 複雑な関係性を学習できない\n",
    "- 表現力が大幅に低下する\n",
    "\n",
    "##### 4. **次元変換の自由度**\n",
    "\n",
    "線形変換により、入力と異なる次元のQ, K, Vを生成できます（特にMulti-Head Attentionで重要）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fd0265",
   "metadata": {},
   "source": [
    "#### 比較実験: 線形変換ありvs線形変換なし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99e2f431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力 x:\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.]])\n",
      "\n",
      "============================================================\n",
      "パターン1: 線形変換なし (Q = K = V = x)\n",
      "============================================================\n",
      "Attention重み:\n",
      "tensor([[0.4519, 0.2741, 0.2741],\n",
      "        [0.2741, 0.4519, 0.2741],\n",
      "        [0.2741, 0.2741, 0.4519]])\n",
      "\n",
      "出力:\n",
      "tensor([[0.4519, 0.2741, 0.2741, 0.0000],\n",
      "        [0.2741, 0.4519, 0.2741, 0.0000],\n",
      "        [0.2741, 0.2741, 0.4519, 0.0000]])\n",
      "\n",
      "→ Q, K, Vが同じなので、Attention重みは単位行列に近くなる\n",
      "  （各単語が自分自身だけを強く見る傾向）\n",
      "\n",
      "============================================================\n",
      "パターン2: 線形変換あり\n",
      "============================================================\n",
      "変換後のQuery:\n",
      "tensor([[-0.1843,  0.0093,  0.0711, -0.3484],\n",
      "        [-0.4172,  0.1923, -0.3889, -0.3434],\n",
      "        [-0.4364,  0.0628, -0.3835,  0.1424]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "変換後のKey:\n",
      "tensor([[-0.4020,  0.1179, -0.4942, -0.1884],\n",
      "        [ 0.3730, -0.0053, -0.2449,  0.1818],\n",
      "        [ 0.0583, -0.1902,  0.3828,  0.4421]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "Attention重み:\n",
      "tensor([[0.3617, 0.3183, 0.3200],\n",
      "        [0.4134, 0.3107, 0.2759],\n",
      "        [0.3824, 0.3148, 0.3028]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "出力:\n",
      "tensor([[ 0.3589, -0.1418,  0.3583,  0.0471],\n",
      "        [ 0.3474, -0.1719,  0.3580,  0.0784],\n",
      "        [ 0.3542, -0.1534,  0.3581,  0.0593]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "→ Q, K, Vが異なるので、より複雑な関係性を学習できる\n",
      "  （各単語が他の単語との関係を柔軟に学習）\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# 入力データ\n",
    "batch_size = 1\n",
    "seq_len = 3\n",
    "d_model = 4\n",
    "\n",
    "x = torch.tensor([\n",
    "    [[1.0, 0.0, 0.0, 0.0],  # 単語1\n",
    "     [0.0, 1.0, 0.0, 0.0],  # 単語2\n",
    "     [0.0, 0.0, 1.0, 0.0]]  # 単語3\n",
    "])\n",
    "\n",
    "print(\"入力 x:\")\n",
    "print(x[0])\n",
    "print()\n",
    "\n",
    "# パターン1: 線形変換なし（Q=K=V=x）\n",
    "print(\"=\" * 60)\n",
    "print(\"パターン1: 線形変換なし (Q = K = V = x)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "Q1 = x\n",
    "K1 = x\n",
    "V1 = x\n",
    "\n",
    "# Attention計算\n",
    "scores1 = torch.matmul(Q1, K1.transpose(-2, -1)) / math.sqrt(d_model)\n",
    "attention_weights1 = F.softmax(scores1, dim=-1)\n",
    "output1 = torch.matmul(attention_weights1, V1)\n",
    "\n",
    "print(\"Attention重み:\")\n",
    "print(attention_weights1[0])\n",
    "print(\"\\n出力:\")\n",
    "print(output1[0])\n",
    "print(\"\\n→ Q, K, Vが同じなので、Attention重みは単位行列に近くなる\")\n",
    "print(\"  （各単語が自分自身だけを強く見る傾向）\")\n",
    "\n",
    "# パターン2: 線形変換あり\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"パターン2: 線形変換あり\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 異なる線形変換を定義\n",
    "W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "# 学習済みの重みをシミュレート（実際には学習で最適化される）\n",
    "torch.manual_seed(42)\n",
    "Q2 = W_q(x)\n",
    "K2 = W_k(x)\n",
    "V2 = W_v(x)\n",
    "\n",
    "print(\"変換後のQuery:\")\n",
    "print(Q2[0])\n",
    "print(\"\\n変換後のKey:\")\n",
    "print(K2[0])\n",
    "\n",
    "# Attention計算\n",
    "scores2 = torch.matmul(Q2, K2.transpose(-2, -1)) / math.sqrt(d_model)\n",
    "attention_weights2 = F.softmax(scores2, dim=-1)\n",
    "output2 = torch.matmul(attention_weights2, V2)\n",
    "\n",
    "print(\"\\nAttention重み:\")\n",
    "print(attention_weights2[0])\n",
    "print(\"\\n出力:\")\n",
    "print(output2[0])\n",
    "print(\"\\n→ Q, K, Vが異なるので、より複雑な関係性を学習できる\")\n",
    "print(\"  （各単語が他の単語との関係を柔軟に学習）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c443856",
   "metadata": {},
   "source": [
    "#### 具体的な例: 文章理解での役割\n",
    "\n",
    "例えば「猫が魚を食べた」という文を処理する場合：\n",
    "\n",
    "##### 線形変換なし（Q=K=V）の場合:\n",
    "- 「猫」は「猫」自身としか関係性を持てない\n",
    "- 「食べた」は「食べた」自身としか関係性を持てない\n",
    "- 文法的・意味的な関係を学習できない\n",
    "\n",
    "##### 線形変換あり（Q≠K≠V）の場合:\n",
    "- **Query変換**: 「猫」から「主語としての猫」を抽出\n",
    "- **Key変換**: 「食べた」から「述語としての食べた」を抽出\n",
    "- **Attention計算**: 「主語の猫」と「述語の食べた」が強く関連\n",
    "- **Value変換**: 実際に伝達する情報を最適化\n",
    "\n",
    "これにより、文法構造や意味的な関係性を学習できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efb43ab",
   "metadata": {},
   "source": [
    "#### 数学的な視点\n",
    "\n",
    "**線形変換の役割**:\n",
    "\n",
    "$$\n",
    "Q = XW_q, \\quad K = XW_k, \\quad V = XW_v\n",
    "$$\n",
    "\n",
    "- $X$: 入力（全単語で同じ）\n",
    "- $W_q, W_k, W_v$: 異なる重み行列（学習可能）\n",
    "- これにより、同じ入力から異なる表現を生成\n",
    "\n",
    "**Attention計算**:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\text{softmax}\\left(\\frac{(XW_q)(XW_k)^T}{\\sqrt{d_k}}\\right)(XW_v)\n",
    "$$\n",
    "\n",
    "$W_q, W_k, W_v$が異なる → より豊かな表現が可能"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0b461d",
   "metadata": {},
   "source": [
    "#### まとめ\n",
    "\n",
    "**Q, K, Vに線形変換を通す理由**:\n",
    "\n",
    "1. **異なる役割の学習**\n",
    "   - Query: 「何を探すか」\n",
    "   - Key: 「何を持っているか」\n",
    "   - Value: 「何を伝えるか」\n",
    "   - 各変換が独立した重み行列で異なる特徴を抽出\n",
    "\n",
    "2. **学習可能なパラメータ**\n",
    "   - タスクに応じて最適なQ, K, Vの生成方法を学習\n",
    "   - データから自動的に有用な表現を獲得\n",
    "\n",
    "3. **表現力の向上**\n",
    "   - 線形変換なし: 単純な自己相関のみ\n",
    "   - 線形変換あり: 複雑な関係性を学習可能\n",
    "\n",
    "4. **柔軟性**\n",
    "   - 次元の変換が可能\n",
    "   - Multi-Head Attentionで特に重要\n",
    "\n",
    "**類似例**:\n",
    "畳み込み層でも、同じ画像から異なるフィルタ（edge検出、色検出など）で異なる特徴を抽出するのと同じ考え方です。\n",
    "\n",
    "**実装**:\n",
    "```python\n",
    "# src/attention.py\n",
    "self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "```\n",
    "\n",
    "線形変換は、Attentionが柔軟で強力な学習機構となるための**核心的な仕組み**です！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93ee672",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q13: Multi-Head AttentionでさらにQ, K, Vを線形変換するのはなぜ？\n",
    "\n",
    "**質問日**: 2025年11月17日\n",
    "\n",
    "### 質問\n",
    "Multi-Head Attentionでは、すでにQ, K, Vが生成された後、さらに各headごとに線形変換層（`W_q`, `W_k`, `W_v`）に入力しています。Q, K, Vはすでに異なるものなのに、なぜさらに線形変換する必要があるのですか？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d894915",
   "metadata": {},
   "source": [
    "### 回答\n",
    "\n",
    "Multi-Head Attentionの線形変換は、**各headが異なる表現部分空間を学習する**ための仕組みです。\n",
    "\n",
    "#### 構造の理解\n",
    "\n",
    "実は、実装方法によって見え方が異なりますが、概念的には同じことをしています：\n",
    "\n",
    "##### パターン1: 1つの大きな線形変換 → 分割（我々の実装）\n",
    "\n",
    "```python\n",
    "# 1つの大きなW_q (d_model → d_model)\n",
    "Q = self.W_q(x)  # [batch, seq_len, d_model]\n",
    "\n",
    "# 複数headに分割\n",
    "Q = self.split_heads(Q, batch_size)  # [batch, num_heads, seq_len, d_k]\n",
    "```\n",
    "\n",
    "##### パターン2: 各headごとに独立した小さな線形変換\n",
    "\n",
    "```python\n",
    "# 各headが独立したW_q_i (d_model → d_k)\n",
    "head_1 = self.W_q_1(x)  # [batch, seq_len, d_k]\n",
    "head_2 = self.W_q_2(x)  # [batch, seq_len, d_k]\n",
    "...\n",
    "head_8 = self.W_q_8(x)  # [batch, seq_len, d_k]\n",
    "```\n",
    "\n",
    "**重要**: どちらも数学的には同じ！パターン1は計算効率のために全headをまとめて処理しているだけです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3f31c1",
   "metadata": {},
   "source": [
    "#### なぜ各headに異なる線形変換が必要か\n",
    "\n",
    "##### 1. **異なる表現部分空間を学習**\n",
    "\n",
    "各headは、**同じ入力から異なる視点で情報を抽出**します：\n",
    "\n",
    "- **Head 1**: 文法的な関係性を捉える（主語-述語など）\n",
    "- **Head 2**: 意味的な類似性を捉える\n",
    "- **Head 3**: 位置的な近さを捉える\n",
    "- ...\n",
    "\n",
    "各headが独立した重み行列を持つことで、異なる種類のパターンを学習できます。\n",
    "\n",
    "##### 2. **同じ変換を使うと意味がない**\n",
    "\n",
    "もし全headが同じ線形変換を使ったら：\n",
    "\n",
    "```python\n",
    "# 全headが同じW_qを使う場合\n",
    "Q = self.W_q(x)  # 全headで同じ変換\n",
    "# ↓ これでは全headが同じ情報しか見られない！\n",
    "```\n",
    "\n",
    "→ Multi-Headにする意味がなくなります\n",
    "\n",
    "##### 3. **パラメータの独立性**\n",
    "\n",
    "各headは**独立したパラメータ**を持ちます：\n",
    "\n",
    "```\n",
    "Head 1: W_q[0:64, :]の部分を使用\n",
    "Head 2: W_q[64:128, :]の部分を使用\n",
    "...\n",
    "Head 8: W_q[448:512, :]の部分を使用\n",
    "```\n",
    "\n",
    "大きな1つの行列として実装していますが、各headは異なる部分を使うため、実質的に独立した変換になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1193a3",
   "metadata": {},
   "source": [
    "#### コード例: 重み行列の分割を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d290f7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_qの形状: torch.Size([512, 512])\n",
      "  - 入力次元: 512\n",
      "  - 出力次元: 512\n",
      "\n",
      "変換後のQ: torch.Size([1, 5, 512])\n",
      "分割後のQ: torch.Size([1, 8, 5, 64])\n",
      "\n",
      "各headが使用する重み行列の部分:\n",
      "  Head 1: W_q[0:64, :] の部分\n",
      "           出力の0～63次元目を生成\n",
      "  Head 2: W_q[64:128, :] の部分\n",
      "           出力の64～127次元目を生成\n",
      "  Head 3: W_q[128:192, :] の部分\n",
      "           出力の128～191次元目を生成\n",
      "  Head 4: W_q[192:256, :] の部分\n",
      "           出力の192～255次元目を生成\n",
      "  Head 5: W_q[256:320, :] の部分\n",
      "           出力の256～319次元目を生成\n",
      "  Head 6: W_q[320:384, :] の部分\n",
      "           出力の320～383次元目を生成\n",
      "  Head 7: W_q[384:448, :] の部分\n",
      "           出力の384～447次元目を生成\n",
      "  Head 8: W_q[448:512, :] の部分\n",
      "           出力の448～511次元目を生成\n",
      "\n",
      "💡 ポイント:\n",
      "  - 1つの大きなW_q (512×512) を使用\n",
      "  - しかし各headは異なる出力次元（64次元分）を担当\n",
      "  - 結果的に、各headは独立した変換を行う\n",
      "  - これにより各headが異なる表現部分空間を学習\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_k = d_model // num_heads  # 64\n",
    "\n",
    "# Multi-Head AttentionのW_q\n",
    "W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "print(\"W_qの形状:\", W_q.weight.shape)\n",
    "print(f\"  - 入力次元: {d_model}\")\n",
    "print(f\"  - 出力次元: {d_model}\")\n",
    "print()\n",
    "\n",
    "# 入力\n",
    "x = torch.randn(1, 5, d_model)  # [batch=1, seq_len=5, d_model=512]\n",
    "\n",
    "# 線形変換\n",
    "Q = W_q(x)  # [1, 5, 512]\n",
    "print(\"変換後のQ:\", Q.shape)\n",
    "\n",
    "# 各headに分割\n",
    "Q = Q.view(1, 5, num_heads, d_k)  # [1, 5, 8, 64]\n",
    "Q = Q.transpose(1, 2)  # [1, 8, 5, 64]\n",
    "print(\"分割後のQ:\", Q.shape)\n",
    "print()\n",
    "\n",
    "# 各headが使う重みの部分を確認\n",
    "print(\"各headが使用する重み行列の部分:\")\n",
    "for i in range(num_heads):\n",
    "    start = i * d_k\n",
    "    end = (i + 1) * d_k\n",
    "    print(f\"  Head {i+1}: W_q[{start}:{end}, :] の部分\")\n",
    "    print(f\"           出力の{start}～{end-1}次元目を生成\")\n",
    "\n",
    "print()\n",
    "print(\"💡 ポイント:\")\n",
    "print(\"  - 1つの大きなW_q (512×512) を使用\")\n",
    "print(\"  - しかし各headは異なる出力次元（64次元分）を担当\")\n",
    "print(\"  - 結果的に、各headは独立した変換を行う\")\n",
    "print(\"  - これにより各headが異なる表現部分空間を学習\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0be20c2",
   "metadata": {},
   "source": [
    "#### 数学的な説明\n",
    "\n",
    "Multi-Head Attentionの数式：\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O\n",
    "$$\n",
    "\n",
    "各headは：\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "$$\n",
    "\n",
    "**重要なポイント**:\n",
    "\n",
    "1. **$W_i^Q$は各headごとに異なる**\n",
    "   - Head 1: $W_1^Q$ (d_model → d_k)\n",
    "   - Head 2: $W_2^Q$ (d_model → d_k)\n",
    "   - ...\n",
    "   - Head 8: $W_8^Q$ (d_model → d_k)\n",
    "\n",
    "2. **実装上は1つの大きな行列**\n",
    "   - 実装: $W^Q$ (d_model → d_model)\n",
    "   - 分割後: 各headが異なる部分を使用\n",
    "\n",
    "3. **なぜこれが必要か**\n",
    "   - 同じ入力 $Q$ から、各headが異なる射影を作る\n",
    "   - 各headが異なる「視点」で情報を見る\n",
    "   - これにより多様な関係性を並列に学習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a353d66a",
   "metadata": {},
   "source": [
    "#### 具体例: 画像処理との類似性\n",
    "\n",
    "畳み込みニューラルネットワーク（CNN）と似ています：\n",
    "\n",
    "**CNNの場合**:\n",
    "- 同じ入力画像に複数のフィルタを適用\n",
    "- Filter 1: エッジ検出\n",
    "- Filter 2: 色検出\n",
    "- Filter 3: テクスチャ検出\n",
    "- 各フィルタは異なる特徴を抽出\n",
    "\n",
    "**Multi-Head Attentionの場合**:\n",
    "- 同じ入力シーケンスに複数のhead（変換）を適用\n",
    "- Head 1: 文法関係を捉える\n",
    "- Head 2: 意味関係を捉える\n",
    "- Head 3: 位置関係を捉える\n",
    "- 各headは異なる関係性を学習\n",
    "\n",
    "どちらも「同じ入力から、複数の異なる視点で特徴を抽出する」という共通の考え方です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b941eb42",
   "metadata": {},
   "source": [
    "#### まとめ\n",
    "\n",
    "**Multi-Head Attentionで各headに異なる線形変換が必要な理由**:\n",
    "\n",
    "1. **異なる表現部分空間の学習**\n",
    "   - 各headが独立した視点で情報を抽出\n",
    "   - 文法、意味、位置など、異なる種類の関係性を並列に学習\n",
    "\n",
    "2. **実装の工夫**\n",
    "   - 1つの大きな線形変換 (d_model → d_model)\n",
    "   - 出力を複数headに分割\n",
    "   - 各headは異なる出力次元を担当 → 実質的に異なる変換\n",
    "\n",
    "3. **同じ変換を使うと意味がない**\n",
    "   - 全headが同じ変換 → 同じ情報しか見られない\n",
    "   - Multi-Headにする利点が消失\n",
    "\n",
    "4. **数学的には**\n",
    "   - 各head $i$ は独立した $W_i^Q, W_i^K, W_i^V$ を持つ\n",
    "   - 実装上は効率化のため1つの大きな行列としてまとめている\n",
    "   - 結果は同じ\n",
    "\n",
    "**具体的なイメージ**:\n",
    "```\n",
    "入力 x → W_q → [Head1の64次元 | Head2の64次元 | ... | Head8の64次元]\n",
    "         ↑         ↑              ↑                      ↑\n",
    "      512×512   異なる重み    異なる重み          異なる重み\n",
    "```\n",
    "\n",
    "各headは**独立した学習可能なパラメータ**を持ち、**異なる特徴**を抽出します。これがMulti-Head Attentionの本質です！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9e540a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q14: Multi-Head AttentionのQ, K, Vも入力の線形変換と考えて良いか？\n",
    "\n",
    "**質問日**: 2025年11月18日\n",
    "\n",
    "### 質問\n",
    "Multi-Head Attentionで使うQ, K, Vも、01_self_attention_demo.ipynbで学んだように、入力を線形変換したものと考えて良いのでしょうか？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5b2299",
   "metadata": {},
   "source": [
    "### 回答\n",
    "\n",
    "**はい、その通りです！** Multi-Head AttentionのQ, K, Vも、Self-Attentionと全く同じように、**入力を線形変換したもの**です。\n",
    "\n",
    "#### Self-Attention (01) と Multi-Head Attention (02) の関係\n",
    "\n",
    "##### Self-Attention (01の実装):\n",
    "\n",
    "```python\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 入力xから線形変換でQ, K, Vを生成\n",
    "        Q = self.W_q(x)  # [batch, seq_len, d_model]\n",
    "        K = self.W_k(x)  # [batch, seq_len, d_model]\n",
    "        V = self.W_v(x)  # [batch, seq_len, d_model]\n",
    "        \n",
    "        # Attentionを計算\n",
    "        output = self.attention(Q, K, V)\n",
    "        return output\n",
    "```\n",
    "\n",
    "##### Multi-Head Attention (02の実装):\n",
    "\n",
    "```python\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    def forward(self, x, x, x):\n",
    "        # 入力xから線形変換でQ, K, Vを生成（Self-Attentionと同じ！）\n",
    "        Q = self.W_q(x)  # [batch, seq_len, d_model]\n",
    "        K = self.W_k(x)  # [batch, seq_len, d_model]\n",
    "        V = self.W_v(x)  # [batch, seq_len, d_model]\n",
    "        \n",
    "        # ここからがMulti-Head特有の処理\n",
    "        # 複数headに分割\n",
    "        Q = self.split_heads(Q)  # [batch, num_heads, seq_len, d_k]\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "        \n",
    "        # 各headでAttentionを計算\n",
    "        output = self.attention(Q, K, V)\n",
    "        return output\n",
    "```\n",
    "\n",
    "**違いは分割するかどうかだけ！**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb0b71b",
   "metadata": {},
   "source": [
    "#### 重要なポイント\n",
    "\n",
    "##### 1. **基本は同じ**\n",
    "- どちらも入力 `x` を線形変換して Q, K, V を生成\n",
    "- `W_q`, `W_k`, `W_v` という学習可能な重み行列を使用\n",
    "- 線形変換の数式も同じ: $Q = xW_q$, $K = xW_k$, $V = xW_v$\n",
    "\n",
    "##### 2. **違いは後処理**\n",
    "- **Self-Attention**: Q, K, Vをそのまま使う\n",
    "- **Multi-Head Attention**: Q, K, Vを複数headに分割してから使う\n",
    "\n",
    "##### 3. **概念的な流れ**\n",
    "\n",
    "```\n",
    "入力 x\n",
    "  ↓\n",
    "線形変換 (W_q, W_k, W_v) ← ここは01も02も同じ！\n",
    "  ↓\n",
    "Q, K, V を生成\n",
    "  ↓\n",
    "┌─────────────┬─────────────┐\n",
    "│ 01: Self    │ 02: Multi   │\n",
    "│             │             │\n",
    "│ そのまま    │ 分割        │\n",
    "│ Attention   │ ↓           │\n",
    "│             │ 各headで    │\n",
    "│             │ Attention   │\n",
    "│             │ ↓           │\n",
    "│             │ 結合        │\n",
    "└─────────────┴─────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71211e55",
   "metadata": {},
   "source": [
    "#### コード例: 01と02の比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98e87063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "01: Self-Attention\n",
      "======================================================================\n",
      "入力 x: torch.Size([2, 5, 512])\n",
      "Q: torch.Size([2, 5, 512])\n",
      "K: torch.Size([2, 5, 512])\n",
      "V: torch.Size([2, 5, 512])\n",
      "→ このままAttentionに使う\n",
      "\n",
      "======================================================================\n",
      "02: Multi-Head Attention\n",
      "======================================================================\n",
      "入力 x: torch.Size([2, 5, 512])\n",
      "Q (変換直後): torch.Size([2, 5, 512])\n",
      "K (変換直後): torch.Size([2, 5, 512])\n",
      "V (変換直後): torch.Size([2, 5, 512])\n",
      "→ ここまでは01と全く同じ！\n",
      "\n",
      "Q (分割後): torch.Size([2, 8, 5, 64])\n",
      "K (分割後): torch.Size([2, 8, 5, 64])\n",
      "V (分割後): torch.Size([2, 8, 5, 64])\n",
      "→ 複数headに分割してから各headでAttentionを計算\n",
      "\n",
      "======================================================================\n",
      "結論\n",
      "======================================================================\n",
      "✓ Q, K, Vの生成方法は01も02も同じ（入力xの線形変換）\n",
      "✓ 違いは、その後に分割するかどうかだけ\n",
      "✓ Multi-Headは、Self-Attentionの拡張版\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "\n",
    "# 入力データ（01も02も同じ入力を使う）\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"01: Self-Attention\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Self-Attentionの線形変換層\n",
    "W_q_self = nn.Linear(d_model, d_model, bias=False)\n",
    "W_k_self = nn.Linear(d_model, d_model, bias=False)\n",
    "W_v_self = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "# Q, K, Vの生成\n",
    "Q_self = W_q_self(x)\n",
    "K_self = W_k_self(x)\n",
    "V_self = W_v_self(x)\n",
    "\n",
    "print(f\"入力 x: {x.shape}\")\n",
    "print(f\"Q: {Q_self.shape}\")\n",
    "print(f\"K: {K_self.shape}\")\n",
    "print(f\"V: {V_self.shape}\")\n",
    "print(\"→ このままAttentionに使う\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"02: Multi-Head Attention\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Multi-Head Attentionの線形変換層\n",
    "W_q_multi = nn.Linear(d_model, d_model, bias=False)\n",
    "W_k_multi = nn.Linear(d_model, d_model, bias=False)\n",
    "W_v_multi = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "# Q, K, Vの生成（Self-Attentionと同じ！）\n",
    "Q_multi = W_q_multi(x)\n",
    "K_multi = W_k_multi(x)\n",
    "V_multi = W_v_multi(x)\n",
    "\n",
    "print(f\"入力 x: {x.shape}\")\n",
    "print(f\"Q (変換直後): {Q_multi.shape}\")\n",
    "print(f\"K (変換直後): {K_multi.shape}\")\n",
    "print(f\"V (変換直後): {V_multi.shape}\")\n",
    "print(\"→ ここまでは01と全く同じ！\")\n",
    "\n",
    "# ここからMulti-Head特有の処理\n",
    "d_k = d_model // num_heads\n",
    "Q_multi = Q_multi.view(batch_size, seq_len, num_heads, d_k).transpose(1, 2)\n",
    "K_multi = K_multi.view(batch_size, seq_len, num_heads, d_k).transpose(1, 2)\n",
    "V_multi = V_multi.view(batch_size, seq_len, num_heads, d_k).transpose(1, 2)\n",
    "\n",
    "print(f\"\\nQ (分割後): {Q_multi.shape}\")\n",
    "print(f\"K (分割後): {K_multi.shape}\")\n",
    "print(f\"V (分割後): {V_multi.shape}\")\n",
    "print(\"→ 複数headに分割してから各headでAttentionを計算\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"結論\")\n",
    "print(\"=\" * 70)\n",
    "print(\"✓ Q, K, Vの生成方法は01も02も同じ（入力xの線形変換）\")\n",
    "print(\"✓ 違いは、その後に分割するかどうかだけ\")\n",
    "print(\"✓ Multi-Headは、Self-Attentionの拡張版\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003a24bd",
   "metadata": {},
   "source": [
    "#### src/attention.pyでの実装を確認\n",
    "\n",
    "実際の`src/attention.py`を見ると、両方とも同じ構造になっています：\n",
    "\n",
    "##### SelfAttentionクラス:\n",
    "```python\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        # 入力からQuery, Key, Valueを生成するための線形変換\n",
    "        self.query_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.key_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.value_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # 入力から Q, K, V を線形変換で生成\n",
    "        query = self.query_linear(x)\n",
    "        key = self.key_linear(x)\n",
    "        value = self.value_linear(x)\n",
    "        \n",
    "        # Attentionを適用\n",
    "        attention_output, attention_weights = self.attention(query, key, value, mask)\n",
    "```\n",
    "\n",
    "##### MultiHeadAttentionクラス:\n",
    "```python\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        # Q, K, V用の線形変換層（全headをまとめて処理）\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # 1. 線形変換（Self-Attentionと同じ！）\n",
    "        Q = self.W_q(query)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "        \n",
    "        # 2. 複数headに分割（Multi-Head特有の処理）\n",
    "        Q = self.split_heads(Q, batch_size)\n",
    "        K = self.split_heads(K, batch_size)\n",
    "        V = self.split_heads(V, batch_size)\n",
    "        \n",
    "        # 3. 各headでAttentionを計算\n",
    "        attention_output, attention_weights = self.attention(Q, K, V, mask)\n",
    "```\n",
    "\n",
    "**変数名が違うだけで、やっていることは同じ！**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e87269c",
   "metadata": {},
   "source": [
    "#### まとめ\n",
    "\n",
    "**Q: Multi-Head AttentionのQ, K, Vも入力の線形変換と考えて良いか？**\n",
    "\n",
    "**A: はい、全くその通りです！**\n",
    "\n",
    "##### 共通点（01も02も同じ）:\n",
    "1. 入力 `x` から線形変換で Q, K, V を生成\n",
    "2. `nn.Linear` を使った学習可能な重み行列\n",
    "3. 数式: $Q = xW_q$, $K = xW_k$, $V = xW_v$\n",
    "4. Q, K, Vはそれぞれ異なる役割を持つ\n",
    "\n",
    "##### 違い（02だけの特徴）:\n",
    "- 生成した Q, K, V を複数headに分割\n",
    "- 各headで独立にAttentionを計算\n",
    "- 最後に全headの結果を結合\n",
    "\n",
    "##### 理解のポイント:\n",
    "```\n",
    "Self-Attention (01):\n",
    "  入力 → 線形変換 → Q, K, V → Attention → 出力\n",
    "\n",
    "Multi-Head Attention (02):\n",
    "  入力 → 線形変換 → Q, K, V → 分割 → 各headでAttention → 結合 → 出力\n",
    "         └─ ここまで01と同じ ─┘\n",
    "```\n",
    "\n",
    "**Multi-Head Attentionは、Self-Attentionの自然な拡張**です。Q, K, Vの生成方法は全く同じで、その後の処理が異なるだけです！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6d15f9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q15: Attentionは本質的にQ, K, Vの関数であり、入力xは必要ない？\n",
    "\n",
    "**質問日**: 2025年11月18日\n",
    "\n",
    "### 質問\n",
    "Attention機構やMulti-Head Attentionは、数式上も実装上も `Attention(Q, K, V)` としてQ, K, Vの関数として記述されています。これまでの説明では入力xから線形変換してQ, K, Vを生成すると説明されていますが、本質的には入力xの話は必要なく、「Q, K, Vが与えられればAttentionは計算できる」と理解すべきでしょうか？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2358c829",
   "metadata": {},
   "source": [
    "### 回答\n",
    "\n",
    "**その通りです！非常に鋭い指摘です。**\n",
    "\n",
    "Attention機構は本質的に**Q, K, Vの関数**であり、入力xがどこから来たかは関係ありません。\n",
    "\n",
    "#### 2つの視点の区別\n",
    "\n",
    "##### 1. **Attention機構そのもの（コアアルゴリズム）**\n",
    "\n",
    "数式:\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "- **入力**: Q, K, V（3つの行列）\n",
    "- **出力**: Attention適用後の行列\n",
    "- **特徴**: Q, K, Vがどこから来たかは知らない・気にしない\n",
    "- **役割**: 純粋な計算メカニズム\n",
    "\n",
    "##### 2. **Self-Attention（Attentionの使い方の一例）**\n",
    "\n",
    "数式:\n",
    "$$\n",
    "Q = xW_q, \\quad K = xW_k, \\quad V = xW_v\n",
    "$$\n",
    "$$\n",
    "\\text{SelfAttention}(x) = \\text{Attention}(xW_q, xW_k, xW_v)\n",
    "$$\n",
    "\n",
    "- **入力**: x（1つの入力）\n",
    "- **特徴**: 同じ入力xからQ, K, Vを生成\n",
    "- **役割**: Attentionの具体的な応用方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34198b40",
   "metadata": {},
   "source": [
    "#### 実装での分離\n",
    "\n",
    "`src/attention.py`でも、この2つは明確に分離されています：\n",
    "\n",
    "##### ScaledDotProductAttention（コアメカニズム）\n",
    "\n",
    "```python\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: Query行列 [batch_size, seq_len, d_k]\n",
    "            key: Key行列 [batch_size, seq_len, d_k]\n",
    "            value: Value行列 [batch_size, seq_len, d_k]\n",
    "        \"\"\"\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        return output, attention_weights\n",
    "```\n",
    "\n",
    "**ポイント**: \n",
    "- Q, K, Vがどこから来たか全く知らない\n",
    "- ただ計算するだけ\n",
    "- 汎用的なアルゴリズム\n",
    "\n",
    "##### SelfAttention（Attentionの応用）\n",
    "\n",
    "```python\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        self.query_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.key_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.value_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.attention = ScaledDotProductAttention()  # ← コアを利用\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # xからQ, K, Vを生成（Self-Attention特有の処理）\n",
    "        query = self.query_linear(x)\n",
    "        key = self.key_linear(x)\n",
    "        value = self.value_linear(x)\n",
    "        \n",
    "        # Attentionを呼び出す（汎用メカニズム）\n",
    "        output, weights = self.attention(query, key, value)\n",
    "        return output, weights\n",
    "```\n",
    "\n",
    "**ポイント**:\n",
    "- 入力xの処理を担当\n",
    "- Q, K, Vを準備\n",
    "- Attentionメカニズムを呼び出す"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb486a4",
   "metadata": {},
   "source": [
    "#### Q, K, Vが異なる入力から来る例: Cross-Attention\n",
    "\n",
    "Self-Attentionでは同じxからQ, K, Vを生成しますが、**異なる入力から生成する場合もあります**。\n",
    "\n",
    "##### Cross-Attention（Transformerのデコーダで使用）\n",
    "\n",
    "```python\n",
    "def cross_attention(encoder_output, decoder_input):\n",
    "    \"\"\"\n",
    "    Q: デコーダの入力から生成\n",
    "    K, V: エンコーダの出力から生成\n",
    "    \"\"\"\n",
    "    Q = W_q(decoder_input)      # デコーダ側\n",
    "    K = W_k(encoder_output)     # エンコーダ側\n",
    "    V = W_v(encoder_output)     # エンコーダ側\n",
    "    \n",
    "    # Attentionメカニズムは同じ！\n",
    "    output = Attention(Q, K, V)\n",
    "    return output\n",
    "```\n",
    "\n",
    "**重要な点**:\n",
    "- Q, K, Vが**異なるソース**から来ている\n",
    "- でもAttention機構は全く同じ\n",
    "- Attentionは「Q, K, Vの関係を計算するだけ」で、出所は関係ない\n",
    "\n",
    "##### 応用例: 翻訳タスク\n",
    "\n",
    "```\n",
    "英語文: \"I love cats\"（エンコーダ）\n",
    "日本語文: \"私は\"（デコーダで生成中）\n",
    "\n",
    "Q: 「私は」の次に何を生成すべきか探している（デコーダ側）\n",
    "K, V: 英語文の情報（エンコーダ側）\n",
    "\n",
    "Attention計算: 「私は」が英語文のどこに注目すべきか\n",
    "結果: \"love\" に注目 → 次は「愛している」を生成\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109c3114",
   "metadata": {},
   "source": [
    "#### コード例: Attentionの汎用性を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27608d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ケース1: Self-Attention\n",
      "======================================================================\n",
      "入力: 同じx torch.Size([1, 3, 4])\n",
      "Q, K, V: 全てxから生成\n",
      "出力: torch.Size([1, 3, 4])\n",
      "\n",
      "======================================================================\n",
      "ケース2: Cross-Attention\n",
      "======================================================================\n",
      "Q: decoder_input torch.Size([1, 2, 4]) から生成\n",
      "K, V: encoder_output torch.Size([1, 3, 4]) から生成\n",
      "出力: torch.Size([1, 2, 4])\n",
      "\n",
      "======================================================================\n",
      "ケース3: ランダムなQ, K, V（どこから来たかわからない）\n",
      "======================================================================\n",
      "Q: torch.Size([1, 5, 4])\n",
      "K: torch.Size([1, 5, 4])\n",
      "V: torch.Size([1, 5, 4])\n",
      "出力: torch.Size([1, 5, 4])\n",
      "\n",
      "======================================================================\n",
      "結論\n",
      "======================================================================\n",
      "✓ Attention関数は、どのケースでも同じように動作\n",
      "✓ Q, K, Vがどこから来たかは全く関係ない\n",
      "✓ Attentionは純粋に「Q, K, Vの関数」として定義される\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "def attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    汎用的なAttention機構\n",
    "    Q, K, Vがどこから来たかは気にしない\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    return output, attention_weights\n",
    "\n",
    "# ケース1: Self-Attention（同じxから生成）\n",
    "print(\"=\" * 70)\n",
    "print(\"ケース1: Self-Attention\")\n",
    "print(\"=\" * 70)\n",
    "x = torch.randn(1, 3, 4)\n",
    "W_q = torch.randn(4, 4)\n",
    "W_k = torch.randn(4, 4)\n",
    "W_v = torch.randn(4, 4)\n",
    "\n",
    "Q1 = torch.matmul(x, W_q)\n",
    "K1 = torch.matmul(x, W_k)\n",
    "V1 = torch.matmul(x, W_v)\n",
    "\n",
    "print(f\"入力: 同じx {x.shape}\")\n",
    "print(f\"Q, K, V: 全てxから生成\")\n",
    "output1, weights1 = attention(Q1, K1, V1)\n",
    "print(f\"出力: {output1.shape}\")\n",
    "\n",
    "# ケース2: Cross-Attention（異なる入力から生成）\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ケース2: Cross-Attention\")\n",
    "print(\"=\" * 70)\n",
    "encoder_output = torch.randn(1, 3, 4)\n",
    "decoder_input = torch.randn(1, 2, 4)\n",
    "\n",
    "Q2 = torch.matmul(decoder_input, W_q)  # デコーダ入力から\n",
    "K2 = torch.matmul(encoder_output, W_k)  # エンコーダ出力から\n",
    "V2 = torch.matmul(encoder_output, W_v)  # エンコーダ出力から\n",
    "\n",
    "print(f\"Q: decoder_input {decoder_input.shape} から生成\")\n",
    "print(f\"K, V: encoder_output {encoder_output.shape} から生成\")\n",
    "output2, weights2 = attention(Q2, K2, V2)\n",
    "print(f\"出力: {output2.shape}\")\n",
    "\n",
    "# ケース3: 完全にランダムなQ, K, V\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ケース3: ランダムなQ, K, V（どこから来たかわからない）\")\n",
    "print(\"=\" * 70)\n",
    "Q3 = torch.randn(1, 5, 4)\n",
    "K3 = torch.randn(1, 5, 4)\n",
    "V3 = torch.randn(1, 5, 4)\n",
    "\n",
    "print(f\"Q: {Q3.shape}\")\n",
    "print(f\"K: {K3.shape}\")\n",
    "print(f\"V: {V3.shape}\")\n",
    "output3, weights3 = attention(Q3, K3, V3)\n",
    "print(f\"出力: {output3.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"結論\")\n",
    "print(\"=\" * 70)\n",
    "print(\"✓ Attention関数は、どのケースでも同じように動作\")\n",
    "print(\"✓ Q, K, Vがどこから来たかは全く関係ない\")\n",
    "print(\"✓ Attentionは純粋に「Q, K, Vの関数」として定義される\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4a7a28",
   "metadata": {},
   "source": [
    "#### なぜ入力xの話をするのか\n",
    "\n",
    "では、なぜこれまで「入力xから線形変換」という説明をしてきたのでしょうか？\n",
    "\n",
    "##### 理由1: **学習のため**\n",
    "\n",
    "Attention機構自体にはパラメータがありません。学習するには：\n",
    "- 線形変換の重み $W_q, W_k, W_v$ が必要\n",
    "- これらを学習することで、タスクに適したQ, K, Vの生成方法を習得\n",
    "\n",
    "##### 理由2: **Self-Attentionが最も一般的**\n",
    "\n",
    "Transformerでは、Self-Attentionが主要な構成要素：\n",
    "- エンコーダ: Self-Attention\n",
    "- デコーダ: Self-Attention + Cross-Attention\n",
    "\n",
    "「同じxからQ, K, Vを生成」というパターンが頻出するため、説明の中心になる\n",
    "\n",
    "##### 理由3: **理解のしやすさ**\n",
    "\n",
    "初学者にとって：\n",
    "- 「入力xがある」という具体的な出発点があると理解しやすい\n",
    "- 抽象的な「Q, K, Vが与えられる」よりイメージしやすい"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d9c978",
   "metadata": {},
   "source": [
    "#### まとめ\n",
    "\n",
    "**Q: Attentionは本質的にQ, K, Vの関数であり、入力xは必要ない？**\n",
    "\n",
    "**A: はい、完全に正しい理解です！**\n",
    "\n",
    "##### Attention機構の本質\n",
    "\n",
    "```\n",
    "Attention(Q, K, V) = softmax(QK^T / √d_k) V\n",
    "```\n",
    "\n",
    "- **純粋な数学的関数**: Q, K, V → 出力\n",
    "- **汎用的なアルゴリズム**: どこから来たQ, K, Vでも同じように処理\n",
    "- **パラメータなし**: 計算するだけ\n",
    "\n",
    "##### 入力xの役割\n",
    "\n",
    "入力xは、**Q, K, Vを準備する方法の一例**：\n",
    "\n",
    "- **Self-Attention**: 同じxからQ, K, Vを生成\n",
    "- **Cross-Attention**: 異なる入力からQ, K, Vを生成\n",
    "- **その他**: Q, K, Vは任意の方法で準備可能\n",
    "\n",
    "##### 理解のポイント\n",
    "\n",
    "| 視点 | 内容 |\n",
    "|------|------|\n",
    "| **Attentionのコア** | Q, K, Vの関数（汎用メカニズム） |\n",
    "| **Self-Attentionなど** | Attentionの使い方（応用例） |\n",
    "| **入力xの役割** | Q, K, Vを生成する手段の1つ |\n",
    "\n",
    "##### 類似例\n",
    "\n",
    "プログラミングの関数と同じ：\n",
    "\n",
    "```python\n",
    "def sort(array):\n",
    "    # 配列がどこから来たかは関係ない\n",
    "    return sorted(array)\n",
    "\n",
    "# 使い方の例\n",
    "list1 = [3, 1, 2]\n",
    "sort(list1)  # リストから\n",
    "\n",
    "dict1 = {'a': 3, 'b': 1}\n",
    "sort(dict1.values())  # 辞書から\n",
    "\n",
    "# sort関数自体は汎用的\n",
    "```\n",
    "\n",
    "**Attention機構も同じ**: Q, K, Vがあれば計算できる。それらをどう準備するかは別の話。\n",
    "\n",
    "この理解は、今後Cross-AttentionやTransformerの全体構造を学ぶときに非常に重要になります！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc108420",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q16: Attention WeightsとVの積で、行と列のどちらに意味があるのか？\n",
    "\n",
    "**質問日**: 2025年11月18日\n",
    "\n",
    "### 質問\n",
    "Attention計算の最後に `Attention_Weights × V` という行列の積を計算します。\n",
    "\n",
    "通常の線形変換 `y = Wx` では、行列Wの列ベクトルが意味を持ち、結果yの各要素は「Wの各列との内積」という解釈になります。\n",
    "\n",
    "しかしTransformerでは、データの**行**に意味があるように見えます（各行が1つのトークンを表す）。これが混乱の原因なのですが、どう理解すればよいでしょうか？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac9904a",
   "metadata": {},
   "source": [
    "### 回答\n",
    "\n",
    "素晴らしい質問です！これは行列の積の理解において重要なポイントです。\n",
    "\n",
    "**結論**: Transformerでは、**行に意味がある行列同士の積**を計算しています。通常の線形変換`y = Wx`とは**転置の関係**になっています。\n",
    "\n",
    "#### 2つの行列の積の解釈\n",
    "\n",
    "##### パターン1: 通常の線形変換（列ベクトルに意味）\n",
    "\n",
    "```python\n",
    "# ベクトルの変換: y = Wx\n",
    "W = [[w11, w12],    # 2×3 行列\n",
    "     [w21, w22],\n",
    "     [w31, w32]]\n",
    "x = [x1, x2, x3]    # 3次元列ベクトル\n",
    "y = Wx              # 2次元列ベクトル\n",
    "```\n",
    "\n",
    "解釈:\n",
    "- xは**列ベクトル** (縦方向)\n",
    "- Wの各**列**がベクトル\n",
    "- yの各要素は、Wの各**行**とxの内積\n",
    "\n",
    "##### パターン2: Transformerのバッチ処理（行ベクトルに意味）\n",
    "\n",
    "```python\n",
    "# バッチ処理: Y = XW\n",
    "X = [[x11, x12, x13],    # 2×3 行列 (2サンプル)\n",
    "     [x21, x22, x23]]\n",
    "W = [[w11, w12],         # 3×2 行列\n",
    "     [w21, w22],\n",
    "     [w31, w32]]\n",
    "Y = XW                   # 2×2 行列\n",
    "```\n",
    "\n",
    "解釈:\n",
    "- Xの各**行**が1つのサンプル (横方向)\n",
    "- Wの各**列**がベクトル\n",
    "- Yの各**行**が結果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f846178",
   "metadata": {},
   "source": [
    "#### Attention計算での行列の形状\n",
    "\n",
    "Transformerでは、データを**[seq_len, d_model]** の形で扱います：\n",
    "\n",
    "```\n",
    "V = [[v1_1, v1_2, ..., v1_d],    # トークン1のベクトル\n",
    "     [v2_1, v2_2, ..., v2_d],    # トークン2のベクトル\n",
    "     [v3_1, v3_2, ..., v3_d]]    # トークン3のベクトル\n",
    "```\n",
    "\n",
    "- **各行**: 1つのトークンの特徴ベクトル\n",
    "- **各列**: 特徴次元の1つ\n",
    "\n",
    "##### Attention Weightsとの積\n",
    "\n",
    "```python\n",
    "attention_weights = [[0.7, 0.2, 0.1],     # トークン1の注目分布\n",
    "                     [0.1, 0.8, 0.1],     # トークン2の注目分布\n",
    "                     [0.2, 0.2, 0.6]]     # トークン3の注目分布\n",
    "                     # [seq_len, seq_len]\n",
    "\n",
    "V = [[v1_1, v1_2, ..., v1_d],     # トークン1\n",
    "     [v2_1, v2_2, ..., v2_d],     # トークン2\n",
    "     [v3_1, v3_2, ..., v3_d]]     # トークン3\n",
    "     # [seq_len, d_model]\n",
    "\n",
    "output = attention_weights @ V\n",
    "# [seq_len, seq_len] @ [seq_len, d_model] = [seq_len, d_model]\n",
    "```\n",
    "\n",
    "**重要**: \n",
    "- `attention_weights`の各**行**：各トークンが他のトークンに注目する重み\n",
    "- `V`の各**行**：各トークンの値ベクトル\n",
    "- `output`の各**行**：Vの行の重み付き和"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4694d6",
   "metadata": {},
   "source": [
    "#### 具体例で理解する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d432721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V (Value行列):\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.]])\n",
      "形状: torch.Size([3, 4]) - 各行が1つのトークンのベクトル\n",
      "\n",
      "Attention Weights:\n",
      "tensor([[0.7000, 0.2000, 0.1000],\n",
      "        [0.1000, 0.8000, 0.1000],\n",
      "        [0.2000, 0.2000, 0.6000]])\n",
      "形状: torch.Size([3, 3])\n",
      "各行の和: tensor([1., 1., 1.])\n",
      "\n",
      "Output = Attention_Weights @ V:\n",
      "tensor([[0.7000, 0.2000, 0.1000, 0.0000],\n",
      "        [0.1000, 0.8000, 0.1000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.6000, 0.0000]])\n",
      "形状: torch.Size([3, 4])\n",
      "\n",
      "============================================================\n",
      "1行目（トークン1の出力）を詳しく計算:\n",
      "============================================================\n",
      "attention_weights[0] = [0.7 0.2 0.1]\n",
      "\n",
      "重み付き和の計算:\n",
      "  0.7 × V[0] = 0.7 × [1. 0. 0. 0.] = [0.7 0.  0.  0. ]\n",
      "  0.2 × V[1] = 0.2 × [0. 1. 0. 0.] = [0.  0.2 0.  0. ]\n",
      "  0.1 × V[2] = 0.1 × [0. 0. 1. 0.] = [0.  0.  0.1 0. ]\n",
      "  合計      = [0.7 0.2 0.1 0. ]\n",
      "\n",
      "💡 ポイント: 出力の各行は、Vの行ベクトルの重み付き和\n",
      "   列ベクトルではなく、行ベクトルを混ぜ合わせている！\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 3つのトークン、各トークンは4次元ベクトル\n",
    "seq_len = 3\n",
    "d_model = 4\n",
    "\n",
    "# Vの各行が1つのトークンを表す\n",
    "V = torch.tensor([\n",
    "    [1.0, 0.0, 0.0, 0.0],  # トークン1: [1,0,0,0]\n",
    "    [0.0, 1.0, 0.0, 0.0],  # トークン2: [0,1,0,0]\n",
    "    [0.0, 0.0, 1.0, 0.0]   # トークン3: [0,0,1,0]\n",
    "])\n",
    "\n",
    "print(\"V (Value行列):\")\n",
    "print(V)\n",
    "print(f\"形状: {V.shape} - 各行が1つのトークンのベクトル\")\n",
    "print()\n",
    "\n",
    "# Attention Weightsの各行が、各トークンの注目分布\n",
    "attention_weights = torch.tensor([\n",
    "    [0.7, 0.2, 0.1],  # トークン1は、自分に70%, トークン2に20%, トークン3に10%\n",
    "    [0.1, 0.8, 0.1],  # トークン2は、自分に80%\n",
    "    [0.2, 0.2, 0.6]   # トークン3は、自分に60%\n",
    "])\n",
    "\n",
    "print(\"Attention Weights:\")\n",
    "print(attention_weights)\n",
    "print(f\"形状: {attention_weights.shape}\")\n",
    "print(\"各行の和:\", attention_weights.sum(dim=1))  # 各行の和は1\n",
    "print()\n",
    "\n",
    "# 行列の積を計算\n",
    "output = torch.matmul(attention_weights, V)\n",
    "\n",
    "print(\"Output = Attention_Weights @ V:\")\n",
    "print(output)\n",
    "print(f\"形状: {output.shape}\")\n",
    "print()\n",
    "\n",
    "# 1行目を詳しく見る\n",
    "print(\"=\" * 60)\n",
    "print(\"1行目（トークン1の出力）を詳しく計算:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"attention_weights[0] =\", attention_weights[0].numpy())\n",
    "print()\n",
    "print(\"重み付き和の計算:\")\n",
    "print(f\"  0.7 × V[0] = 0.7 × {V[0].numpy()} = {(0.7 * V[0]).numpy()}\")\n",
    "print(f\"  0.2 × V[1] = 0.2 × {V[1].numpy()} = {(0.2 * V[1]).numpy()}\")\n",
    "print(f\"  0.1 × V[2] = 0.1 × {V[2].numpy()} = {(0.1 * V[2]).numpy()}\")\n",
    "print(f\"  合計      = {output[0].numpy()}\")\n",
    "print()\n",
    "print(\"💡 ポイント: 出力の各行は、Vの行ベクトルの重み付き和\")\n",
    "print(\"   列ベクトルではなく、行ベクトルを混ぜ合わせている！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddc1c49",
   "metadata": {},
   "source": [
    "#### 数学的な説明\n",
    "\n",
    "##### 行列の積の一般形\n",
    "\n",
    "$$\n",
    "C = AB\n",
    "$$\n",
    "\n",
    "ここで $A$ が $[m \\times n]$、$B$ が $[n \\times p]$ のとき、$C$ は $[m \\times p]$\n",
    "\n",
    "結果の要素:\n",
    "$$\n",
    "C_{ij} = \\sum_{k=1}^{n} A_{ik} B_{kj}\n",
    "$$\n",
    "\n",
    "##### Attentionでの解釈\n",
    "\n",
    "$$\n",
    "\\text{output} = \\text{attention\\_weights} \\times V\n",
    "$$\n",
    "\n",
    "- `attention_weights`: $[\\text{seq\\_len} \\times \\text{seq\\_len}]$\n",
    "- `V`: $[\\text{seq\\_len} \\times d_{\\text{model}}]$\n",
    "- `output`: $[\\text{seq\\_len} \\times d_{\\text{model}}]$\n",
    "\n",
    "出力の第$i$行、第$j$列の要素:\n",
    "\n",
    "$$\n",
    "\\text{output}_{ij} = \\sum_{k=1}^{\\text{seq\\_len}} \\text{attention\\_weights}_{ik} \\cdot V_{kj}\n",
    "$$\n",
    "\n",
    "**重要な洞察**:\n",
    "\n",
    "- $\\text{output}$の第$i$**行**は、$V$の全ての**行**の重み付き和\n",
    "- 重みは$\\text{attention\\_weights}$の第$i$行から来る\n",
    "- つまり、「各トークン（行）が他のトークン（行）をどう組み合わせるか」を計算している"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02e8289",
   "metadata": {},
   "source": [
    "#### なぜ通常の線形変換と違うのか\n",
    "\n",
    "##### 通常の線形変換 (y = Wx)\n",
    "\n",
    "```python\n",
    "# 1つのベクトルを変換\n",
    "x = [x1, x2, x3]     # 列ベクトル (縦)\n",
    "W @ x = y            # 変換後の列ベクトル\n",
    "```\n",
    "\n",
    "- データ: **列ベクトル**（縦方向）\n",
    "- 1つのサンプルを変換\n",
    "\n",
    "##### Transformerのバッチ処理 (Y = XW)\n",
    "\n",
    "```python\n",
    "# 複数のベクトルをまとめて変換\n",
    "X = [[x1_1, x1_2, x1_3],    # サンプル1 (横)\n",
    "     [x2_1, x2_2, x2_3]]    # サンプル2 (横)\n",
    "X @ W = Y                    # 変換後の行列（各行がサンプル）\n",
    "```\n",
    "\n",
    "- データ: 行列の**各行**がサンプル（横方向）\n",
    "- 複数のサンプルをまとめて処理（効率化）\n",
    "\n",
    "##### 転置の関係\n",
    "\n",
    "実は同じことをしています！\n",
    "\n",
    "```python\n",
    "# 通常の書き方\n",
    "y = W @ x           # 列ベクトル\n",
    "\n",
    "# 転置を取ると\n",
    "y^T = x^T @ W^T     # 行ベクトル\n",
    "```\n",
    "\n",
    "Transformerは効率化のため、**最初から転置した形**（行ベクトル）で処理しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f811a63a",
   "metadata": {},
   "source": [
    "#### まとめ\n",
    "\n",
    "**Q: Attention WeightsとVの積で、行と列のどちらに意味があるのか？**\n",
    "\n",
    "**A: Transformerでは「行」に意味があります！**\n",
    "\n",
    "##### 重要なポイント\n",
    "\n",
    "| 観点 | 通常の線形変換 | Transformer |\n",
    "|------|--------------|------------|\n",
    "| **データの向き** | 列ベクトル（縦） | 行ベクトル（横） |\n",
    "| **形式** | $y = Wx$ | $Y = XW$ |\n",
    "| **サンプル** | 1つずつ | バッチ（複数まとめて） |\n",
    "| **意味のある方向** | 縦（列） | 横（行） |\n",
    "\n",
    "##### Attention計算の構造\n",
    "\n",
    "```\n",
    "attention_weights @ V\n",
    "[seq_len, seq_len] @ [seq_len, d_model] = [seq_len, d_model]\n",
    "     ↓                      ↓                     ↓\n",
    "  各行が注目分布        各行がトークン        各行が出力トークン\n",
    "```\n",
    "\n",
    "**各行**が意味を持つ:\n",
    "- Vの各**行**: 各トークンの値ベクトル\n",
    "- attention_weightsの各**行**: 各トークンの注目分布  \n",
    "- outputの各**行**: Vの行ベクトルの重み付き和\n",
    "\n",
    "##### なぜこの形式か\n",
    "\n",
    "1. **バッチ処理の効率化**: 複数のトークンを並列処理\n",
    "2. **実装の統一性**: PyTorchなどの深層学習フレームワークの慣習\n",
    "3. **数学的には同じ**: 転置を取れば従来の列ベクトル形式と等価\n",
    "\n",
    "##### 理解のコツ\n",
    "\n",
    "**「行列の積は、左側の行と右側の列の内積」**という基本を思い出す:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "\\text{行1} \\\\\n",
    "\\text{行2} \\\\\n",
    "\\text{行3}\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix} \n",
    "\\text{列1} & \\text{列2} & \\text{列3}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Transformerでは:\n",
    "- 左側（attention_weights）の**行**: 各トークンの注目分布\n",
    "- 右側（V）の**列**: 特徴次元\n",
    "- 結果: 各トークンが他のトークンの値を重み付けして集約\n",
    "\n",
    "この理解があれば、Transformerの計算がすっきり見えてきます！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a88c740",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q17: Self-Attentionの厳密な定義について\n",
    "\n",
    "**質問日**: 2025年11月21日\n",
    "\n",
    "### 質問\n",
    "厳密に言うとSelf-Attentionとは、入力から線形変換層の出力（Q, K, V）をScaled Dot-product Attentionに入れた出力のことと理解して良いか？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a791f254",
   "metadata": {},
   "source": [
    "### 回答\n",
    "\n",
    "はい、その理解で正確です。\n",
    "\n",
    "#### Self-Attentionの厳密な定義\n",
    "\n",
    "**Self-Attention = 線形変換(Q, K, V) + Scaled Dot-Product Attention + 出力線形変換**\n",
    "\n",
    "という構造です。\n",
    "\n",
    "#### 具体的な処理フロー\n",
    "\n",
    "```\n",
    "入力 X (shape: [batch, seq_len, d_model])\n",
    "    ↓\n",
    "Q = X @ W_q  (Query への線形変換)\n",
    "K = X @ W_k  (Key への線形変換)  \n",
    "V = X @ W_v  (Value への線形変換)\n",
    "    ↓\n",
    "Attention(Q, K, V) = softmax(QK^T / √d_k) @ V  (Scaled Dot-Product Attention)\n",
    "    ↓\n",
    "output = Attention @ W_o  (出力線形変換)\n",
    "```\n",
    "\n",
    "#### 実装での対応\n",
    "\n",
    "`src/attention.py`の`SelfAttention`クラスはまさにこの構造を実装しています:\n",
    "\n",
    "1. **線形変換層** (`__init__`で定義):\n",
    "   - `self.query_linear`, `self.key_linear`, `self.value_linear`\n",
    "\n",
    "2. **Scaled Dot-Product Attention** (`forward`内):\n",
    "   - `scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)`\n",
    "   - `attention_weights = F.softmax(scores, dim=-1)`\n",
    "   - `output = torch.matmul(attention_weights, V)`\n",
    "\n",
    "3. **出力線形変換**:\n",
    "   - `self.output_linear`\n",
    "\n",
    "#### \"Self\"の意味\n",
    "\n",
    "\"Self\"というのは、Q, K, V **すべてが同じ入力Xから生成される**ことを意味します（他の系列からではなく、自分自身から）。\n",
    "\n",
    "これに対してCross-Attentionでは、QとK/Vが異なる入力から生成されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590d551b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q18: Self-Attentionの出力形状について\n",
    "\n",
    "**質問日**: 2025年11月21日\n",
    "\n",
    "### 質問\n",
    "Self-Attentionは入力と同じ形をしていると考えて良いか？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d39a6ef",
   "metadata": {},
   "source": [
    "### 回答\n",
    "\n",
    "はい、**Self-Attentionの出力は入力と同じ形**をしています。\n",
    "\n",
    "#### 形状の一致\n",
    "\n",
    "- **入力**: `[batch_size, seq_len, d_model]`\n",
    "- **出力**: `[batch_size, seq_len, d_model]`\n",
    "\n",
    "#### なぜ同じ形状なのか\n",
    "\n",
    "1. **残差接続(Residual Connection)が可能**: \n",
    "   - Transformerでは `output = LayerNorm(x + SelfAttention(x))` のように入力を足し合わせる\n",
    "   - そのため、形状が同じである必要があります\n",
    "\n",
    "2. **層を積み重ねられる**: \n",
    "   - 出力が入力と同じ形なので、Self-Attention層を何層も重ねることができます\n",
    "\n",
    "3. **系列長が保持される**: \n",
    "   - 各位置の情報が保持され、系列全体の文脈を考慮した新しい表現に変換されます\n",
    "\n",
    "#### 内部での次元変化\n",
    "\n",
    "内部では次元が変化しますが、最終的に元に戻ります:\n",
    "\n",
    "```\n",
    "入力 X: [batch, seq_len, d_model]\n",
    "    ↓ 線形変換\n",
    "Q, K, V: [batch, seq_len, d_k]  (d_k は通常 d_model と同じか小さい)\n",
    "    ↓ Scaled Dot-Product Attention\n",
    "Attention出力: [batch, seq_len, d_k]\n",
    "    ↓ 出力線形変換(あれば)\n",
    "最終出力: [batch, seq_len, d_model]  (入力と同じ!)\n",
    "```\n",
    "\n",
    "#### 実装での確認\n",
    "\n",
    "`src/attention.py`の実装でも、`d_model`を入力・出力の両方で使用することで、この形状の一致を保証しています。\n",
    "\n",
    "```python\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.query_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.key_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.value_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.output_linear = nn.Linear(d_model, d_model)  # 同じd_model\n",
    "```\n",
    "\n",
    "この入出力の形状一致が、Transformerの深い構造を可能にしています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ca3559",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q19: Multi-Head Attentionのhead分割について\n",
    "\n",
    "**質問日**: 2025年11月21日\n",
    "\n",
    "### 質問\n",
    "Multi-Head Attentionについて、head分割とは特徴量ベクトルを分割することと捉えていいか？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9c6399",
   "metadata": {},
   "source": [
    "### 回答\n",
    "\n",
    "はい、その理解で正確です！\n",
    "\n",
    "**Multi-Head Attentionのhead分割 = 特徴量ベクトル（d_model次元）を分割すること**\n",
    "\n",
    "#### 具体的な分割の仕組み\n",
    "\n",
    "```\n",
    "入力: [batch, seq_len, d_model=512]\n",
    "    ↓ 線形変換 (W_q)\n",
    "Q: [batch, seq_len, d_model=512]\n",
    "    ↓ head分割\n",
    "Q: [batch, num_heads=8, seq_len, d_k=64]\n",
    "```\n",
    "\n",
    "例えば `d_model=512`, `num_heads=8` の場合:\n",
    "- 各headは **d_k = 512 / 8 = 64次元** を担当\n",
    "- 512次元の特徴量ベクトルを、8個の64次元ベクトルに分割\n",
    "- 各headは **特徴量空間の異なる部分** を見る\n",
    "\n",
    "#### 重要なポイント\n",
    "\n",
    "1. **分割は特徴量次元**: \n",
    "   - シーケンス長（seq_len）は分割されず、そのまま保持\n",
    "   - 特徴量次元（d_model）だけが分割される\n",
    "\n",
    "2. **独立した部分空間**: \n",
    "   - 各headは特徴量の異なる側面を学習\n",
    "   - Head 1: 文法的な関係を捉える\n",
    "   - Head 2: 意味的な類似性を捉える\n",
    "   - Head 3: 位置関係を捉える（など）\n",
    "\n",
    "3. **最後に結合**: \n",
    "   - 全headの出力を結合（concat）して元の`d_model`次元に戻す\n",
    "   - `[batch, num_heads, seq_len, d_k]` → `[batch, seq_len, d_model]`\n",
    "\n",
    "#### 実装での確認\n",
    "\n",
    "`src/attention.py`の`MultiHeadAttention`クラスの`split_heads`メソッドがまさにこの分割処理をしています:\n",
    "\n",
    "```python\n",
    "def split_heads(self, x, batch_size):\n",
    "    \"\"\"\n",
    "    入力を複数のheadに分割\n",
    "    \n",
    "    Args:\n",
    "        x: shape [batch_size, seq_len, d_model]\n",
    "    Returns:\n",
    "        shape [batch_size, num_heads, seq_len, d_k]\n",
    "    \"\"\"\n",
    "    # [batch, seq_len, d_model] -> [batch, seq_len, num_heads, d_k]\n",
    "    x = x.view(batch_size, -1, self.num_heads, self.d_k)\n",
    "    # [batch, seq_len, num_heads, d_k] -> [batch, num_heads, seq_len, d_k]\n",
    "    return x.transpose(1, 2)\n",
    "```\n",
    "\n",
    "これで512次元の特徴量を8個の64次元に分割し、各headで並列処理しています！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a229df2",
   "metadata": {},
   "source": [
    "#### コード例: head分割の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70adfc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元の入力形状: torch.Size([2, 5, 512])\n",
      "  - batch_size: 2\n",
      "  - seq_len: 5\n",
      "  - d_model: 512\n",
      "\n",
      "reshape後: torch.Size([2, 5, 8, 64])\n",
      "transpose後（head分割完了）: torch.Size([2, 8, 5, 64])\n",
      "  - batch_size: 2\n",
      "  - num_heads: 8 (8個のheadに分割)\n",
      "  - seq_len: 5 (シーケンス長は保持)\n",
      "  - d_k: 64 (各headは64次元を担当)\n",
      "\n",
      "💡 特徴量の512次元が、8個の64次元に分割されました！\n",
      "   各headが特徴量空間の異なる部分を並列に処理します。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# パラメータ設定\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_k = d_model // num_heads  # 64\n",
    "\n",
    "# サンプル入力\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "print(f\"元の入力形状: {x.shape}\")\n",
    "print(f\"  - batch_size: {batch_size}\")\n",
    "print(f\"  - seq_len: {seq_len}\")\n",
    "print(f\"  - d_model: {d_model}\")\n",
    "\n",
    "# head分割のシミュレーション\n",
    "# [batch, seq_len, d_model] -> [batch, seq_len, num_heads, d_k]\n",
    "x_reshaped = x.view(batch_size, seq_len, num_heads, d_k)\n",
    "print(f\"\\nreshape後: {x_reshaped.shape}\")\n",
    "\n",
    "# [batch, seq_len, num_heads, d_k] -> [batch, num_heads, seq_len, d_k]\n",
    "x_split = x_reshaped.transpose(1, 2)\n",
    "print(f\"transpose後（head分割完了）: {x_split.shape}\")\n",
    "print(f\"  - batch_size: {x_split.shape[0]}\")\n",
    "print(f\"  - num_heads: {x_split.shape[1]} (8個のheadに分割)\")\n",
    "print(f\"  - seq_len: {x_split.shape[2]} (シーケンス長は保持)\")\n",
    "print(f\"  - d_k: {x_split.shape[3]} (各headは{d_k}次元を担当)\")\n",
    "\n",
    "print(f\"\\n💡 特徴量の{d_model}次元が、{num_heads}個の{d_k}次元に分割されました！\")\n",
    "print(f\"   各headが特徴量空間の異なる部分を並列に処理します。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bbfc92",
   "metadata": {},
   "source": [
    "#### 結合の仕組み\n",
    "\n",
    "分割と逆の操作で、全headの出力を結合します:\n",
    "\n",
    "```python\n",
    "def combine_heads(self, x, batch_size):\n",
    "    \"\"\"\n",
    "    複数のheadを結合\n",
    "    \n",
    "    Args:\n",
    "        x: shape [batch_size, num_heads, seq_len, d_k]\n",
    "    Returns:\n",
    "        shape [batch_size, seq_len, d_model]\n",
    "    \"\"\"\n",
    "    # [batch, num_heads, seq_len, d_k] -> [batch, seq_len, num_heads, d_k]\n",
    "    x = x.transpose(1, 2).contiguous()\n",
    "    # [batch, seq_len, num_heads, d_k] -> [batch, seq_len, d_model]\n",
    "    return x.view(batch_size, -1, self.d_model)\n",
    "```\n",
    "\n",
    "この分割と結合により、Multi-Head Attentionは複数の表現部分空間から並列に情報を学習できます！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3affc73",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q17: Self-Attentionの厳密な定義について\n",
    "\n",
    "**質問日**: 2025年11月21日\n",
    "\n",
    "### 質問\n",
    "厳密に言うとSelf-Attentionとは、入力から線形変換層の出力（Q, K, V）をScaled Dot-product Attentionに入れた出力のことと理解して良いか？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebb1919",
   "metadata": {},
   "source": [
    "### 回答\n",
    "\n",
    "はい、その理解で正確です。\n",
    "\n",
    "#### Self-Attentionの厳密な定義\n",
    "\n",
    "**Self-Attention = 線形変換(Q, K, V) + Scaled Dot-Product Attention + 出力線形変換**\n",
    "\n",
    "という構造です。\n",
    "\n",
    "#### 具体的な処理フロー\n",
    "\n",
    "```\n",
    "入力 X (shape: [batch, seq_len, d_model])\n",
    "    ↓\n",
    "Q = X @ W_q  (Query への線形変換)\n",
    "K = X @ W_k  (Key への線形変換)  \n",
    "V = X @ W_v  (Value への線形変換)\n",
    "    ↓\n",
    "Attention(Q, K, V) = softmax(QK^T / √d_k) @ V  (Scaled Dot-Product Attention)\n",
    "    ↓\n",
    "output = Attention @ W_o  (出力線形変換)\n",
    "```\n",
    "\n",
    "#### 実装での対応\n",
    "\n",
    "`src/attention.py`の`SelfAttention`クラスはまさにこの構造を実装しています:\n",
    "\n",
    "1. **線形変換層** (`__init__`で定義):\n",
    "   - `self.query_linear`, `self.key_linear`, `self.value_linear`\n",
    "\n",
    "2. **Scaled Dot-Product Attention** (`forward`内):\n",
    "   - `scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)`\n",
    "   - `attention_weights = F.softmax(scores, dim=-1)`\n",
    "   - `output = torch.matmul(attention_weights, V)`\n",
    "\n",
    "3. **出力線形変換**:\n",
    "   - `self.output_linear`\n",
    "\n",
    "#### \"Self\"の意味\n",
    "\n",
    "\"Self\"というのは、Q, K, V **すべてが同じ入力Xから生成される**ことを意味します（他の系列からではなく、自分自身から）。\n",
    "\n",
    "これに対してCross-Attentionでは、QとK/Vが異なる入力から生成されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bbacfd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q18: Self-Attentionの出力形状について\n",
    "\n",
    "**質問日**: 2025年11月21日\n",
    "\n",
    "### 質問\n",
    "Self-Attentionは入力と同じ形をしていると考えて良いか？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aecc52d",
   "metadata": {},
   "source": [
    "### 回答\n",
    "\n",
    "はい、**Self-Attentionの出力は入力と同じ形**をしています。\n",
    "\n",
    "#### 形状の一致\n",
    "\n",
    "- **入力**: `[batch_size, seq_len, d_model]`\n",
    "- **出力**: `[batch_size, seq_len, d_model]`\n",
    "\n",
    "#### なぜ同じ形状なのか\n",
    "\n",
    "1. **残差接続(Residual Connection)が可能**: \n",
    "   - Transformerでは `output = LayerNorm(x + SelfAttention(x))` のように入力を足し合わせる\n",
    "   - そのため、形状が同じである必要があります\n",
    "\n",
    "2. **層を積み重ねられる**: \n",
    "   - 出力が入力と同じ形なので、Self-Attention層を何層も重ねることができます\n",
    "\n",
    "3. **系列長が保持される**: \n",
    "   - 各位置の情報が保持され、系列全体の文脈を考慮した新しい表現に変換されます\n",
    "\n",
    "#### 内部での次元変化\n",
    "\n",
    "内部では次元が変化しますが、最終的に元に戻ります:\n",
    "\n",
    "```\n",
    "入力 X: [batch, seq_len, d_model]\n",
    "    ↓ 線形変換\n",
    "Q, K, V: [batch, seq_len, d_k]  (d_k は通常 d_model と同じか小さい)\n",
    "    ↓ Scaled Dot-Product Attention\n",
    "Attention出力: [batch, seq_len, d_k]\n",
    "    ↓ 出力線形変換(あれば)\n",
    "最終出力: [batch, seq_len, d_model]  (入力と同じ!)\n",
    "```\n",
    "\n",
    "#### 実装での確認\n",
    "\n",
    "`src/attention.py`の実装でも、`d_model`を入力・出力の両方で使用することで、この形状の一致を保証しています。\n",
    "\n",
    "```python\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.query_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.key_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.value_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.output_linear = nn.Linear(d_model, d_model)  # 同じd_model\n",
    "```\n",
    "\n",
    "この入出力の形状一致が、Transformerの深い構造を可能にしています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44376693",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q19: Multi-Head Attentionのhead分割について\n",
    "\n",
    "**質問日**: 2025年11月21日\n",
    "\n",
    "### 質問\n",
    "Multi-Head Attentionについて、head分割とは特徴量ベクトルを分割することと捉えていいか？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c04f96",
   "metadata": {},
   "source": [
    "### 回答\n",
    "\n",
    "はい、その理解で正確です！\n",
    "\n",
    "**Multi-Head Attentionのhead分割 = 特徴量ベクトル（d_model次元）を分割すること**\n",
    "\n",
    "#### 具体的な分割の仕組み\n",
    "\n",
    "```\n",
    "入力: [batch, seq_len, d_model=512]\n",
    "    ↓ 線形変換 (W_q)\n",
    "Q: [batch, seq_len, d_model=512]\n",
    "    ↓ head分割\n",
    "Q: [batch, num_heads=8, seq_len, d_k=64]\n",
    "```\n",
    "\n",
    "例えば `d_model=512`, `num_heads=8` の場合:\n",
    "- 各headは **d_k = 512 / 8 = 64次元** を担当\n",
    "- 512次元の特徴量ベクトルを、8個の64次元ベクトルに分割\n",
    "- 各headは **特徴量空間の異なる部分** を見る\n",
    "\n",
    "#### 重要なポイント\n",
    "\n",
    "1. **分割は特徴量次元**: \n",
    "   - シーケンス長（seq_len）は分割されず、そのまま保持\n",
    "   - 特徴量次元（d_model）だけが分割される\n",
    "\n",
    "2. **独立した部分空間**: \n",
    "   - 各headは特徴量の異なる側面を学習\n",
    "   - Head 1: 文法的な関係を捉える\n",
    "   - Head 2: 意味的な類似性を捉える\n",
    "   - Head 3: 位置関係を捉える（など）\n",
    "\n",
    "3. **最後に結合**: \n",
    "   - 全headの出力を結合（concat）して元の`d_model`次元に戻す\n",
    "   - `[batch, num_heads, seq_len, d_k]` → `[batch, seq_len, d_model]`\n",
    "\n",
    "#### 実装での確認\n",
    "\n",
    "`src/attention.py`の`MultiHeadAttention`クラスの`split_heads`メソッドがまさにこの分割処理をしています:\n",
    "\n",
    "```python\n",
    "def split_heads(self, x, batch_size):\n",
    "    \"\"\"\n",
    "    入力を複数のheadに分割\n",
    "    \n",
    "    Args:\n",
    "        x: shape [batch_size, seq_len, d_model]\n",
    "    Returns:\n",
    "        shape [batch_size, num_heads, seq_len, d_k]\n",
    "    \"\"\"\n",
    "    # [batch, seq_len, d_model] -> [batch, seq_len, num_heads, d_k]\n",
    "    x = x.view(batch_size, -1, self.num_heads, self.d_k)\n",
    "    # [batch, seq_len, num_heads, d_k] -> [batch, num_heads, seq_len, d_k]\n",
    "    return x.transpose(1, 2)\n",
    "```\n",
    "\n",
    "これで512次元の特徴量を8個の64次元に分割し、各headで並列処理しています！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ee0358",
   "metadata": {},
   "source": [
    "#### コード例: head分割の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36a4634f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元の入力形状: torch.Size([2, 5, 512])\n",
      "  - batch_size: 2\n",
      "  - seq_len: 5\n",
      "  - d_model: 512\n",
      "\n",
      "reshape後: torch.Size([2, 5, 8, 64])\n",
      "transpose後（head分割完了）: torch.Size([2, 8, 5, 64])\n",
      "  - batch_size: 2\n",
      "  - num_heads: 8 (8個のheadに分割)\n",
      "  - seq_len: 5 (シーケンス長は保持)\n",
      "  - d_k: 64 (各headは64次元を担当)\n",
      "\n",
      "💡 特徴量の512次元が、8個の64次元に分割されました！\n",
      "   各headが特徴量空間の異なる部分を並列に処理します。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# パラメータ設定\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_k = d_model // num_heads  # 64\n",
    "\n",
    "# サンプル入力\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "print(f\"元の入力形状: {x.shape}\")\n",
    "print(f\"  - batch_size: {batch_size}\")\n",
    "print(f\"  - seq_len: {seq_len}\")\n",
    "print(f\"  - d_model: {d_model}\")\n",
    "\n",
    "# head分割のシミュレーション\n",
    "# [batch, seq_len, d_model] -> [batch, seq_len, num_heads, d_k]\n",
    "x_reshaped = x.view(batch_size, seq_len, num_heads, d_k)\n",
    "print(f\"\\nreshape後: {x_reshaped.shape}\")\n",
    "\n",
    "# [batch, seq_len, num_heads, d_k] -> [batch, num_heads, seq_len, d_k]\n",
    "x_split = x_reshaped.transpose(1, 2)\n",
    "print(f\"transpose後（head分割完了）: {x_split.shape}\")\n",
    "print(f\"  - batch_size: {x_split.shape[0]}\")\n",
    "print(f\"  - num_heads: {x_split.shape[1]} (8個のheadに分割)\")\n",
    "print(f\"  - seq_len: {x_split.shape[2]} (シーケンス長は保持)\")\n",
    "print(f\"  - d_k: {x_split.shape[3]} (各headは{d_k}次元を担当)\")\n",
    "\n",
    "print(f\"\\n💡 特徴量の{d_model}次元が、{num_heads}個の{d_k}次元に分割されました！\")\n",
    "print(f\"   各headが特徴量空間の異なる部分を並列に処理します。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a400b392",
   "metadata": {},
   "source": [
    "#### 結合の仕組み\n",
    "\n",
    "分割と逆の操作で、全headの出力を結合します:\n",
    "\n",
    "```python\n",
    "def combine_heads(self, x, batch_size):\n",
    "    \"\"\"\n",
    "    複数のheadを結合\n",
    "    \n",
    "    Args:\n",
    "        x: shape [batch_size, num_heads, seq_len, d_k]\n",
    "    Returns:\n",
    "        shape [batch_size, seq_len, d_model]\n",
    "    \"\"\"\n",
    "    # [batch, num_heads, seq_len, d_k] -> [batch, seq_len, num_heads, d_k]\n",
    "    x = x.transpose(1, 2).contiguous()\n",
    "    # [batch, seq_len, num_heads, d_k] -> [batch, seq_len, d_model]\n",
    "    return x.view(batch_size, -1, self.d_model)\n",
    "```\n",
    "\n",
    "この分割と結合により、Multi-Head Attentionは複数の表現部分空間から並列に情報を学習できます！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5475aa85",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q20: Multi-Head Attentionで特徴量を部分空間に分割する利点\n",
    "\n",
    "**質問日**: 2025年11月21日\n",
    "\n",
    "### 質問\n",
    "特徴量を部分空間にすることでどんな良い所があるのか？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1932c1a",
   "metadata": {},
   "source": [
    "### 回答\n",
    "\n",
    "Multi-Head Attentionで特徴量を部分空間に分割することには、**複数の重要な利点**があります。\n",
    "\n",
    "#### 1. 異なる種類の関係性を並列に学習できる\n",
    "\n",
    "各headが異なる表現部分空間を担当することで、**多様な視点**から情報を捉えられます。\n",
    "\n",
    "**例**: 自然言語処理の場合\n",
    "- **Head 1**: 文法的な依存関係（主語-述語など）\n",
    "- **Head 2**: 意味的な類似性（同義語、関連語）\n",
    "- **Head 3**: 位置的な近接性（隣接する単語）\n",
    "- **Head 4**: 長距離依存（文をまたぐ参照関係）\n",
    "- **Head 5-8**: その他の抽象的なパターン\n",
    "\n",
    "これらを**同時に**学習することで、単一のAttentionでは捉えきれない複雑な関係性を表現できます。\n",
    "\n",
    "#### 2. 計算効率が変わらない（パラメータ数が増えない）\n",
    "\n",
    "重要な点として、headを増やしても**総パラメータ数は変わりません**:\n",
    "\n",
    "- **Single-Head** (d_model=512): \n",
    "  - パラメータ数 = 4 × (512 × 512) = 1,048,576\n",
    "  \n",
    "- **Multi-Head** (d_model=512, num_heads=8, d_k=64):\n",
    "  - パラメータ数 = 4 × (512 × 512) = 1,048,576\n",
    "\n",
    "各headの次元が小さくなる（d_k = d_model / num_heads）ため、総計算量は同じです。\n",
    "\n",
    "#### 3. 過学習のリスクを分散\n",
    "\n",
    "1つの大きな空間で学習するよりも、複数の小さな部分空間で学習することで:\n",
    "\n",
    "- **特化した学習**: 各headが特定のパターンに特化\n",
    "- **冗長性の確保**: 一部のheadが失敗しても他がカバー\n",
    "- **汎化性能の向上**: 異なる視点の組み合わせで頑健に\n",
    "\n",
    "#### 4. 解釈可能性の向上\n",
    "\n",
    "各headの役割を可視化・分析することで、モデルが**何を学習しているか**が理解しやすくなります。\n",
    "\n",
    "実際の研究では、各headが以下のような役割を学習していることが観察されています:\n",
    "- 位置的注意（直前・直後の単語に注目）\n",
    "- 構文的注意（文法構造に沿った注目）\n",
    "- 意味的注意（意味的に関連する単語に注目）\n",
    "\n",
    "#### 5. アンサンブル効果\n",
    "\n",
    "複数のheadを持つことは、ある種の**アンサンブル学習**と似た効果があります:\n",
    "\n",
    "- 異なる視点からの予測を統合\n",
    "- より安定した表現の獲得\n",
    "- 単一モデルでアンサンブルの利点を享受"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43caff6",
   "metadata": {},
   "source": [
    "#### コード例: Single-Head vs Multi-Headの比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c6fcf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Single-Head Attention\n",
      "======================================================================\n",
      "パラメータ数: 1,048,576\n",
      "出力形状: torch.Size([2, 10, 512])\n",
      "Attention重み形状: torch.Size([2, 1, 10, 10])\n",
      "  - 1つのheadで全体を見る\n",
      "  - d_k = 512 (= d_model)\n",
      "\n",
      "======================================================================\n",
      "Multi-Head Attention (8 heads)\n",
      "======================================================================\n",
      "パラメータ数: 1,048,576\n",
      "出力形状: torch.Size([2, 10, 512])\n",
      "Attention重み形状: torch.Size([2, 8, 10, 10])\n",
      "  - 8つのheadで異なる視点から見る\n",
      "  - d_k = 64 (= d_model / 8)\n",
      "\n",
      "======================================================================\n",
      "比較結果\n",
      "======================================================================\n",
      "パラメータ数の差: 0 (ほぼ同じ)\n",
      "出力形状: 両方とも torch.Size([2, 10, 512]) (同じ)\n",
      "\n",
      "💡 重要な違い:\n",
      "  - Single-Head: 1つの大きな空間で関係性を学習\n",
      "  - Multi-Head: 8つの独立した部分空間で異なる関係性を並列学習\n",
      "\n",
      "Multi-Headの利点:\n",
      "  ✓ 多様な視点から情報を捉える\n",
      "  ✓ パラメータ数は同じまま表現力が向上\n",
      "  ✓ より複雑なパターンを学習可能\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "import torch\n",
    "from attention import SelfAttention, MultiHeadAttention\n",
    "\n",
    "d_model = 512\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "# テスト入力\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Single-Head Attention\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Single-Head (実質的にはnum_heads=1のMulti-Head)\n",
    "single_head = MultiHeadAttention(d_model, num_heads=1)\n",
    "output_single, attn_single = single_head(x, x, x)\n",
    "\n",
    "single_params = sum(p.numel() for p in single_head.parameters())\n",
    "print(f\"パラメータ数: {single_params:,}\")\n",
    "print(f\"出力形状: {output_single.shape}\")\n",
    "print(f\"Attention重み形状: {attn_single.shape}\")\n",
    "print(f\"  - 1つのheadで全体を見る\")\n",
    "print(f\"  - d_k = {single_head.d_k} (= d_model)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Multi-Head Attention (8 heads)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Multi-Head\n",
    "multi_head = MultiHeadAttention(d_model, num_heads=8)\n",
    "output_multi, attn_multi = multi_head(x, x, x)\n",
    "\n",
    "multi_params = sum(p.numel() for p in multi_head.parameters())\n",
    "print(f\"パラメータ数: {multi_params:,}\")\n",
    "print(f\"出力形状: {output_multi.shape}\")\n",
    "print(f\"Attention重み形状: {attn_multi.shape}\")\n",
    "print(f\"  - 8つのheadで異なる視点から見る\")\n",
    "print(f\"  - d_k = {multi_head.d_k} (= d_model / 8)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"比較結果\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"パラメータ数の差: {multi_params - single_params:,} (ほぼ同じ)\")\n",
    "print(f\"出力形状: 両方とも {output_single.shape} (同じ)\")\n",
    "print(f\"\\n💡 重要な違い:\")\n",
    "print(f\"  - Single-Head: 1つの大きな空間で関係性を学習\")\n",
    "print(f\"  - Multi-Head: 8つの独立した部分空間で異なる関係性を並列学習\")\n",
    "print(f\"\\nMulti-Headの利点:\")\n",
    "print(f\"  ✓ 多様な視点から情報を捉える\")\n",
    "print(f\"  ✓ パラメータ数は同じまま表現力が向上\")\n",
    "print(f\"  ✓ より複雑なパターンを学習可能\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a63a6e",
   "metadata": {},
   "source": [
    "#### 視覚的な理解\n",
    "\n",
    "**Single-Head Attention**:\n",
    "```\n",
    "[512次元の特徴量空間]\n",
    "    ↓\n",
    "  全体を一度に見る\n",
    "    ↓\n",
    "  1つの視点\n",
    "```\n",
    "\n",
    "**Multi-Head Attention (8 heads)**:\n",
    "```\n",
    "[512次元の特徴量空間]\n",
    "    ↓ 分割\n",
    "[64次元] [64次元] [64次元] [64次元] [64次元] [64次元] [64次元] [64次元]\n",
    "  Head1    Head2    Head3    Head4    Head5    Head6    Head7    Head8\n",
    "    ↓        ↓        ↓        ↓        ↓        ↓        ↓        ↓\n",
    "  文法的   意味的   位置的   長距離   ...     ...     ...     ...\n",
    "  関係     類似性   近接性   依存\n",
    "    ↓\n",
    "  結合して統合\n",
    "    ↓\n",
    "[512次元の豊かな表現]\n",
    "```\n",
    "\n",
    "#### まとめ\n",
    "\n",
    "部分空間への分割は、**計算コストを増やさずに表現力を高める**巧妙な設計です:\n",
    "\n",
    "1. **多様性**: 異なる視点から情報を捉える\n",
    "2. **効率性**: パラメータ数を増やさない\n",
    "3. **専門化**: 各headが特定のパターンに特化\n",
    "4. **統合**: 複数の視点を組み合わせて豊かな表現を獲得\n",
    "5. **解釈性**: 各headの役割が可視化・分析可能\n",
    "\n",
    "これがMulti-Head Attentionが**Transformerの成功の鍵**となった理由の1つです！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
